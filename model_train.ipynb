{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from test_evaluate_functions.test_evaluate_functions import calculate_accuracies, test_model, return_variable_importance\n",
    "from training_functions.training_functions import calculate_binary_variables, split, split_x_y, evaluate_models, train_ada_boost, split_easy_and_sudden_errors\n",
    "from predictive_models.predictive_models_functions import adboc_predictor, test_adboc_model\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** Change the directories to your local machine when reading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for lights\n",
    "## \"adboc\" model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nan columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading done!\n",
      "Drop done!\n"
     ]
    }
   ],
   "source": [
    "dff = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/preprocessing_results/out_lights.csv\")\n",
    "print(\"Reading done!\")\n",
    "# We will drop all the columns readings and lon and lat:\n",
    "drop_cols = [\n",
    "                col for col in dff.columns if \n",
    "                (col.startswith(\"power\")) | (col.startswith(\"Active\")) | (col.startswith(\"Reactive\") | \n",
    "                (col in [\"lat\", \"lon\"])) | \n",
    "                (col == \"Unnamed: 0\") |\n",
    "                (col.startswith(\"week\")) |\n",
    "                (col == \"type\") |\n",
    "                (col == \"ebox_id\") |\n",
    "                (col == \"location\") |\n",
    "                (col == \"id\")\n",
    "            ]\n",
    "df = dff.drop(drop_cols, axis=1)\n",
    "print(\"Drop done!\")\n",
    "# Interpolate with the mean in case it is necessary:\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "# Interpolation in this case should not be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8648401942667765\n",
      "0.3588560269454507\n",
      "0.3295029149558065\n",
      "0.31164105809874276\n",
      "2014-10-06 00:00:00\n",
      "2019-04-29 00:00:00\n",
      "2019-05-06 00:00:00\n",
      "2021-03-29 00:00:00\n",
      "2021-04-05 00:00:00\n",
      "2023-03-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = calculate_binary_variables(df)\n",
    "# Preprocessing and split:\n",
    "df[\"current_week\"] = pd.to_datetime(df[\"current_week\"])\n",
    "train, validation, test = split(df, n_weeks=100)\n",
    "\n",
    "cols_to_train = df.drop([\"current_week\", \"hours_next_four_weeks\", \"error_next_four_weeks\", \"hours_week+1\", \"hours_week+2\", \"hours_week+3\", \"hours_week+4\"], axis=1).columns\n",
    "x_train, y_train = split_x_y(train, cols_to_train)\n",
    "x_validation, y_validation = split_x_y(validation, cols_to_train)\n",
    "x_test, y_test = split_x_y(test, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with the whole train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = evaluate_models(\n",
    "    max_depth_l = [1, 2, 3],\n",
    "    n_estimators_l = [1, 2, 5, 10, 15, 20, 30],\n",
    "    lr_l = [0.1, 0.2, 0.5],\n",
    "    prob_threshold_l = np.arange(0.45, 0.55, 0.01).tolist(),\n",
    "    x_train = x_train,\n",
    "    y_train = y_train,\n",
    "    x_validation = x_validation,\n",
    "    y_validation = y_validation\n",
    ")\n",
    "display(model_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid search ends explore the dataframe model_results and fins the model that best fits your training and validation data. In our case we will go with the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.9647697583278902,\n",
       " 'yes_accuracy': 0.9018666666666667,\n",
       " 'no_accuracy': 0.9796551985258853}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model = train_ada_boost(\n",
    "    max_depth_tree = 1,\n",
    "    n_estimators = 30,\n",
    "    lr = 0.5,\n",
    "    x  = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_model,\n",
    "    x = x_test, \n",
    "    y = y_test,\n",
    "    prob_threshold = 0.45\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_current_week</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_min_min_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_std_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_avg_week-4</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_avg_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_max_max_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_min_min_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_max_max_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    variable  importance\n",
       "0               hours_week-4    0.033333\n",
       "0               hours_week-3    0.033333\n",
       "0               hours_week-2    0.033333\n",
       "0               hours_week-1    0.033333\n",
       "0         hours_current_week    0.100000\n",
       "0        Temp_min_min_week-4    0.033333\n",
       "0         Dew_avg_std_week-4    0.033333\n",
       "0         Hum_avg_avg_week-4    0.066667\n",
       "0        Pres_avg_std_week-4    0.033333\n",
       "0        Temp_avg_avg_week-3    0.033333\n",
       "0        Temp_avg_std_week-3    0.033333\n",
       "0         Hum_avg_avg_week-3    0.033333\n",
       "0        Wind_max_max_week-3    0.033333\n",
       "0        Pres_avg_avg_week-3    0.033333\n",
       "0        Pres_avg_std_week-3    0.033333\n",
       "0        Temp_avg_avg_week-2    0.033333\n",
       "0         Hum_avg_std_week-2    0.033333\n",
       "0         Hum_min_min_week-2    0.033333\n",
       "0        Pres_avg_std_week-2    0.033333\n",
       "0        Temp_avg_std_week-1    0.033333\n",
       "0         Hum_avg_std_week-1    0.033333\n",
       "0        Pres_avg_avg_week-1    0.033333\n",
       "0        Pres_avg_std_week-1    0.033333\n",
       "0  Temp_avg_avg_current_week    0.033333\n",
       "0   Hum_avg_std_current_week    0.033333\n",
       "0  Pres_max_max_current_week    0.033333\n",
       "0  Pres_avg_avg_current_week    0.033333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = return_variable_importance(\n",
    "    ada_model = ada_model, \n",
    "    trained_columns = cols_to_train\n",
    ")\n",
    "display(feature_importances.loc[feature_importances[\"importance\"] != 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sudden errors\n",
    "We have a suspicion that the model we just trained does a very simple classification task using the errors of the last weeks, If the model finds errors in that weeks then it predicts an error in the following 4 weeks because it is normal to have errors recurrent errors.\n",
    "\n",
    "From now on we will denote this errors as **\"easy errors\"**. This kind of error is easy to predict and we are more interested in predicting the from now on denoted **\"sudden errors\"**. This kind of errors are errors that appear without having a hard history of errors in the past weeks.\n",
    "\n",
    "For example an error of type sudden whould be a lampost that suffers and error but we have all the variables hors_week-i at 0 or even one of them >0.\n",
    "\n",
    "We will define an error as sudden error if we have 2 or less past weeks with errors. We consider the variable current_week to be a past week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to extract this kind of errors from the test data and see how the model we just trained predicts them to verify our theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sudden, test_easy = split_easy_and_sudden_errors(test)\n",
    "\n",
    "x_test_sudden, y_test_sudden = split_x_y(test_sudden, cols_to_train)\n",
    "x_test_easy, y_test_easy = split_x_y(test_easy, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the \"ada_model\" to the splited test dataset to verify our theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.2809917355371901,\n",
       " 'yes_accuracy': 0.2809917355371901,\n",
       " 'no_accuracy': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ada_model, x_test_sudden, y_test_sudden, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.9829741311568161,\n",
       " 'yes_accuracy': 0.9991979764328459,\n",
       " 'no_accuracy': 0.9796551985258853}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ada_model, x_test_easy, y_test_easy, 0.45)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model almost has a perfect fit with the test_easy dataset. On the other hand, it has very bad performance on the test_sudden dataset. We can not consider ada_model to be the optimal predictor because it has a limited capacity of detecting sudden errors. We will have to design a better model for this cases.\n",
    "\n",
    "To do it, the best way will be to split the datasets train, validation and test to train the sub-model so we make sure that the models are trained with the same datasests or subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sudden, train_easy = split_easy_and_sudden_errors(train)\n",
    "validation_sudden, validation_easy = split_easy_and_sudden_errors(validation)\n",
    "\n",
    "# The dataset train_sudden has just rows with the column \"error_next_four_weeks\" = \"Yes\" so we will have to add\n",
    "# some \"No\" rows in order to the model to train. To do this we will simply add a random sample of length len(train_sudden)\n",
    "# of \"No\" rows to the dataset train_sudden.\n",
    "train_sudden = pd.concat(\n",
    "    [\n",
    "        train.loc[train[\"error_next_four_weeks\"] == \"No\"].sample(len(train_sudden)),\n",
    "        train_sudden\n",
    "    ],\n",
    "    sort=True\n",
    ").sample(frac=1.0, random_state=42)\n",
    "\n",
    "validation_sudden = pd.concat(\n",
    "    [\n",
    "        validation.loc[validation[\"error_next_four_weeks\"] == \"No\"].sample(len(validation_sudden)),\n",
    "        validation_sudden\n",
    "    ],\n",
    "    sort=True\n",
    ").sample(frac=1.0, random_state=42)\n",
    "\n",
    "x_train_sudden, y_train_sudden = split_x_y(train_sudden, cols_to_train)\n",
    "x_validation_sudden, y_validation_sudden = split_x_y(validation_sudden, cols_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "model_sudden_results = evaluate_models(\n",
    "    max_depth_l = [1, 2, 3, 4],\n",
    "    n_estimators_l = [1, 5, 10, 30, 100, 200, 300],\n",
    "    lr_l = [0.05, 0.1, 0.2, 0.5, 0.7],\n",
    "    prob_threshold_l = np.arange(0.40, 0.60, 0.01).tolist(),\n",
    "    x_train = x_train_sudden,\n",
    "    y_train = y_train_sudden,\n",
    "    x_validation = x_validation_sudden,\n",
    "    y_validation = y_validation_sudden\n",
    ")\n",
    "\n",
    "display(model_sudden_results.loc[(model_sudden_results[\"no_accuracy\"] >= model_sudden_results[\"no_accuracy\"].max()-0.37) & (model_sudden_results[\"yes_accuracy\"] >= model_sudden_results[\"yes_accuracy\"].max()-0.37)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go with the model max_depth_tree=3, n_estimators=10, lr=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.5269578905942542,\n",
       " 'yes_accuracy': 0.5269578905942542,\n",
       " 'no_accuracy': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_sudden_model = train_ada_boost(\n",
    "    max_depth_tree = 3,\n",
    "    n_estimators = 10,\n",
    "    lr = 0.7,\n",
    "    x  = x_train_sudden,\n",
    "    y = y_train_sudden\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_sudden_model, \n",
    "    x = x_test_sudden, \n",
    "    y = y_test_sudden,\n",
    "    prob_threshold = 0.48\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to double our accuracy with this model on the sudden errors. Let's check the accuracy now for the whole test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.6450951175702155,\n",
       " 'yes_accuracy': 0.8564266666666667,\n",
       " 'no_accuracy': 0.5950854431178535}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test:\n",
    "test_model(\n",
    "    model = ada_sudden_model,\n",
    "    x = x_test, \n",
    "    y = y_test,\n",
    "    prob_threshold = 0.48\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-4</td>\n",
       "      <td>0.025177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-3</td>\n",
       "      <td>0.038182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-2</td>\n",
       "      <td>0.027792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-1</td>\n",
       "      <td>0.013506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_current_week</td>\n",
       "      <td>0.097503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_week-4</td>\n",
       "      <td>0.018184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_min_min_week-4</td>\n",
       "      <td>0.001437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_std_week-4</td>\n",
       "      <td>0.028706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_min_min_week-4</td>\n",
       "      <td>0.019918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_max_max_week-4</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_avg_week-4</td>\n",
       "      <td>0.016374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_max_max_week-4</td>\n",
       "      <td>0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_week-4</td>\n",
       "      <td>0.017702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-4</td>\n",
       "      <td>0.023708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_max_max_week-3</td>\n",
       "      <td>0.003308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_avg_week-3</td>\n",
       "      <td>0.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_std_week-3</td>\n",
       "      <td>0.014192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_week-3</td>\n",
       "      <td>0.020332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-3</td>\n",
       "      <td>0.017033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_week-3</td>\n",
       "      <td>0.042449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_week-2</td>\n",
       "      <td>0.030398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_week-2</td>\n",
       "      <td>0.022916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_max_max_week-2</td>\n",
       "      <td>0.019913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_avg_week-2</td>\n",
       "      <td>0.022296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_week-2</td>\n",
       "      <td>0.030886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-2</td>\n",
       "      <td>0.013664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_week-1</td>\n",
       "      <td>0.023129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_week-1</td>\n",
       "      <td>0.027417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_max_max_week-1</td>\n",
       "      <td>0.011877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_std_week-1</td>\n",
       "      <td>0.023788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_week-1</td>\n",
       "      <td>0.024703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_min_min_week-1</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_max_max_week-1</td>\n",
       "      <td>0.033736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-1</td>\n",
       "      <td>0.017620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_max_max_week-1</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_min_min_week-1</td>\n",
       "      <td>0.023080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_std_current_week</td>\n",
       "      <td>0.016378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_max_max_current_week</td>\n",
       "      <td>0.009194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_avg_current_week</td>\n",
       "      <td>0.012312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_std_current_week</td>\n",
       "      <td>0.027598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_min_min_current_week</td>\n",
       "      <td>0.011839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_max_max_current_week</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_current_week</td>\n",
       "      <td>0.024679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_max_max_current_week</td>\n",
       "      <td>0.020071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_current_week</td>\n",
       "      <td>0.032458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_current_week</td>\n",
       "      <td>0.035195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    variable  importance\n",
       "0               hours_week-4    0.025177\n",
       "0               hours_week-3    0.038182\n",
       "0               hours_week-2    0.027792\n",
       "0               hours_week-1    0.013506\n",
       "0         hours_current_week    0.097503\n",
       "0        Temp_avg_std_week-4    0.018184\n",
       "0        Temp_min_min_week-4    0.001437\n",
       "0         Dew_avg_std_week-4    0.028706\n",
       "0         Dew_min_min_week-4    0.019918\n",
       "0         Hum_max_max_week-4    0.000082\n",
       "0         Hum_avg_avg_week-4    0.016374\n",
       "0        Wind_max_max_week-4    0.008442\n",
       "0        Pres_avg_avg_week-4    0.017702\n",
       "0        Pres_avg_std_week-4    0.023708\n",
       "0        Temp_max_max_week-3    0.003308\n",
       "0         Dew_avg_avg_week-3    0.017429\n",
       "0         Dew_avg_std_week-3    0.014192\n",
       "0         Hum_avg_std_week-3    0.020332\n",
       "0        Wind_avg_avg_week-3    0.017033\n",
       "0        Pres_avg_avg_week-3    0.042449\n",
       "0        Temp_avg_avg_week-2    0.030398\n",
       "0        Temp_avg_std_week-2    0.022916\n",
       "0         Dew_max_max_week-2    0.019913\n",
       "0         Hum_avg_avg_week-2    0.022296\n",
       "0         Hum_avg_std_week-2    0.030886\n",
       "0        Wind_avg_avg_week-2    0.013664\n",
       "0        Temp_avg_avg_week-1    0.023129\n",
       "0        Temp_avg_std_week-1    0.027417\n",
       "0         Dew_max_max_week-1    0.011877\n",
       "0         Dew_avg_std_week-1    0.023788\n",
       "0         Hum_avg_std_week-1    0.024703\n",
       "0         Hum_min_min_week-1    0.004500\n",
       "0        Wind_max_max_week-1    0.033736\n",
       "0        Wind_avg_avg_week-1    0.017620\n",
       "0        Pres_max_max_week-1    0.011264\n",
       "0        Pres_min_min_week-1    0.023080\n",
       "0  Temp_avg_std_current_week    0.016378\n",
       "0   Dew_max_max_current_week    0.009194\n",
       "0   Dew_avg_avg_current_week    0.012312\n",
       "0   Hum_avg_std_current_week    0.027598\n",
       "0   Hum_min_min_current_week    0.011839\n",
       "0  Wind_max_max_current_week    0.017635\n",
       "0  Wind_avg_avg_current_week    0.024679\n",
       "0  Pres_max_max_current_week    0.020071\n",
       "0  Pres_avg_avg_current_week    0.032458\n",
       "0  Pres_avg_std_current_week    0.035195"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check of the importance of the variables:\n",
    "feature_importances = return_variable_importance(\n",
    "    ada_model = ada_sudden_model,\n",
    "    trained_columns = cols_to_train\n",
    ")\n",
    "display(feature_importances.loc[feature_importances[\"importance\"] != 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model focuses more on all the other variables instead of focusing a lot on the hours\n",
    "\n",
    "The new accuracies are not as good as the first model but we are sure that we can detect better the sudden errors. The strategy that we will follow to do predictions will use a combination of the two models. Depending on the row that we want to predict we will have two possibilities:\n",
    "* The first one will be when the row has three or more weeks with errors in the past weeks (remember that we consider current_week as a past week). In this case we will use the model \"ada_model\"\n",
    "* The second case will be when the row has two or less weeks with errors in the past weeks. In this case we sill use the model \"ada_sudden_model\" \n",
    "\n",
    "Let's see if this strategy works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, preds = test_adboc_model(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    ada_model,\n",
    "    ada_sudden_model,\n",
    "    0.45,\n",
    "    0.48\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a slight increase in the accuracy so this is the winner!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictive_models/ada_model.pk1\", \"wb\") as file:\n",
    "    pickle.dump(ada_model, file)\n",
    "\n",
    "with open(\"predictive_models/ada_sudden_model.pk1\", \"wb\") as file:\n",
    "    pickle.dump(ada_sudden_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the probability thresholds:\n",
    "with open(\"predictive_models/ada_prob.pk1\", \"wb\") as file:\n",
    "    pickle.dump(\n",
    "        {\"prob_ada_model\": 0.45, \"prob_sudden_model\": 0.48},\n",
    "        file\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"default\" model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nan rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading done!\n"
     ]
    }
   ],
   "source": [
    "dff = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/preprocessing_results/out_lights.csv\")\n",
    "print(\"Reading done!\")\n",
    "# drop nan rows:\n",
    "df = dff.loc[~dff[\"ActivePeak_current_week\"].isna()]\n",
    "\n",
    "# Drop some usless columns for the model:\n",
    "drop_cols = [\n",
    "    col for col in df.columns if\n",
    "        (col in [\"lat\", \"lon\"]) | \n",
    "        (col == \"Unnamed: 0\") |\n",
    "        (col.startswith(\"week\")) |\n",
    "        (col == \"type\") |\n",
    "        (col == \"ebox_id\") |\n",
    "        (col == \"location\") |\n",
    "        (col == \"id\")\n",
    "]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "# Interpolate some left missing values:\n",
    "df = df.fillna(df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606851470373555\n",
      "0.32915929993025256\n",
      "0.38210248008953623\n",
      "0.2887382199802112\n",
      "2017-03-20 00:00:00\n",
      "2020-03-16 00:00:00\n",
      "2020-03-23 00:00:00\n",
      "2021-09-06 00:00:00\n",
      "2021-09-13 00:00:00\n",
      "2023-03-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = calculate_binary_variables(df)\n",
    "# Preprocessing and split:\n",
    "df[\"current_week\"] = pd.to_datetime(df[\"current_week\"])\n",
    "train, validation, test = split(df, n_weeks=77)\n",
    "\n",
    "cols_to_train = df.drop([\"current_week\", \"hours_next_four_weeks\", \"error_next_four_weeks\", \"hours_week+1\", \"hours_week+2\", \"hours_week+3\", \"hours_week+4\"], axis=1).columns\n",
    "x_train, y_train = split_x_y(train, cols_to_train)\n",
    "x_validation, y_validation = split_x_y(validation, cols_to_train)\n",
    "x_test, y_test = split_x_y(test, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with the whole train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = evaluate_models(\n",
    "    max_depth_l = [1, 2, 3],\n",
    "    n_estimators_l = [1, 2, 5, 10, 15, 20, 30],\n",
    "    lr_l = [0.1, 0.2, 0.5],\n",
    "    prob_threshold_l = np.arange(0.45, 0.55, 0.02).tolist(),\n",
    "    x_train = x_train,\n",
    "    y_train = y_train,\n",
    "    x_validation = x_validation,\n",
    "    y_validation = y_validation\n",
    ")\n",
    "display(model_results.loc[(model_results[\"yes_accuracy\"] >= model_results[\"yes_accuracy\"].max()-0.05)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going with the model lr=0.5, n_estimators=30, prob=0.47, depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.9632043143643616,\n",
       " 'yes_accuracy': 0.91628145865434,\n",
       " 'no_accuracy': 0.9808636486643213}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model = train_ada_boost(\n",
    "    max_depth_tree = 1,\n",
    "    n_estimators = 30,\n",
    "    lr = 0.5,\n",
    "    x  = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_model,\n",
    "    x = x_test, \n",
    "    y = y_test,\n",
    "    prob_threshold = 0.47\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_current_week</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReactivePeak_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powerReactive_p2_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powerReactive_p2_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powerReactive_p3_current_week</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActivePeak_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powerActive_p3_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powerActive_p3_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_min_min_week-4</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temp_avg_avg_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_avg_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_max_max_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_min_min_week-3</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_std_week-2</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_min_min_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_avg_std_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hum_avg_avg_current_week</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_max_max_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_avg_avg_current_week</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        variable  importance\n",
       "0                   hours_week-4    0.033333\n",
       "0                   hours_week-2    0.033333\n",
       "0                   hours_week-1    0.033333\n",
       "0             hours_current_week    0.133333\n",
       "0            ReactivePeak_week-1    0.033333\n",
       "0        powerReactive_p2_week-1    0.033333\n",
       "0  powerReactive_p2_current_week    0.033333\n",
       "0  powerReactive_p3_current_week    0.066667\n",
       "0              ActivePeak_week-4    0.033333\n",
       "0          powerActive_p3_week-4    0.033333\n",
       "0          powerActive_p3_week-2    0.033333\n",
       "0            Wind_avg_avg_week-4    0.033333\n",
       "0            Pres_avg_avg_week-4    0.033333\n",
       "0            Pres_min_min_week-4    0.033333\n",
       "0            Temp_avg_avg_week-3    0.033333\n",
       "0             Dew_avg_avg_week-3    0.033333\n",
       "0            Pres_max_max_week-3    0.033333\n",
       "0            Pres_min_min_week-3    0.033333\n",
       "0            Pres_avg_std_week-2    0.033333\n",
       "0             Hum_min_min_week-1    0.033333\n",
       "0            Wind_avg_avg_week-1    0.033333\n",
       "0       Dew_avg_std_current_week    0.033333\n",
       "0       Hum_avg_avg_current_week    0.066667\n",
       "0      Pres_max_max_current_week    0.033333\n",
       "0      Pres_avg_avg_current_week    0.033333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check of the importance of the variables:\n",
    "feature_importances = return_variable_importance(\n",
    "    ada_model = ada_model, \n",
    "    trained_columns = cols_to_train\n",
    ")\n",
    "display(feature_importances.loc[feature_importances[\"importance\"] != 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this case the model does use the readings for doing predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sudden errors\n",
    "We have a suspicion that the model we just trained does a very simple classification task using the errors of the last weeks, If the model finds errors in that weeks then it predicts an error in the following 4 weeks because it is normal to have errors recurrent errors.\n",
    "\n",
    "From now on we will denote this errors as **\"easy errors\"**. This kind of error is easy to predict and we are more interested in predicting the from now on denoted **\"sudden errors\"**. This kind of errors are errors that appear without having a hard history of errors in the past weeks.\n",
    "\n",
    "For example an error of type sudden whould be a lampost that suffers and error but we have all the variables hors_week-i at 0 or even one of them >0.\n",
    "\n",
    "We will define an error as sudden error if we have 2 or less past weeks with errors. We consider the variable current_week to be a past week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_sudden, test_easy = split_easy_and_sudden_errors(test)\n",
    "\n",
    "x_test_sudden, y_test_sudden = split_x_y(test_sudden, cols_to_train)\n",
    "x_test_easy, y_test_easy = split_x_y(test_easy, cols_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sudden, test_easy = split_easy_and_sudden_errors(test)\n",
    "\n",
    "x_test_sudden, y_test_sudden = split_x_y(test_sudden, cols_to_train)\n",
    "x_test_easy, y_test_easy = split_x_y(test_easy, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the model \"ada_model\" to the splited test dataset to verify our theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.2453748782862707,\n",
       " 'yes_accuracy': 0.2453748782862707,\n",
       " 'no_accuracy': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ada_model, x_test_sudden, y_test_sudden, 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.984526391901663,\n",
       " 'yes_accuracy': 0.9954065227377125,\n",
       " 'no_accuracy': 0.9808636486643213}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ada_model, x_test_easy, y_test_easy, 0.47)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model almost has a perfect fit with the test_easy dataset. On the other hand, it has very bad performance on the test_sudden dataset. We can not consider ada_model to be the optimal predictor because it has a limited capacity of detecting sudden errors. We will have to design a better model for this cases.\n",
    "\n",
    "To do it, the best way will be to split the datasets train, validation and test to train the sub-model so we make sure that the models are trained with the same datasests or subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sudden, train_easy = split_easy_and_sudden_errors(train)\n",
    "validation_sudden, validation_easy = split_easy_and_sudden_errors(validation)\n",
    "\n",
    "# The dataset train_sudden has just rows with the column \"error_next_four_weeks\" = \"Yes\" so we will have to add\n",
    "# some \"No\" rows in order to the model to train. To do this we will simply add a random sample of length len(train_sudden)\n",
    "# of \"No\" rows to the dataset train_sudden.\n",
    "train_sudden = pd.concat(\n",
    "    [\n",
    "        train.loc[train[\"error_next_four_weeks\"] == \"No\"].sample(len(train_sudden)),\n",
    "        train_sudden\n",
    "    ],\n",
    "    sort=True\n",
    ").sample(frac=1.0, random_state=42)\n",
    "\n",
    "validation_sudden = pd.concat(\n",
    "    [\n",
    "        validation.loc[validation[\"error_next_four_weeks\"] == \"No\"].sample(len(validation_sudden)),\n",
    "        validation_sudden\n",
    "    ],\n",
    "    sort=True\n",
    ").sample(frac=1.0, random_state=42)\n",
    "\n",
    "x_train_sudden, y_train_sudden = split_x_y(train_sudden, cols_to_train)\n",
    "x_validation_sudden, y_validation_sudden = split_x_y(validation_sudden, cols_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "model_sudden_results = evaluate_models(\n",
    "    max_depth_l = [1, 2, 3],\n",
    "    n_estimators_l = [1, 10, 30, 100],\n",
    "    lr_l = [0.1, 0.2, 0.5],\n",
    "    prob_threshold_l = np.arange(0.45, 0.55, 0.02).tolist(),\n",
    "    x_train = x_train_sudden,\n",
    "    y_train = y_train_sudden,\n",
    "    x_validation = x_validation_sudden,\n",
    "    y_validation = y_validation_sudden\n",
    ")\n",
    "\n",
    "display(model_sudden_results.loc[(model_sudden_results[\"no_accuracy\"] >= model_sudden_results[\"no_accuracy\"].max()-0.37) & (model_sudden_results[\"yes_accuracy\"] >= model_sudden_results[\"yes_accuracy\"].max()-0.37)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go with the model max_depth = 3, n_estimators=100, lr=0.2, prob=0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.5666991236611489,\n",
       " 'yes_accuracy': 0.5666991236611489,\n",
       " 'no_accuracy': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_sudden_model = train_ada_boost(\n",
    "    max_depth_tree = 3,\n",
    "    n_estimators = 100,\n",
    "    lr = 0.2,\n",
    "    x  = x_train_sudden,\n",
    "    y = y_train_sudden\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_sudden_model, \n",
    "    x = x_test_sudden, \n",
    "    y = y_test_sudden,\n",
    "    prob_threshold = 0.49\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting much better results than when dropping the readings columns. Since this model is based in much more less data and more columns, there is no need to develop it further with the adboc aproach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictive_models/ada_model_readings.pk1\", \"wb\") as file:\n",
    "    pickle.dump(ada_model, file)\n",
    "\n",
    "# Save the probability thresholds:\n",
    "with open(\"predictive_models/ada_prob_readings.pk1\", \"wb\") as file:\n",
    "    pickle.dump({\"prob_ada_model\": 0.47}, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for eboxes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the readings columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading done!\n",
      "Drop done!\n"
     ]
    }
   ],
   "source": [
    "dff = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/preprocessing_results/out_eboxes.csv\")\n",
    "print(\"Reading done!\")\n",
    "\n",
    "# We will drop all the columns readings and lon and lat:\n",
    "drop_cols = [\n",
    "                col for col in dff.columns if \n",
    "                (col.startswith(\"power\")) | (col.startswith(\"Active\")) | (col.startswith(\"Reactive\") | \n",
    "                (col in [\"lat\", \"lon\"])) | \n",
    "                (col == \"Unnamed: 0\") |\n",
    "                (col.startswith(\"week\")) |\n",
    "                (col == \"type\") |\n",
    "                (col == \"ebox_id\") |\n",
    "                (col == \"location\") |\n",
    "                (col == \"id\")\n",
    "            ]\n",
    "df = dff.drop(drop_cols, axis=1)\n",
    "print(\"Drop done!\")\n",
    "# Interpolate with the mean in case it is necessary:\n",
    "df = df.fillna(df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6905388399496625\n",
      "0.3274224917057545\n",
      "0.3386340235670976\n",
      "0.3339434847271479\n",
      "2015-06-08 00:00:00\n",
      "2020-01-20 00:00:00\n",
      "2020-01-27 00:00:00\n",
      "2021-08-02 00:00:00\n",
      "2021-08-09 00:00:00\n",
      "2023-02-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = calculate_binary_variables(df)\n",
    "# Preprocessing and split:\n",
    "df[\"current_week\"] = pd.to_datetime(df[\"current_week\"])\n",
    "train, validation, test = split(df, n_weeks=80)\n",
    "\n",
    "cols_to_train = df.drop([\"current_week\", \"hours_next_four_weeks\", \"error_next_four_weeks\", \"hours_week+1\", \"hours_week+2\", \"hours_week+3\", \"hours_week+4\"], axis=1).columns\n",
    "x_train, y_train = split_x_y(train, cols_to_train)\n",
    "x_validation, y_validation = split_x_y(validation, cols_to_train)\n",
    "x_test, y_test = split_x_y(test, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with the whole train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = evaluate_models(\n",
    "    max_depth_l = [1],\n",
    "    n_estimators_l = [10, 20],\n",
    "    lr_l = [0.5, 0.7, 0.8],\n",
    "    prob_threshold_l = np.arange(0.46, 0.5, 0.01).tolist(),\n",
    "    x_train = x_train,\n",
    "    y_train = y_train,\n",
    "    x_validation = x_validation,\n",
    "    y_validation = y_validation\n",
    ")\n",
    "\n",
    "display(model_results.loc[(model_results[\"yes_accuracy\"] >= model_results[\"yes_accuracy\"].max()-0.37) & ((model_results[\"no_accuracy\"] >= model_results[\"no_accuracy\"].max()-0.37))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going with the model lr=0.5, n_estimators=10, prob_threshold=0.48, tree_depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.7115450496745461,\n",
       " 'yes_accuracy': 0.5689381933438986,\n",
       " 'no_accuracy': 0.7508741258741258}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model = train_ada_boost(\n",
    "    max_depth_tree = 1,\n",
    "    n_estimators = 10,\n",
    "    lr = 0.5,\n",
    "    x  = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_model,\n",
    "    x = x_test, \n",
    "    y = y_test,\n",
    "    prob_threshold = 0.48\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_week-1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours_current_week</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_min_min_week-4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_max_max_week-3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind_avg_avg_week-3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dew_max_max_current_week</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pres_min_min_current_week</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    variable  importance\n",
       "0               hours_week-4         0.1\n",
       "0               hours_week-3         0.1\n",
       "0               hours_week-2         0.1\n",
       "0               hours_week-1         0.1\n",
       "0         hours_current_week         0.1\n",
       "0        Pres_min_min_week-4         0.1\n",
       "0        Wind_max_max_week-3         0.1\n",
       "0        Wind_avg_avg_week-3         0.1\n",
       "0   Dew_max_max_current_week         0.1\n",
       "0  Pres_min_min_current_week         0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check of the importance of the variables:\n",
    "feature_importances = return_variable_importance(\n",
    "    ada_model = ada_model, \n",
    "    trained_columns = cols_to_train\n",
    ")\n",
    "display(feature_importances.loc[feature_importances[\"importance\"] != 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test with droping the nan rows and keeping the readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan rows:\n",
    "df = dff.loc[~dff[\"ActivePeak_current_week\"].isna()]\n",
    "\n",
    "# Drop some usless columns for the model:\n",
    "drop_cols = [\n",
    "    col for col in df.columns if\n",
    "        (col in [\"lat\", \"lon\"]) | \n",
    "        (col == \"Unnamed: 0\") |\n",
    "        (col.startswith(\"week\")) |\n",
    "        (col == \"type\") |\n",
    "        (col == \"ebox_id\") |\n",
    "        (col == \"location\") |\n",
    "        (col == \"id\")\n",
    "]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "# Interpolate some left missing values:\n",
    "df = df.fillna(df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6592943654555029\n",
      "0.41758820431806215\n",
      "0.28949447077409163\n",
      "0.2929173249078462\n",
      "2015-06-08 00:00:00\n",
      "2020-06-08 00:00:00\n",
      "2020-06-15 00:00:00\n",
      "2021-08-02 00:00:00\n",
      "2021-08-09 00:00:00\n",
      "2022-10-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = calculate_binary_variables(df)\n",
    "# Preprocessing and split:\n",
    "df[\"current_week\"] = pd.to_datetime(df[\"current_week\"])\n",
    "train, validation, test = split(df, n_weeks=60)\n",
    "\n",
    "cols_to_train = df.drop([\"current_week\", \"hours_next_four_weeks\", \"error_next_four_weeks\", \"hours_week+1\", \"hours_week+2\", \"hours_week+3\", \"hours_week+4\"], axis=1).columns\n",
    "x_train, y_train = split_x_y(train, cols_to_train)\n",
    "x_validation, y_validation = split_x_y(validation, cols_to_train)\n",
    "x_test, y_test = split_x_y(test, cols_to_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with the whole train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = evaluate_models(\n",
    "    max_depth_l = [1, 2, 3],\n",
    "    n_estimators_l = [1, 2, 5, 10, 15],\n",
    "    lr_l = [0.1, 0.2, 0.5],\n",
    "    prob_threshold_l = np.arange(0.40, 0.55, 0.02).tolist(),\n",
    "    x_train = x_train,\n",
    "    y_train = y_train,\n",
    "    x_validation = x_validation,\n",
    "    y_validation = y_validation\n",
    ")\n",
    "\n",
    "display(model_results.loc[(model_results[\"yes_accuracy\"] >= model_results[\"yes_accuracy\"].max()-0.33) & ((model_results[\"no_accuracy\"] >= model_results[\"no_accuracy\"].max()-0.33))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go with the model lr=0.5, n_estimators=15, prob_threshold=0.47, tree_depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_accuracy': 0.29842696629213483,\n",
       " 'yes_accuracy': 0.8971428571428571,\n",
       " 'no_accuracy': 0.11352941176470588}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model_readings = train_ada_boost(\n",
    "    max_depth_tree = 1,\n",
    "    n_estimators = 15,\n",
    "    lr = 0.5,\n",
    "    x  = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "# Test:\n",
    "test_model(\n",
    "    model = ada_model_readings,\n",
    "    x = x_test,\n",
    "    y = y_test,\n",
    "    prob_threshold = 0.47\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get worse results so for the sake of simplicity we are going to just use the model that does not use the readings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the best model (Not the one we have trained in the last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictive_models/ada_model_eboxes.pk1\", \"wb\") as file:\n",
    "    pickle.dump(ada_model, file)\n",
    "\n",
    "# Save the probability thresholds:\n",
    "with open(\"predictive_models/ada_prob_eboxes.pk1\", \"wb\") as file:\n",
    "    pickle.dump({\"prob_ada_model\": 0.48}, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predic_maint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
