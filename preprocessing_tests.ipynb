{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "from preprocessing_functions.functions_preprocessing_alarms import first_eboxes_preprocess, first_lights_preprocess, big_preprocess_eboxes, big_preprocess_lights\n",
    "from preprocessing_functions.functions_preprocessing_readings import split_readings, pre_power, pre_power_peak, get_indexes_split, pre_readings_batches\n",
    "from preprocessing_functions.functions_preprocessing_meteo import first_meteo_preprocess, meteo_groupby\n",
    "from preprocessing_functions.functions_joins import join_light_alarms_readings_meteo, join_eboxes_alarms_readings_meteo\n",
    "from preprocessing_functions.utils_preprocessing import return_monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipleine test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municiplality = \"canyelles\"\n",
    "\n",
    "meteo = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/meteo_raw_data/meteo_{municiplality}.csv\")\n",
    "lights_alarms = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/raw_data/{municiplality}_lights_alarms.csv\")\n",
    "eboxes_alarms = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/raw_data/{municiplality}_eboxes_alarms.csv\")\n",
    "nodes = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/raw_data/{municiplality}_nodes.csv\")\n",
    "readings = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/raw_data/{municiplality}_readings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocess meteorological data:\n",
    "meteo = first_meteo_preprocess(\n",
    "    meteo = meteo,\n",
    "    filter_dates = True,\n",
    "    date_min = \"2018-12-31\",\n",
    "    date_max = \"2100-01-01\"\n",
    ")\n",
    "\n",
    "meteo = meteo_groupby(meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp_max_max</th>\n",
       "      <th>Temp_avg_mean</th>\n",
       "      <th>Temp_avg_std</th>\n",
       "      <th>Temp_min_min</th>\n",
       "      <th>Dew_max_max</th>\n",
       "      <th>Dew_avg_mean</th>\n",
       "      <th>Dew_avg_std</th>\n",
       "      <th>Dew_min_min</th>\n",
       "      <th>Hum_max_max</th>\n",
       "      <th>Hum_avg_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Wind_avg_mean</th>\n",
       "      <th>Wind_avg_std</th>\n",
       "      <th>Wind_min_min</th>\n",
       "      <th>Pres_max_max</th>\n",
       "      <th>Pres_avg_mean</th>\n",
       "      <th>Pres_avg_std</th>\n",
       "      <th>Pres_min_min</th>\n",
       "      <th>Precipitation_mean</th>\n",
       "      <th>Precipitation_sum</th>\n",
       "      <th>dated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>8.785714</td>\n",
       "      <td>1.262969</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.634921</td>\n",
       "      <td>2.459729</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>93</td>\n",
       "      <td>67.485714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.871429</td>\n",
       "      <td>0.930438</td>\n",
       "      <td>1</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.442857</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>9.825397</td>\n",
       "      <td>1.715167</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>5.603175</td>\n",
       "      <td>2.086461</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>94</td>\n",
       "      <td>75.728571</td>\n",
       "      <td>...</td>\n",
       "      <td>11.285714</td>\n",
       "      <td>3.068892</td>\n",
       "      <td>0</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.428571</td>\n",
       "      <td>0.179947</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.063492</td>\n",
       "      <td>1.462645</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>2.499324</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>100</td>\n",
       "      <td>77.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.785714</td>\n",
       "      <td>1.366783</td>\n",
       "      <td>0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>29.885714</td>\n",
       "      <td>0.146385</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>1.418934</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.595238</td>\n",
       "      <td>3.009196</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>87</td>\n",
       "      <td>56.114286</td>\n",
       "      <td>...</td>\n",
       "      <td>11.271429</td>\n",
       "      <td>3.034092</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>30.057143</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.777778</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>3.076193</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.202692</td>\n",
       "      <td>-3.888889</td>\n",
       "      <td>93</td>\n",
       "      <td>65.085714</td>\n",
       "      <td>...</td>\n",
       "      <td>13.114286</td>\n",
       "      <td>3.745855</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.457143</td>\n",
       "      <td>0.214920</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>18.888889</td>\n",
       "      <td>12.023810</td>\n",
       "      <td>1.523093</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>7.873016</td>\n",
       "      <td>1.477442</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>100</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>1.145176</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>30.242857</td>\n",
       "      <td>0.127242</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>10.706349</td>\n",
       "      <td>1.953870</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.968254</td>\n",
       "      <td>1.631057</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>88</td>\n",
       "      <td>69.385714</td>\n",
       "      <td>...</td>\n",
       "      <td>9.514286</td>\n",
       "      <td>1.475837</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.157143</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>16.111111</td>\n",
       "      <td>10.785714</td>\n",
       "      <td>1.563613</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.007937</td>\n",
       "      <td>3.081110</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>94</td>\n",
       "      <td>64.814286</td>\n",
       "      <td>...</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>5.014645</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>0.404145</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>13.888889</td>\n",
       "      <td>6.619048</td>\n",
       "      <td>1.889006</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>-2.388889</td>\n",
       "      <td>3.175912</td>\n",
       "      <td>-12.222222</td>\n",
       "      <td>81</td>\n",
       "      <td>54.871429</td>\n",
       "      <td>...</td>\n",
       "      <td>11.271429</td>\n",
       "      <td>3.436914</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>30.042857</td>\n",
       "      <td>0.151186</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>12.777778</td>\n",
       "      <td>6.203704</td>\n",
       "      <td>0.680111</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.481481</td>\n",
       "      <td>0.940624</td>\n",
       "      <td>-3.888889</td>\n",
       "      <td>81</td>\n",
       "      <td>63.433333</td>\n",
       "      <td>...</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>1.349074</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.116667</td>\n",
       "      <td>0.172240</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Temp_max_max  Temp_avg_mean  Temp_avg_std  Temp_min_min  Dew_max_max  \\\n",
       "0       17.222222       8.785714      1.262969      2.777778    10.000000   \n",
       "1       17.222222       9.825397      1.715167      3.888889    12.222222   \n",
       "2       15.000000       8.063492      1.462645      2.222222    12.222222   \n",
       "3       15.000000       8.500000      1.418934      2.222222     5.000000   \n",
       "4       17.777778       8.777778      3.076193      2.222222     8.888889   \n",
       "..            ...            ...           ...           ...          ...   \n",
       "417     18.888889      12.023810      1.523093      7.222222    13.888889   \n",
       "418     17.222222      10.706349      1.953870      3.888889    12.222222   \n",
       "419     16.111111      10.785714      1.563613      5.000000    12.222222   \n",
       "420     13.888889       6.619048      1.889006     -1.111111     6.111111   \n",
       "421     12.777778       6.203704      0.680111      2.222222     5.000000   \n",
       "\n",
       "     Dew_avg_mean  Dew_avg_std  Dew_min_min  Hum_max_max  Hum_avg_mean  ...  \\\n",
       "0        2.634921     2.459729    -5.000000           93     67.485714  ...   \n",
       "1        5.603175     2.086461     1.111111           94     75.728571  ...   \n",
       "2        4.095238     2.499324    -1.111111          100     77.200000  ...   \n",
       "3       -0.595238     3.009196   -13.888889           87     56.114286  ...   \n",
       "4        2.111111     2.202692    -3.888889           93     65.085714  ...   \n",
       "..            ...          ...          ...          ...           ...  ...   \n",
       "417      7.873016     1.477442     2.777778          100     76.500000  ...   \n",
       "418      4.968254     1.631057     1.111111           88     69.385714  ...   \n",
       "419      4.007937     3.081110    -7.222222           94     64.814286  ...   \n",
       "420     -2.388889     3.175912   -12.222222           81     54.871429  ...   \n",
       "421     -0.481481     0.940624    -3.888889           81     63.433333  ...   \n",
       "\n",
       "     Wind_avg_mean  Wind_avg_std  Wind_min_min  Pres_max_max  Pres_avg_mean  \\\n",
       "0        10.871429      0.930438             1          30.6      30.442857   \n",
       "1        11.285714      3.068892             0          30.7      30.428571   \n",
       "2         9.785714      1.366783             0          30.2      29.885714   \n",
       "3        11.271429      3.034092             0          30.3      30.057143   \n",
       "4        13.114286      3.745855             1          30.0      29.457143   \n",
       "..             ...           ...           ...           ...            ...   \n",
       "417       7.714286      1.145176             0          30.5      30.242857   \n",
       "418       9.514286      1.475837             0          30.4      30.157143   \n",
       "419      12.400000      5.014645             0          30.4      29.900000   \n",
       "420      11.271429      3.436914             0          30.3      30.042857   \n",
       "421       9.700000      1.349074             0          30.4      30.116667   \n",
       "\n",
       "     Pres_avg_std  Pres_min_min  Precipitation_mean  Precipitation_sum  \\\n",
       "0        0.097590          30.2                 0.0                0.0   \n",
       "1        0.179947          30.2                 0.0                0.0   \n",
       "2        0.146385          29.7                 0.0                0.0   \n",
       "3        0.171825          29.7                 0.0                0.0   \n",
       "4        0.214920          29.1                 0.0                0.0   \n",
       "..            ...           ...                 ...                ...   \n",
       "417      0.127242          30.1                 0.0                0.0   \n",
       "418      0.171825          29.8                 0.0                0.0   \n",
       "419      0.404145          29.2                 0.0                0.0   \n",
       "420      0.151186          29.6                 0.0                0.0   \n",
       "421      0.172240          29.9                 0.0                0.0   \n",
       "\n",
       "         dated  \n",
       "0   2015-01-01  \n",
       "1   2015-01-08  \n",
       "2   2015-01-15  \n",
       "3   2015-01-22  \n",
       "4   2015-01-29  \n",
       "..         ...  \n",
       "417 2022-12-29  \n",
       "418 2023-01-05  \n",
       "419 2023-01-12  \n",
       "420 2023-01-19  \n",
       "421 2023-01-26  \n",
       "\n",
       "[422 rows x 23 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dated</th>\n",
       "      <th>value</th>\n",
       "      <th>reading</th>\n",
       "      <th>component</th>\n",
       "      <th>componentid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1458542</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>2370000</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>sum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458543</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>690000</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458544</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>1030000</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458545</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>650000</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458546</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-01 00:15:01</td>\n",
       "      <td>2340000</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>sum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292687</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-06 07:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292688</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-06 07:45:01</td>\n",
       "      <td>0</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>sum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292689</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-06 07:45:01</td>\n",
       "      <td>0</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292690</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-06 07:45:01</td>\n",
       "      <td>0</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292691</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-06 07:45:01</td>\n",
       "      <td>0</td>\n",
       "      <td>powerReactive</td>\n",
       "      <td>p3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5834150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                dated    value        reading component  \\\n",
       "1458542  AA000000D44C  2019-01-01 00:00:01  2370000  powerReactive       sum   \n",
       "1458543  AA000000D44C  2019-01-01 00:00:01   690000  powerReactive        p1   \n",
       "1458544  AA000000D44C  2019-01-01 00:00:01  1030000  powerReactive        p2   \n",
       "1458545  AA000000D44C  2019-01-01 00:00:01   650000  powerReactive        p3   \n",
       "1458546  AA000000D44C  2019-01-01 00:15:01  2340000  powerReactive       sum   \n",
       "...               ...                  ...      ...            ...       ...   \n",
       "7292687  AA000001169B  2022-10-06 07:30:01        0  powerReactive        p3   \n",
       "7292688  AA000001169B  2022-10-06 07:45:01        0  powerReactive       sum   \n",
       "7292689  AA000001169B  2022-10-06 07:45:01        0  powerReactive        p1   \n",
       "7292690  AA000001169B  2022-10-06 07:45:01        0  powerReactive        p2   \n",
       "7292691  AA000001169B  2022-10-06 07:45:01        0  powerReactive        p3   \n",
       "\n",
       "         componentid  \n",
       "1458542            0  \n",
       "1458543            0  \n",
       "1458544            0  \n",
       "1458545            0  \n",
       "1458546            0  \n",
       "...              ...  \n",
       "7292687            0  \n",
       "7292688            0  \n",
       "7292689            0  \n",
       "7292690            0  \n",
       "7292691            0  \n",
       "\n",
       "[5834150 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dated</th>\n",
       "      <th>powerReactive_sum</th>\n",
       "      <th>powerReactive_p1</th>\n",
       "      <th>powerReactive_p2</th>\n",
       "      <th>powerReactive_p3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1.354618e+06</td>\n",
       "      <td>418489.583333</td>\n",
       "      <td>579010.416667</td>\n",
       "      <td>357100.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>1.411488e+06</td>\n",
       "      <td>457842.261905</td>\n",
       "      <td>567083.333333</td>\n",
       "      <td>387068.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>1.409673e+06</td>\n",
       "      <td>469687.500000</td>\n",
       "      <td>565535.714286</td>\n",
       "      <td>375208.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>1.384539e+06</td>\n",
       "      <td>455744.047619</td>\n",
       "      <td>554776.785714</td>\n",
       "      <td>373675.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA000000D44C</td>\n",
       "      <td>2019-01-28</td>\n",
       "      <td>1.376741e+06</td>\n",
       "      <td>443898.809524</td>\n",
       "      <td>549122.023810</td>\n",
       "      <td>384404.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>6.023810e+01</td>\n",
       "      <td>51.928571</td>\n",
       "      <td>51.047619</td>\n",
       "      <td>71.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>4.861905e+01</td>\n",
       "      <td>37.547619</td>\n",
       "      <td>49.523810</td>\n",
       "      <td>76.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-09-19</td>\n",
       "      <td>5.313522e+01</td>\n",
       "      <td>41.485884</td>\n",
       "      <td>56.748886</td>\n",
       "      <td>68.255572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>5.760479e+01</td>\n",
       "      <td>40.455090</td>\n",
       "      <td>49.988024</td>\n",
       "      <td>68.910180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>AA000001169B</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>6.435000e+01</td>\n",
       "      <td>65.550000</td>\n",
       "      <td>58.800000</td>\n",
       "      <td>79.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2167 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id       dated  powerReactive_sum  powerReactive_p1  \\\n",
       "0     AA000000D44C  2018-12-31       1.354618e+06     418489.583333   \n",
       "1     AA000000D44C  2019-01-07       1.411488e+06     457842.261905   \n",
       "2     AA000000D44C  2019-01-14       1.409673e+06     469687.500000   \n",
       "3     AA000000D44C  2019-01-21       1.384539e+06     455744.047619   \n",
       "4     AA000000D44C  2019-01-28       1.376741e+06     443898.809524   \n",
       "...            ...         ...                ...               ...   \n",
       "2162  AA000001169B  2022-09-05       6.023810e+01         51.928571   \n",
       "2163  AA000001169B  2022-09-12       4.861905e+01         37.547619   \n",
       "2164  AA000001169B  2022-09-19       5.313522e+01         41.485884   \n",
       "2165  AA000001169B  2022-09-26       5.760479e+01         40.455090   \n",
       "2166  AA000001169B  2022-10-03       6.435000e+01         65.550000   \n",
       "\n",
       "      powerReactive_p2  powerReactive_p3  \n",
       "0        579010.416667     357100.694444  \n",
       "1        567083.333333     387068.452381  \n",
       "2        565535.714286     375208.333333  \n",
       "3        554776.785714     373675.595238  \n",
       "4        549122.023810     384404.761905  \n",
       "...                ...               ...  \n",
       "2162         51.047619         71.095238  \n",
       "2163         49.523810         76.785714  \n",
       "2164         56.748886         68.255572  \n",
       "2165         49.988024         68.910180  \n",
       "2166         58.800000         79.100000  \n",
       "\n",
       "[2167 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Readings preprocess:\n",
    "powerReactive, powerActive, powerReactivePeak, powerActivePeak = split_readings(readings)\n",
    "display(powerReactive)\n",
    "powerReactive, powerActive = pre_power(powerReactive, \"Reactive\"), pre_power(powerActive, \"Active\")\n",
    "display(powerReactive)\n",
    "powerReactivePeak, powerActivePeak = pre_power_peak(powerReactivePeak, \"Reactive\"), pre_power_peak(powerActivePeak, \"Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA0000013912\n",
      "AA0000013926\n",
      "AA0000013931\n",
      "AA000001393D\n",
      "AA000001393E\n",
      "AA0000016F53\n",
      "AA000001726C\n",
      "AA0000017324\n",
      "AA00000184EB\n",
      "AA0000018505\n",
      "AA0000018696\n",
      "AA0000018697\n",
      "AA0000018698\n",
      "AA0000018699\n",
      "AA00000186B4\n",
      "AA00000186D0\n",
      "AA0000018704\n",
      "AA0000018705\n",
      "AA0000018706\n",
      "AA0000018719\n",
      "AA0000018745\n",
      "AA0000018765\n",
      "AA0000018769\n",
      "AA000001876F\n",
      "AA000001877F\n",
      "AA0000018978\n",
      "AA0000018981\n",
      "AA00000189AC\n",
      "AA00000189AD\n",
      "AA00000189B8\n",
      "AA00000189B9\n",
      "AA00000189C6\n",
      "AA00000189CB\n",
      "AA00000189E7\n",
      "AA0000018A0B\n",
      "AA0000018A38\n",
      "AA0000018A54\n",
      "AA0000018A60\n",
      "AA0000018A6C\n",
      "AA0000018B4F\n",
      "AA0000018B5F\n",
      "AA0000018B99\n",
      "AA0000018BA7\n",
      "AA0000018BA8\n",
      "AA0000018BB7\n",
      "AA0000018C28\n",
      "AA0000018C65\n",
      "AA0000018C9B\n",
      "AA0000018D53\n",
      "AA0000018D6A\n",
      "AA0000018D78\n",
      "AA0000018D7B\n",
      "AA0000018D85\n",
      "AA0000018D87\n",
      "AA0000018D89\n",
      "AA0000018D8A\n",
      "AA0000018D8C\n",
      "AA0000018D93\n",
      "AA0000018D9D\n",
      "AA0000018DA1\n",
      "AA0000018DA8\n",
      "AA0000018DAB\n",
      "AA0000018DB1\n",
      "AA0000018DBD\n",
      "AA0000018DD0\n",
      "AA0000018DD8\n",
      "AA0000018DDF\n",
      "AA0000018DE3\n",
      "AA0000018DEF\n",
      "AA0000018DF7\n",
      "AA0000018DFC\n",
      "AA0000018E0C\n",
      "AA0000018E10\n",
      "AA0000018E9D\n",
      "AA0000018EB4\n",
      "AA0000018EC0\n",
      "AA0000018ECB\n",
      "AA0000018ED0\n",
      "AA0000018ED2\n",
      "AA0000018EE1\n",
      "AA0000018EE2\n",
      "AA0000018EF0\n",
      "AA0000018EF2\n",
      "AA0000018EF4\n",
      "AA0000018EF9\n",
      "AA0000018F04\n",
      "AA0000018F07\n",
      "AA0000018F0A\n",
      "AA0000018F0F\n",
      "AA0000018F17\n",
      "AA0000018F1F\n",
      "AA0000018F21\n",
      "AA0000018F28\n",
      "AA0000018F2A\n",
      "AA0000018F31\n",
      "AA0000018F41\n",
      "AA0000018F53\n",
      "AA0000018F78\n",
      "AA0000018F83\n",
      "AA0000018F91\n",
      "AA0000018F95\n",
      "AA0000018F9B\n",
      "AA0000018F9C\n",
      "AA0000018FAC\n",
      "AA0000018FAD\n",
      "AA0000018FAE\n",
      "AA0000018FBB\n",
      "AA0000018FBC\n",
      "AA0000018FBD\n",
      "AA0000018FC1\n",
      "AA0000018FF0\n",
      "AA0000019033\n",
      "AA00000191C1\n",
      "AA00000191D0\n",
      "AA00000191F4\n",
      "AA0000019204\n",
      "AA0000019252\n",
      "AA0000019261\n",
      "AA000001926A\n",
      "AA000001926D\n",
      "AA0000019277\n",
      "AA000001927D\n",
      "AA00000192DE\n",
      "AA0000019325\n",
      "AA0000019329\n",
      "AA0000019350\n",
      "AA0000019352\n",
      "AA0000019356\n",
      "AA000001935D\n",
      "AA0000019365\n",
      "AA000001936A\n",
      "AA000001937B\n",
      "AA0000019383\n",
      "AA0000019387\n",
      "AA000001938E\n",
      "AA00000193A8\n",
      "AA0000019689\n",
      "AA000001971E\n",
      "AA000001CEEC\n",
      "Execution time:115.6778838634491\n",
      "AA000000D44C\n",
      "AA0000011276\n",
      "AA0000011277\n",
      "AA000001128F\n",
      "AA0000011290\n",
      "AA00000112BE\n",
      "AA00000112CB\n",
      "AA00000112CD\n",
      "AA000001167D\n",
      "AA0000011681\n",
      "AA000001169B\n",
      "Execution time:12.534911870956421\n"
     ]
    }
   ],
   "source": [
    "# Alarms preprocess\n",
    "lights_alarms = first_lights_preprocess(lights_alarms)\n",
    "eboxes_alarms = first_eboxes_preprocess(eboxes_alarms)\n",
    "\n",
    "lights_alarms = big_preprocess_lights(lights_alarms)\n",
    "eboxes_alarms = big_preprocess_eboxes(eboxes_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join alarms, and nodes\n",
    "lights_alarms_join = pd.merge(lights_alarms, nodes, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to run the function join_eboxes_readings_meteo before runing join_light_alarms_readings_meteo\n",
    "# because this last function modifies the datasets of the readings. In case you run the lights join\n",
    "# first the code is going to return a Key error.\n",
    "eboxes_alarms_join = join_eboxes_alarms_readings_meteo(\n",
    "    eboxes_alarms = eboxes_alarms,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")\n",
    "\n",
    "lights_alarms_join = join_light_alarms_readings_meteo(\n",
    "    light_errors = lights_alarms_join,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                   0\n",
       "week-4                               0\n",
       "hours_week-4                         0\n",
       "week-3                               0\n",
       "hours_week-3                         0\n",
       "week-2                               0\n",
       "hours_week-2                         0\n",
       "week-1                               0\n",
       "hours_week-1                         0\n",
       "current_week                         0\n",
       "hours_current_week                   0\n",
       "week+1                               0\n",
       "hours_week+1                         0\n",
       "week+2                               0\n",
       "hours_week+2                         0\n",
       "week+3                               0\n",
       "hours_week+3                         0\n",
       "week+4                               0\n",
       "hours_week+4                         0\n",
       "type                              2618\n",
       "ebox_id                           2618\n",
       "lat                               2618\n",
       "lon                               2618\n",
       "ReactivePeak_week-4               4488\n",
       "ReactivePeak_week-3               4488\n",
       "ReactivePeak_week-2               4488\n",
       "ReactivePeak_week-1               4488\n",
       "ReactivePeak_current_week         4488\n",
       "powerReactive_sum_week-4          4488\n",
       "powerReactive_p1_week-4           4488\n",
       "powerReactive_p2_week-4           4488\n",
       "powerReactive_p3_week-4           4488\n",
       "powerReactive_sum_week-3          4488\n",
       "powerReactive_p1_week-3           4488\n",
       "powerReactive_p2_week-3           4488\n",
       "powerReactive_p3_week-3           4488\n",
       "powerReactive_sum_week-2          4488\n",
       "powerReactive_p1_week-2           4488\n",
       "powerReactive_p2_week-2           4488\n",
       "powerReactive_p3_week-2           4488\n",
       "powerReactive_sum_week-1          4488\n",
       "powerReactive_p1_week-1           4488\n",
       "powerReactive_p2_week-1           4488\n",
       "powerReactive_p3_week-1           4488\n",
       "powerReactive_sum_current_week    4488\n",
       "powerReactive_p1_current_week     4488\n",
       "powerReactive_p2_current_week     4488\n",
       "powerReactive_p3_current_week     4488\n",
       "ActivePeak_week-4                 4488\n",
       "ActivePeak_week-3                 4488\n",
       "ActivePeak_week-2                 4488\n",
       "ActivePeak_week-1                 4488\n",
       "ActivePeak_current_week           4488\n",
       "powerActive_sum_week-4            4488\n",
       "powerActive_p1_week-4             4488\n",
       "powerActive_p2_week-4             4488\n",
       "powerActive_p3_week-4             4488\n",
       "powerActive_sum_week-3            4488\n",
       "powerActive_p1_week-3             4488\n",
       "powerActive_p2_week-3             4488\n",
       "powerActive_p3_week-3             4488\n",
       "powerActive_sum_week-2            4488\n",
       "powerActive_p1_week-2             4488\n",
       "powerActive_p2_week-2             4488\n",
       "powerActive_p3_week-2             4488\n",
       "powerActive_sum_week-1            4488\n",
       "powerActive_p1_week-1             4488\n",
       "powerActive_p2_week-1             4488\n",
       "powerActive_p3_week-1             4488\n",
       "powerActive_sum_current_week      4488\n",
       "powerActive_p1_current_week       4488\n",
       "powerActive_p2_current_week       4488\n",
       "powerActive_p3_current_week       4488\n",
       "Temp_max_max_week-4                  0\n",
       "Temp_avg_avg_week-4                  0\n",
       "Temp_avg_std_week-4                  0\n",
       "Temp_min_min_week-4                  0\n",
       "Dew_max_max_week-4                   0\n",
       "Dew_avg_avg_week-4                   0\n",
       "Dew_avg_std_week-4                   0\n",
       "Dew_min_min_week-4                   0\n",
       "Hum_max_max_week-4                   0\n",
       "Hum_avg_avg_week-4                   0\n",
       "Hum_avg_std_week-4                   0\n",
       "Hum_min_min_week-4                   0\n",
       "Wind_max_max_week-4                  0\n",
       "Wind_avg_avg_week-4                  0\n",
       "Wind_avg_std_week-4                  0\n",
       "Wind_min_min_week-4                  0\n",
       "Pres_max_max_week-4                  0\n",
       "Pres_avg_avg_week-4                  0\n",
       "Pres_avg_std_week-4                  0\n",
       "Pres_min_min_week-4                  0\n",
       "Precipitation_avg_week-4             0\n",
       "Precipitation_sum_week-4             0\n",
       "Temp_max_max_week-3                  0\n",
       "Temp_avg_avg_week-3                  0\n",
       "Temp_avg_std_week-3                  0\n",
       "Temp_min_min_week-3                  0\n",
       "Dew_max_max_week-3                   0\n",
       "Dew_avg_avg_week-3                   0\n",
       "Dew_avg_std_week-3                   0\n",
       "Dew_min_min_week-3                   0\n",
       "Hum_max_max_week-3                   0\n",
       "Hum_avg_avg_week-3                   0\n",
       "Hum_avg_std_week-3                   0\n",
       "Hum_min_min_week-3                   0\n",
       "Wind_max_max_week-3                  0\n",
       "Wind_avg_avg_week-3                  0\n",
       "Wind_avg_std_week-3                  0\n",
       "Wind_min_min_week-3                  0\n",
       "Pres_max_max_week-3                  0\n",
       "Pres_avg_avg_week-3                  0\n",
       "Pres_avg_std_week-3                  0\n",
       "Pres_min_min_week-3                  0\n",
       "Precipitation_avg_week-3             0\n",
       "Precipitation_sum_week-3             0\n",
       "Temp_max_max_week-2                  0\n",
       "Temp_avg_avg_week-2                  0\n",
       "Temp_avg_std_week-2                  0\n",
       "Temp_min_min_week-2                  0\n",
       "Dew_max_max_week-2                   0\n",
       "Dew_avg_avg_week-2                   0\n",
       "Dew_avg_std_week-2                   0\n",
       "Dew_min_min_week-2                   0\n",
       "Hum_max_max_week-2                   0\n",
       "Hum_avg_avg_week-2                   0\n",
       "Hum_avg_std_week-2                   0\n",
       "Hum_min_min_week-2                   0\n",
       "Wind_max_max_week-2                  0\n",
       "Wind_avg_avg_week-2                  0\n",
       "Wind_avg_std_week-2                  0\n",
       "Wind_min_min_week-2                  0\n",
       "Pres_max_max_week-2                  0\n",
       "Pres_avg_avg_week-2                  0\n",
       "Pres_avg_std_week-2                  0\n",
       "Pres_min_min_week-2                  0\n",
       "Precipitation_avg_week-2             0\n",
       "Precipitation_sum_week-2             0\n",
       "Temp_max_max_week-1                  0\n",
       "Temp_avg_avg_week-1                  0\n",
       "Temp_avg_std_week-1                  0\n",
       "Temp_min_min_week-1                  0\n",
       "Dew_max_max_week-1                   0\n",
       "Dew_avg_avg_week-1                   0\n",
       "Dew_avg_std_week-1                   0\n",
       "Dew_min_min_week-1                   0\n",
       "Hum_max_max_week-1                   0\n",
       "Hum_avg_avg_week-1                   0\n",
       "Hum_avg_std_week-1                   0\n",
       "Hum_min_min_week-1                   0\n",
       "Wind_max_max_week-1                  0\n",
       "Wind_avg_avg_week-1                  0\n",
       "Wind_avg_std_week-1                  0\n",
       "Wind_min_min_week-1                  0\n",
       "Pres_max_max_week-1                  0\n",
       "Pres_avg_avg_week-1                  0\n",
       "Pres_avg_std_week-1                  0\n",
       "Pres_min_min_week-1                  0\n",
       "Precipitation_avg_week-1             0\n",
       "Precipitation_sum_week-1             0\n",
       "Temp_max_max_current_week            0\n",
       "Temp_avg_avg_current_week            0\n",
       "Temp_avg_std_current_week            0\n",
       "Temp_min_min_current_week            0\n",
       "Dew_max_max_current_week             0\n",
       "Dew_avg_avg_current_week             0\n",
       "Dew_avg_std_current_week             0\n",
       "Dew_min_min_current_week             0\n",
       "Hum_max_max_current_week             0\n",
       "Hum_avg_avg_current_week             0\n",
       "Hum_avg_std_current_week             0\n",
       "Hum_min_min_current_week             0\n",
       "Wind_max_max_current_week            0\n",
       "Wind_avg_avg_current_week            0\n",
       "Wind_avg_std_current_week            0\n",
       "Wind_min_min_current_week            0\n",
       "Pres_max_max_current_week            0\n",
       "Pres_avg_avg_current_week            0\n",
       "Pres_avg_std_current_week            0\n",
       "Pres_min_min_current_week            0\n",
       "Precipitation_avg_current_week       0\n",
       "Precipitation_sum_current_week       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(lights_alarms_join))\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(lights_alarms_join.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                  0\n",
       "week-4                              0\n",
       "hours_week-4                        0\n",
       "week-3                              0\n",
       "hours_week-3                        0\n",
       "week-2                              0\n",
       "hours_week-2                        0\n",
       "week-1                              0\n",
       "hours_week-1                        0\n",
       "current_week                        0\n",
       "hours_current_week                  0\n",
       "week+1                              0\n",
       "hours_week+1                        0\n",
       "week+2                              0\n",
       "hours_week+2                        0\n",
       "week+3                              0\n",
       "hours_week+3                        0\n",
       "week+4                              0\n",
       "hours_week+4                        0\n",
       "ReactivePeak_week-4               176\n",
       "ReactivePeak_week-3               187\n",
       "ReactivePeak_week-2               198\n",
       "ReactivePeak_week-1               209\n",
       "ReactivePeak_current_week         220\n",
       "powerReactive_sum_week-4          176\n",
       "powerReactive_p1_week-4           176\n",
       "powerReactive_p2_week-4           176\n",
       "powerReactive_p3_week-4           176\n",
       "powerReactive_sum_week-3          187\n",
       "powerReactive_p1_week-3           187\n",
       "powerReactive_p2_week-3           187\n",
       "powerReactive_p3_week-3           187\n",
       "powerReactive_sum_week-2          198\n",
       "powerReactive_p1_week-2           198\n",
       "powerReactive_p2_week-2           198\n",
       "powerReactive_p3_week-2           198\n",
       "powerReactive_sum_week-1          209\n",
       "powerReactive_p1_week-1           209\n",
       "powerReactive_p2_week-1           209\n",
       "powerReactive_p3_week-1           209\n",
       "powerReactive_sum_current_week    220\n",
       "powerReactive_p1_current_week     220\n",
       "powerReactive_p2_current_week     220\n",
       "powerReactive_p3_current_week     220\n",
       "ActivePeak_week-4                 176\n",
       "ActivePeak_week-3                 187\n",
       "ActivePeak_week-2                 198\n",
       "ActivePeak_week-1                 209\n",
       "ActivePeak_current_week           220\n",
       "powerActive_sum_week-4            176\n",
       "powerActive_p1_week-4             176\n",
       "powerActive_p2_week-4             176\n",
       "powerActive_p3_week-4             176\n",
       "powerActive_sum_week-3            187\n",
       "powerActive_p1_week-3             187\n",
       "powerActive_p2_week-3             187\n",
       "powerActive_p3_week-3             187\n",
       "powerActive_sum_week-2            198\n",
       "powerActive_p1_week-2             198\n",
       "powerActive_p2_week-2             198\n",
       "powerActive_p3_week-2             198\n",
       "powerActive_sum_week-1            209\n",
       "powerActive_p1_week-1             209\n",
       "powerActive_p2_week-1             209\n",
       "powerActive_p3_week-1             209\n",
       "powerActive_sum_current_week      220\n",
       "powerActive_p1_current_week       220\n",
       "powerActive_p2_current_week       220\n",
       "powerActive_p3_current_week       220\n",
       "Temp_max_max_week-4               187\n",
       "Temp_avg_avg_week-4               187\n",
       "Temp_avg_std_week-4               187\n",
       "Temp_min_min_week-4               187\n",
       "Dew_max_max_week-4                187\n",
       "Dew_avg_avg_week-4                187\n",
       "Dew_avg_std_week-4                187\n",
       "Dew_min_min_week-4                187\n",
       "Hum_max_max_week-4                187\n",
       "Hum_avg_avg_week-4                187\n",
       "Hum_avg_std_week-4                187\n",
       "Hum_min_min_week-4                187\n",
       "Wind_max_max_week-4               187\n",
       "Wind_avg_avg_week-4               187\n",
       "Wind_avg_std_week-4               187\n",
       "Wind_min_min_week-4               187\n",
       "Pres_max_max_week-4               187\n",
       "Pres_avg_avg_week-4               187\n",
       "Pres_avg_std_week-4               187\n",
       "Pres_min_min_week-4               187\n",
       "Precipitation_avg_week-4          187\n",
       "Precipitation_sum_week-4          187\n",
       "Temp_max_max_week-3               198\n",
       "Temp_avg_avg_week-3               198\n",
       "Temp_avg_std_week-3               198\n",
       "Temp_min_min_week-3               198\n",
       "Dew_max_max_week-3                198\n",
       "Dew_avg_avg_week-3                198\n",
       "Dew_avg_std_week-3                198\n",
       "Dew_min_min_week-3                198\n",
       "Hum_max_max_week-3                198\n",
       "Hum_avg_avg_week-3                198\n",
       "Hum_avg_std_week-3                198\n",
       "Hum_min_min_week-3                198\n",
       "Wind_max_max_week-3               198\n",
       "Wind_avg_avg_week-3               198\n",
       "Wind_avg_std_week-3               198\n",
       "Wind_min_min_week-3               198\n",
       "Pres_max_max_week-3               198\n",
       "Pres_avg_avg_week-3               198\n",
       "Pres_avg_std_week-3               198\n",
       "Pres_min_min_week-3               198\n",
       "Precipitation_avg_week-3          198\n",
       "Precipitation_sum_week-3          198\n",
       "Temp_max_max_week-2               209\n",
       "Temp_avg_avg_week-2               209\n",
       "Temp_avg_std_week-2               209\n",
       "Temp_min_min_week-2               209\n",
       "Dew_max_max_week-2                209\n",
       "Dew_avg_avg_week-2                209\n",
       "Dew_avg_std_week-2                209\n",
       "Dew_min_min_week-2                209\n",
       "Hum_max_max_week-2                209\n",
       "Hum_avg_avg_week-2                209\n",
       "Hum_avg_std_week-2                209\n",
       "Hum_min_min_week-2                209\n",
       "Wind_max_max_week-2               209\n",
       "Wind_avg_avg_week-2               209\n",
       "Wind_avg_std_week-2               209\n",
       "Wind_min_min_week-2               209\n",
       "Pres_max_max_week-2               209\n",
       "Pres_avg_avg_week-2               209\n",
       "Pres_avg_std_week-2               209\n",
       "Pres_min_min_week-2               209\n",
       "Precipitation_avg_week-2          209\n",
       "Precipitation_sum_week-2          209\n",
       "Temp_max_max_week-1               220\n",
       "Temp_avg_avg_week-1               220\n",
       "Temp_avg_std_week-1               220\n",
       "Temp_min_min_week-1               220\n",
       "Dew_max_max_week-1                220\n",
       "Dew_avg_avg_week-1                220\n",
       "Dew_avg_std_week-1                220\n",
       "Dew_min_min_week-1                220\n",
       "Hum_max_max_week-1                220\n",
       "Hum_avg_avg_week-1                220\n",
       "Hum_avg_std_week-1                220\n",
       "Hum_min_min_week-1                220\n",
       "Wind_max_max_week-1               220\n",
       "Wind_avg_avg_week-1               220\n",
       "Wind_avg_std_week-1               220\n",
       "Wind_min_min_week-1               220\n",
       "Pres_max_max_week-1               220\n",
       "Pres_avg_avg_week-1               220\n",
       "Pres_avg_std_week-1               220\n",
       "Pres_min_min_week-1               220\n",
       "Precipitation_avg_week-1          220\n",
       "Precipitation_sum_week-1          220\n",
       "Temp_max_max_current_week         231\n",
       "Temp_avg_avg_current_week         231\n",
       "Temp_avg_std_current_week         231\n",
       "Temp_min_min_current_week         231\n",
       "Dew_max_max_current_week          231\n",
       "Dew_avg_avg_current_week          231\n",
       "Dew_avg_std_current_week          231\n",
       "Dew_min_min_current_week          231\n",
       "Hum_max_max_current_week          231\n",
       "Hum_avg_avg_current_week          231\n",
       "Hum_avg_std_current_week          231\n",
       "Hum_min_min_current_week          231\n",
       "Wind_max_max_current_week         231\n",
       "Wind_avg_avg_current_week         231\n",
       "Wind_avg_std_current_week         231\n",
       "Wind_min_min_current_week         231\n",
       "Pres_max_max_current_week         231\n",
       "Pres_avg_avg_current_week         231\n",
       "Pres_avg_std_current_week         231\n",
       "Pres_min_min_current_week         231\n",
       "Precipitation_avg_current_week    231\n",
       "Precipitation_sum_current_week    231\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(eboxes_alarms_join))\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(eboxes_alarms_join.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings spliting to be fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality = \"illora\"\n",
    "data_dir = \"/home/leibniz/Desktop/IHMAN\"\n",
    "\n",
    "readings_dates = pd.read_csv(f\"{data_dir}/raw_data/{municipality}_readings_sorted.csv\", usecols=[\"dated\"]).reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_dates[\"dated\"] = pd.to_datetime(readings_dates[\"dated\"])\n",
    "start_date = return_monday(pd.to_datetime(readings_dates[\"dated\"].min()))\n",
    "end_date = return_monday(pd.to_datetime(readings_dates[\"dated\"].max()) + pd.Timedelta(days=7))\n",
    "range_dates = pd.date_range(start=start_date, end=end_date, freq=\"20W\")\n",
    "\n",
    "# Convert to timestamps and add the last date of the dataframe:\n",
    "range_dates = [pd.Timestamp(return_monday(elem)) for elem in range_dates]\n",
    "range_dates.append(pd.Timestamp(end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this list we will store the indexes that will delimitate the batches:\n",
    "spliting_indexes = np.array([])\n",
    "for i in range(len(range_dates) -1):\n",
    "    \n",
    "    temp_df = readings_dates.loc[(range_dates[i] <= readings_dates[\"dated\"]) & (readings_dates[\"dated\"] < range_dates[i+1])]\n",
    "    if len(temp_df) != 0:\n",
    "        spliting_indexes = np.append(spliting_indexes, temp_df.index[-1])\n",
    "\n",
    "# Add the index zero:\n",
    "spliting_indexes = np.insert(spliting_indexes, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated batches will be added to this dataframes:\n",
    "powerReactive = pd.DataFrame()\n",
    "powerActive = pd.DataFrame()\n",
    "powerReactivePeak = pd.DataFrame()\n",
    "powerActivePeak = pd.DataFrame()\n",
    "\n",
    "# Name of the columns of the readings df:\n",
    "readings_column_names = [\"id\", \"dated\", \"value\", \"reading\", \"component\", \"componentid\"]\n",
    "\n",
    "# Convert to a list to iterate:\n",
    "spliting_indexes_list = list(spliting_indexes)\n",
    "\n",
    "# The readings preprocess must be done in batches because the datasets are too massive for some machines. In case that you \n",
    "# get a Memory error it is a good idea to take make larger the arg range_split_weeks_readings when running the code:\n",
    "for i in range(len(spliting_indexes_list)-1):\n",
    "\n",
    "    # For the first batch of date the function read_csv is able to read the names of the columns from the\n",
    "    # first row of the csv file\n",
    "    if i == 0:\n",
    "        readings = pd.read_csv(\n",
    "            f\"{data_dir}/raw_data/{municipality}_readings_sorted.csv\", \n",
    "            skiprows = int(spliting_indexes_list[i]), \n",
    "            nrows = int(spliting_indexes_list[i+1]-spliting_indexes_list[i]+1)\n",
    "        )\n",
    "    # For the other batches the function read_csv does not have acces to the column names so we have to\n",
    "    # specify them\n",
    "    else:\n",
    "        readings = pd.read_csv(\n",
    "            f\"{data_dir}/raw_data/{municipality}_readings_sorted.csv\",\n",
    "            skiprows = int(spliting_indexes_list[i] + 2), \n",
    "            nrows = int(spliting_indexes_list[i+1]-spliting_indexes_list[i]),\n",
    "            names = readings_column_names\n",
    "        )\n",
    "\n",
    "    powerReactive_, powerActive_, powerReactivePeak_, powerActivePeak_ = split_readings(readings)\n",
    "    powerReactive_, powerActive_ = pre_power(powerReactive_, \"Reactive\"), pre_power(powerActive_, \"Active\")\n",
    "    powerReactivePeak_, powerActivePeak_ = pre_power_peak(powerReactivePeak_, \"Reactive\"), pre_power_peak(powerActivePeak_, \"Active\")\n",
    "\n",
    "    powerReactive = pd.concat([powerReactive, powerReactive_])\n",
    "    powerActive = pd.concat([powerActive, powerActive_])\n",
    "    powerReactivePeak = pd.concat([powerReactivePeak, powerReactivePeak_])\n",
    "    powerActivePeak = pd.concat([powerActivePeak, powerActivePeak_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'municiplality' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m meteo \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/leibniz/Desktop/IHMAN/meteo_raw_data/meteo_\u001b[39m\u001b[39m{\u001b[39;00mmuniciplality\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'municiplality' is not defined"
     ]
    }
   ],
   "source": [
    "meteo = pd.read_csv(f\"/home/leibniz/Desktop/IHMAN/meteo_raw_data/meteo_{municiplality}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42724532"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(readings_dates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output datasets tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessing_results/out_boxes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_boxes \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessing_results/out_boxes.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m out_lights \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mpreprocessing_results/out_lights.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessing_results/out_boxes.csv'"
     ]
    }
   ],
   "source": [
    "out_boxes = pd.read_csv(\"preprocessing_results/out_boxes.csv\")\n",
    "out_lights = pd.read_csv(\"preprocessing_results/out_lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192280"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejorada = out_lights.loc[out_lights[\"location\"] == \"mejorada\"]\n",
    "len(mejorada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155716/1412182631.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_lights_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n"
     ]
    }
   ],
   "source": [
    "raw_eboxes_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_eboxes_alarms.csv\")\n",
    "raw_lights_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n",
    "\n",
    "# raw_nodes = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_nodes.csv\")\n",
    "# raw_readings = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_readings.csv\")\n",
    "# raw_readings_sorted = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_readings_sorted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_meteo = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/meteo_raw_data/new_meteo_mejorada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-3-9'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(raw_meteo[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-1-1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(raw_meteo[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2036-01-21 21:49:44'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(raw_lights_alarms[\"dated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-09-10 18:51:55'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(raw_lights_alarms[\"dated\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9059/2358804247.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_light_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5631094"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_light_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n",
    "len(raw_light_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dated</th>\n",
       "      <th>alarm</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA000000AB62</td>\n",
       "      <td>2015-03-24 18:22:12</td>\n",
       "      <td>lightcomm</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA000000AB62</td>\n",
       "      <td>2015-03-24 18:22:12</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA000000AB63</td>\n",
       "      <td>2020-04-19 18:58:20</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA000000AB63</td>\n",
       "      <td>2020-04-19 18:58:20</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA000000AB63</td>\n",
       "      <td>2020-04-19 18:58:45</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196843</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-02 18:43:16</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196844</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-03 05:50:01</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196845</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-03 05:50:01</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196846</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-03 18:44:16</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196847</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-03 18:44:16</td>\n",
       "      <td>lighterr</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193738 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id               dated      alarm flag\n",
       "0       AA000000AB62 2015-03-24 18:22:12  lightcomm  off\n",
       "1       AA000000AB62 2015-03-24 18:22:12   lighterr  off\n",
       "2       AA000000AB63 2020-04-19 18:58:20   lighterr   on\n",
       "3       AA000000AB63 2020-04-19 18:58:20   lighterr   on\n",
       "4       AA000000AB63 2020-04-19 18:58:45   lighterr  off\n",
       "...              ...                 ...        ...  ...\n",
       "196843  AA00000195F9 2023-04-02 18:43:16   lighterr  off\n",
       "196844  AA00000195F9 2023-04-03 05:50:01   lighterr   on\n",
       "196845  AA00000195F9 2023-04-03 05:50:01   lighterr   on\n",
       "196846  AA00000195F9 2023-04-03 18:44:16   lighterr  off\n",
       "196847  AA00000195F9 2023-04-03 18:44:16   lighterr  off\n",
       "\n",
       "[193738 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_lights_preprocess(raw_light_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = \"2022/06/09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'09'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy_meteo.scrapy_meteo import return_years_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2014\n",
      "in!\n",
      "[4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2014: [4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2015: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2016: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2017: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2018: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2019: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2020: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2021: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2022: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 2023: [1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_years_months(min_date=\"2014-04-01\", max_date=\"2023-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "date.today().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.today().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'illora': ('2015-01-01', '2023-01-01'),\n",
       " 'canyelles': ('2015-01-01', '2023-01-01'),\n",
       " 'mejorada': ('2014-01-01', '2023-01-01')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(\"{'illora': ('2015-01-01', '2023-01-01'), 'canyelles': ('2015-01-01', '2023-01-01'), 'mejorada': ('2014-01-01', '2023-01-01')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273039/815741648.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lights_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n"
     ]
    }
   ],
   "source": [
    "meteo = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/meteo_raw_data/new_meteo_mejorada.csv\")\n",
    "lights_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/raw_data/mejorada_lights_alarms.csv\")\n",
    "\n",
    "meteo = meteo_groupby(first_meteo_preprocess(meteo))\n",
    "lights_alarms = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>Temp_avg</th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Dew_max</th>\n",
       "      <th>Dew_avg</th>\n",
       "      <th>Dew_min</th>\n",
       "      <th>Hum_max</th>\n",
       "      <th>Hum_avg</th>\n",
       "      <th>Hum_min</th>\n",
       "      <th>Wind_max</th>\n",
       "      <th>Wind_avg</th>\n",
       "      <th>Wind_min</th>\n",
       "      <th>Pres_max</th>\n",
       "      <th>Pres_avg</th>\n",
       "      <th>Pres_min</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.277778</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>7.777778</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>100</td>\n",
       "      <td>96.7</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.777778</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>10.277778</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>100</td>\n",
       "      <td>96.2</td>\n",
       "      <td>94</td>\n",
       "      <td>18</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.777778</td>\n",
       "      <td>11.944444</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>10.444444</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>100</td>\n",
       "      <td>91.1</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>10.2</td>\n",
       "      <td>3</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.111111</td>\n",
       "      <td>8.722222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>100</td>\n",
       "      <td>81.1</td>\n",
       "      <td>57</td>\n",
       "      <td>26</td>\n",
       "      <td>16.4</td>\n",
       "      <td>5</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.888889</td>\n",
       "      <td>6.388889</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>7.777778</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>100</td>\n",
       "      <td>86.5</td>\n",
       "      <td>65</td>\n",
       "      <td>14</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>12.611111</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>-6.111111</td>\n",
       "      <td>70</td>\n",
       "      <td>41.2</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>23.888889</td>\n",
       "      <td>13.611111</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.722222</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>36.7</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>26.111111</td>\n",
       "      <td>16.111111</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>40.8</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>22.777778</td>\n",
       "      <td>16.944444</td>\n",
       "      <td>7.777778</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>71</td>\n",
       "      <td>46.4</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>22.222222</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>63</td>\n",
       "      <td>52.9</td>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>12.1</td>\n",
       "      <td>6</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3377 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temp_max   Temp_avg   Temp_min    Dew_max    Dew_avg    Dew_min  \\\n",
       "0     10.000000   8.277778   7.222222   8.888889   7.777778   6.111111   \n",
       "1     12.777778  11.000000   8.888889  12.222222  10.277778   8.888889   \n",
       "2     12.777778  11.944444  11.111111  12.222222  10.444444   8.888889   \n",
       "3     11.111111   8.722222   5.000000  11.111111   4.666667 -17.777778   \n",
       "4      8.888889   6.388889   3.888889   7.777778   3.666667 -17.777778   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3372  20.000000  12.611111   5.000000   0.000000  -1.666667  -6.111111   \n",
       "3373  23.888889  13.611111   2.777778   0.000000  -2.722222  -5.000000   \n",
       "3374  26.111111  16.111111   6.111111   3.888889   1.277778   0.000000   \n",
       "3375  22.777778  16.944444   7.777778   8.888889   4.777778   1.111111   \n",
       "3376  22.222222  17.000000  12.222222  10.000000   7.166667   3.888889   \n",
       "\n",
       "      Hum_max  Hum_avg  Hum_min  Wind_max  Wind_avg  Wind_min  Pres_max  \\\n",
       "0         100     96.7       87        10       7.1         3      28.1   \n",
       "1         100     96.2       94        18      11.7         6      28.0   \n",
       "2         100     91.1       82        15      10.2         3      28.1   \n",
       "3         100     81.1       57        26      16.4         5      28.1   \n",
       "4         100     86.5       65        14       7.8         5      28.2   \n",
       "...       ...      ...      ...       ...       ...       ...       ...   \n",
       "3372       70     41.2       17         7       2.8         0      28.1   \n",
       "3373       70     36.7       14         7       3.1         0      28.1   \n",
       "3374       66     40.8       18        13       5.3         1      28.1   \n",
       "3375       71     46.4       27        22      10.1         0      28.1   \n",
       "3376       63     52.9       38        20      12.1         6      28.1   \n",
       "\n",
       "      Pres_avg  Pres_min  Precipitation       Date  \n",
       "0         28.0      28.0            0.0 2014-01-01  \n",
       "1         27.9      27.9            0.0 2014-01-02  \n",
       "2         28.1      28.0            0.0 2014-01-03  \n",
       "3         27.9      27.8            0.0 2014-01-04  \n",
       "4         28.2      28.1            0.0 2014-01-05  \n",
       "...        ...       ...            ...        ...  \n",
       "3372      28.1      28.0            0.0 2023-03-27  \n",
       "3373      28.1      28.1            0.0 2023-03-28  \n",
       "3374      28.1      28.0            0.0 2023-03-29  \n",
       "3375      28.1      28.0            0.0 2023-03-30  \n",
       "3376      28.1      28.0            0.0 2023-03-31  \n",
       "\n",
       "[3377 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_meteo_preprocess(meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>Temp_avg</th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Dew_max</th>\n",
       "      <th>Dew_avg</th>\n",
       "      <th>Dew_min</th>\n",
       "      <th>Hum_max</th>\n",
       "      <th>Hum_avg</th>\n",
       "      <th>Hum_min</th>\n",
       "      <th>Wind_max</th>\n",
       "      <th>Wind_avg</th>\n",
       "      <th>Wind_min</th>\n",
       "      <th>Pres_max</th>\n",
       "      <th>Pres_avg</th>\n",
       "      <th>Pres_min</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>46.9</td>\n",
       "      <td>45</td>\n",
       "      <td>48</td>\n",
       "      <td>46.0</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>96.7</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>51.8</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>50.5</td>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "      <td>96.2</td>\n",
       "      <td>94</td>\n",
       "      <td>18</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>53.5</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>50.8</td>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "      <td>91.1</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>10.2</td>\n",
       "      <td>3</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>47.7</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>40.4</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>81.1</td>\n",
       "      <td>57</td>\n",
       "      <td>26</td>\n",
       "      <td>16.4</td>\n",
       "      <td>5</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>43.5</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "      <td>38.6</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>86.5</td>\n",
       "      <td>65</td>\n",
       "      <td>14</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-1-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>26</td>\n",
       "      <td>68</td>\n",
       "      <td>54.7</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>41.2</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-3-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>27</td>\n",
       "      <td>75</td>\n",
       "      <td>56.5</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "      <td>27.1</td>\n",
       "      <td>23</td>\n",
       "      <td>70</td>\n",
       "      <td>36.7</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-3-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>61.0</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>34.3</td>\n",
       "      <td>32</td>\n",
       "      <td>66</td>\n",
       "      <td>40.8</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-3-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "      <td>62.5</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>40.6</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "      <td>46.4</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-3-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>30</td>\n",
       "      <td>72</td>\n",
       "      <td>62.6</td>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>44.9</td>\n",
       "      <td>39</td>\n",
       "      <td>63</td>\n",
       "      <td>52.9</td>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>12.1</td>\n",
       "      <td>6</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-3-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3377 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Temp_max  Temp_avg  Temp_min  Dew_max  Dew_avg  Dew_min  \\\n",
       "0              0        50      46.9        45       48     46.0       43   \n",
       "1              1        55      51.8        48       54     50.5       48   \n",
       "2              2        55      53.5        52       54     50.8       48   \n",
       "3              3        52      47.7        41       52     40.4        0   \n",
       "4              4        48      43.5        39       46     38.6        0   \n",
       "...          ...       ...       ...       ...      ...      ...      ...   \n",
       "3372          26        68      54.7        41       32     29.0       21   \n",
       "3373          27        75      56.5        37       32     27.1       23   \n",
       "3374          28        79      61.0        43       39     34.3       32   \n",
       "3375          29        73      62.5        46       48     40.6       34   \n",
       "3376          30        72      62.6        54       50     44.9       39   \n",
       "\n",
       "      Hum_max  Hum_avg  Hum_min  Wind_max  Wind_avg  Wind_min  Pres_max  \\\n",
       "0         100     96.7       87        10       7.1         3      28.1   \n",
       "1         100     96.2       94        18      11.7         6      28.0   \n",
       "2         100     91.1       82        15      10.2         3      28.1   \n",
       "3         100     81.1       57        26      16.4         5      28.1   \n",
       "4         100     86.5       65        14       7.8         5      28.2   \n",
       "...       ...      ...      ...       ...       ...       ...       ...   \n",
       "3372       70     41.2       17         7       2.8         0      28.1   \n",
       "3373       70     36.7       14         7       3.1         0      28.1   \n",
       "3374       66     40.8       18        13       5.3         1      28.1   \n",
       "3375       71     46.4       27        22      10.1         0      28.1   \n",
       "3376       63     52.9       38        20      12.1         6      28.1   \n",
       "\n",
       "      Pres_avg  Pres_min  Precipitation       Date  \n",
       "0         28.0      28.0            0.0   2014-1-1  \n",
       "1         27.9      27.9            0.0   2014-1-2  \n",
       "2         28.1      28.0            0.0   2014-1-3  \n",
       "3         27.9      27.8            0.0   2014-1-4  \n",
       "4         28.2      28.1            0.0   2014-1-5  \n",
       "...        ...       ...            ...        ...  \n",
       "3372      28.1      28.0            0.0  2023-3-27  \n",
       "3373      28.1      28.1            0.0  2023-3-28  \n",
       "3374      28.1      28.0            0.0  2023-3-29  \n",
       "3375      28.1      28.0            0.0  2023-3-30  \n",
       "3376      28.1      28.0            0.0  2023-3-31  \n",
       "\n",
       "[3377 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-09-10 18:51:55'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lights_alarms[\"dated\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2036-01-21 21:49:44'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lights_alarms[\"dated\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lights = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/preprocessing_results/out_lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314413"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_lights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                             0\n",
       "id                                     0\n",
       "week-4                                 0\n",
       "hours_week-4                           0\n",
       "week-3                                 0\n",
       "hours_week-3                           0\n",
       "week-2                                 0\n",
       "hours_week-2                           0\n",
       "week-1                                 0\n",
       "hours_week-1                           0\n",
       "current_week                           0\n",
       "hours_current_week                     0\n",
       "week+1                                 0\n",
       "hours_week+1                           0\n",
       "week+2                                 0\n",
       "hours_week+2                           0\n",
       "week+3                                 0\n",
       "hours_week+3                           0\n",
       "week+4                                 0\n",
       "hours_week+4                           0\n",
       "type                               48642\n",
       "ebox_id                            57189\n",
       "lat                                48642\n",
       "lon                                48642\n",
       "ReactivePeak_week-4               192681\n",
       "ReactivePeak_week-3               193006\n",
       "ReactivePeak_week-2               193331\n",
       "ReactivePeak_week-1               193656\n",
       "ReactivePeak_current_week         193981\n",
       "powerReactive_sum_week-4          192707\n",
       "powerReactive_p1_week-4           192707\n",
       "powerReactive_p2_week-4           192707\n",
       "powerReactive_p3_week-4           192707\n",
       "powerReactive_sum_week-3          193032\n",
       "powerReactive_p1_week-3           193032\n",
       "powerReactive_p2_week-3           193032\n",
       "powerReactive_p3_week-3           193032\n",
       "powerReactive_sum_week-2          193357\n",
       "powerReactive_p1_week-2           193357\n",
       "powerReactive_p2_week-2           193357\n",
       "powerReactive_p3_week-2           193357\n",
       "powerReactive_sum_week-1          193682\n",
       "powerReactive_p1_week-1           193682\n",
       "powerReactive_p2_week-1           193682\n",
       "powerReactive_p3_week-1           193682\n",
       "powerReactive_sum_current_week    194007\n",
       "powerReactive_p1_current_week     194007\n",
       "powerReactive_p2_current_week     194007\n",
       "powerReactive_p3_current_week     194007\n",
       "ActivePeak_week-4                 189811\n",
       "ActivePeak_week-3                 190136\n",
       "ActivePeak_week-2                 190461\n",
       "ActivePeak_week-1                 190786\n",
       "ActivePeak_current_week           191111\n",
       "powerActive_sum_week-4            192681\n",
       "powerActive_p1_week-4             192681\n",
       "powerActive_p2_week-4             192681\n",
       "powerActive_p3_week-4             192681\n",
       "powerActive_sum_week-3            193006\n",
       "powerActive_p1_week-3             193006\n",
       "powerActive_p2_week-3             193006\n",
       "powerActive_p3_week-3             193006\n",
       "powerActive_sum_week-2            193331\n",
       "powerActive_p1_week-2             193331\n",
       "powerActive_p2_week-2             193331\n",
       "powerActive_p3_week-2             193331\n",
       "powerActive_sum_week-1            193656\n",
       "powerActive_p1_week-1             193656\n",
       "powerActive_p2_week-1             193656\n",
       "powerActive_p3_week-1             193656\n",
       "powerActive_sum_current_week      193981\n",
       "powerActive_p1_current_week       193981\n",
       "powerActive_p2_current_week       193981\n",
       "powerActive_p3_current_week       193981\n",
       "Temp_max_max_week-4               314413\n",
       "Temp_avg_avg_week-4               314413\n",
       "Temp_avg_std_week-4               314413\n",
       "Temp_min_min_week-4               314413\n",
       "Dew_max_max_week-4                314413\n",
       "Dew_avg_avg_week-4                314413\n",
       "Dew_avg_std_week-4                314413\n",
       "Dew_min_min_week-4                314413\n",
       "Hum_max_max_week-4                314413\n",
       "Hum_avg_avg_week-4                314413\n",
       "Hum_avg_std_week-4                314413\n",
       "Hum_min_min_week-4                314413\n",
       "Wind_max_max_week-4               314413\n",
       "Wind_avg_avg_week-4               314413\n",
       "Wind_avg_std_week-4               314413\n",
       "Wind_min_min_week-4               314413\n",
       "Pres_max_max_week-4               314413\n",
       "Pres_avg_avg_week-4               314413\n",
       "Pres_avg_std_week-4               314413\n",
       "Pres_min_min_week-4               314413\n",
       "Precipitation_avg_week-4          314413\n",
       "Precipitation_sum_week-4          314413\n",
       "Temp_max_max_week-3               314413\n",
       "Temp_avg_avg_week-3               314413\n",
       "Temp_avg_std_week-3               314413\n",
       "Temp_min_min_week-3               314413\n",
       "Dew_max_max_week-3                314413\n",
       "Dew_avg_avg_week-3                314413\n",
       "Dew_avg_std_week-3                314413\n",
       "Dew_min_min_week-3                314413\n",
       "Hum_max_max_week-3                314413\n",
       "Hum_avg_avg_week-3                314413\n",
       "Hum_avg_std_week-3                314413\n",
       "Hum_min_min_week-3                314413\n",
       "Wind_max_max_week-3               314413\n",
       "Wind_avg_avg_week-3               314413\n",
       "Wind_avg_std_week-3               314413\n",
       "Wind_min_min_week-3               314413\n",
       "Pres_max_max_week-3               314413\n",
       "Pres_avg_avg_week-3               314413\n",
       "Pres_avg_std_week-3               314413\n",
       "Pres_min_min_week-3               314413\n",
       "Precipitation_avg_week-3          314413\n",
       "Precipitation_sum_week-3          314413\n",
       "Temp_max_max_week-2               314413\n",
       "Temp_avg_avg_week-2               314413\n",
       "Temp_avg_std_week-2               314413\n",
       "Temp_min_min_week-2               314413\n",
       "Dew_max_max_week-2                314413\n",
       "Dew_avg_avg_week-2                314413\n",
       "Dew_avg_std_week-2                314413\n",
       "Dew_min_min_week-2                314413\n",
       "Hum_max_max_week-2                314413\n",
       "Hum_avg_avg_week-2                314413\n",
       "Hum_avg_std_week-2                314413\n",
       "Hum_min_min_week-2                314413\n",
       "Wind_max_max_week-2               314413\n",
       "Wind_avg_avg_week-2               314413\n",
       "Wind_avg_std_week-2               314413\n",
       "Wind_min_min_week-2               314413\n",
       "Pres_max_max_week-2               314413\n",
       "Pres_avg_avg_week-2               314413\n",
       "Pres_avg_std_week-2               314413\n",
       "Pres_min_min_week-2               314413\n",
       "Precipitation_avg_week-2          314413\n",
       "Precipitation_sum_week-2          314413\n",
       "Temp_max_max_week-1               314413\n",
       "Temp_avg_avg_week-1               314413\n",
       "Temp_avg_std_week-1               314413\n",
       "Temp_min_min_week-1               314413\n",
       "Dew_max_max_week-1                314413\n",
       "Dew_avg_avg_week-1                314413\n",
       "Dew_avg_std_week-1                314413\n",
       "Dew_min_min_week-1                314413\n",
       "Hum_max_max_week-1                314413\n",
       "Hum_avg_avg_week-1                314413\n",
       "Hum_avg_std_week-1                314413\n",
       "Hum_min_min_week-1                314413\n",
       "Wind_max_max_week-1               314413\n",
       "Wind_avg_avg_week-1               314413\n",
       "Wind_avg_std_week-1               314413\n",
       "Wind_min_min_week-1               314413\n",
       "Pres_max_max_week-1               314413\n",
       "Pres_avg_avg_week-1               314413\n",
       "Pres_avg_std_week-1               314413\n",
       "Pres_min_min_week-1               314413\n",
       "Precipitation_avg_week-1          314413\n",
       "Precipitation_sum_week-1          314413\n",
       "Temp_max_max_current_week         314413\n",
       "Temp_avg_avg_current_week         314413\n",
       "Temp_avg_std_current_week         314413\n",
       "Temp_min_min_current_week         314413\n",
       "Dew_max_max_current_week          314413\n",
       "Dew_avg_avg_current_week          314413\n",
       "Dew_avg_std_current_week          314413\n",
       "Dew_min_min_current_week          314413\n",
       "Hum_max_max_current_week          314413\n",
       "Hum_avg_avg_current_week          314413\n",
       "Hum_avg_std_current_week          314413\n",
       "Hum_min_min_current_week          314413\n",
       "Wind_max_max_current_week         314413\n",
       "Wind_avg_avg_current_week         314413\n",
       "Wind_avg_std_current_week         314413\n",
       "Wind_min_min_current_week         314413\n",
       "Pres_max_max_current_week         314413\n",
       "Pres_avg_avg_current_week         314413\n",
       "Pres_avg_std_current_week         314413\n",
       "Pres_min_min_current_week         314413\n",
       "Precipitation_avg_current_week    314413\n",
       "Precipitation_sum_current_week    314413\n",
       "location                               0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(preprocessed_lights.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = preprocessed_lights.loc[preprocessed_lights[\"Precipitation_sum_current_week\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-10-06'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[\"current_week\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-06-01'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[\"current_week\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-10-06'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[\"current_week\"].min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuck it! Let's go step by step to see where the code is failing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/leibniz/Desktop/IHMAN\"\n",
    "municipality = \"canyelles\"\n",
    "n_weeks = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canyelles\n",
      "2015-1-1\n",
      "2023-4-9\n",
      "illora\n",
      "2015-1-1\n",
      "2023-4-9\n",
      "mejorada\n",
      "2014-1-1\n",
      "2023-4-9\n"
     ]
    }
   ],
   "source": [
    "for municipality in [\"canyelles\", \"illora\", \"mejorada\"]:\n",
    "    meteo = pd.read_csv(f\"{data_dir}/meteo_raw_data/new_meteo_{municipality}.csv\")\n",
    "    \n",
    "    print(municipality)\n",
    "    print(meteo[\"Date\"].min())\n",
    "    print(meteo[\"Date\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = first_meteo_preprocess(meteo=meteo, filter_dates=True, date_min=\"2013-12-20\", date_max=\"2023-12-25\")\n",
    "meteo = meteo_groupby(meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerReactive, powerActive, powerReactivePeak, powerActivePeak = pre_readings_batches(splitting_indexes, municipality, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights_alarms = first_lights_preprocess(lights_alarms)\n",
    "eboxes_alarms = first_eboxes_preprocess(eboxes_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA0000013912\n",
      "AA0000013926\n",
      "AA0000013931\n",
      "AA000001393D\n",
      "AA000001393E\n",
      "AA0000016F53\n",
      "AA000001726C\n",
      "AA0000017324\n",
      "AA00000184EB\n",
      "AA0000018505\n",
      "AA0000018696\n",
      "AA0000018697\n",
      "AA0000018698\n",
      "AA0000018699\n",
      "AA00000186B4\n",
      "AA00000186D0\n",
      "AA0000018704\n",
      "AA0000018705\n",
      "AA0000018706\n",
      "AA0000018719\n",
      "AA0000018745\n",
      "AA0000018765\n",
      "AA0000018769\n",
      "AA000001876F\n",
      "AA000001877F\n",
      "AA0000018978\n",
      "AA0000018981\n",
      "AA00000189AC\n",
      "AA00000189AD\n",
      "AA00000189B8\n",
      "AA00000189B9\n",
      "AA00000189C6\n",
      "AA00000189CB\n",
      "AA00000189E7\n",
      "AA0000018A0B\n",
      "AA0000018A38\n",
      "AA0000018A54\n",
      "AA0000018A60\n",
      "AA0000018A6C\n",
      "AA0000018B4F\n",
      "AA0000018B5F\n",
      "AA0000018B99\n",
      "AA0000018BA7\n",
      "AA0000018BA8\n",
      "AA0000018BB7\n",
      "AA0000018C28\n",
      "AA0000018C65\n",
      "AA0000018C9B\n",
      "AA0000018D53\n",
      "AA0000018D6A\n",
      "AA0000018D78\n",
      "AA0000018D7B\n",
      "AA0000018D85\n",
      "AA0000018D87\n",
      "AA0000018D89\n",
      "AA0000018D8A\n",
      "AA0000018D8C\n",
      "AA0000018D93\n",
      "AA0000018D9D\n",
      "AA0000018DA1\n",
      "AA0000018DA8\n",
      "AA0000018DAB\n",
      "AA0000018DB1\n",
      "AA0000018DBD\n",
      "AA0000018DD0\n",
      "AA0000018DD8\n",
      "AA0000018DDF\n",
      "AA0000018DE3\n",
      "AA0000018DEF\n",
      "AA0000018DF7\n",
      "AA0000018DFC\n",
      "AA0000018E0C\n",
      "AA0000018E10\n",
      "AA0000018E9D\n",
      "AA0000018EB4\n",
      "AA0000018EC0\n",
      "AA0000018ECB\n",
      "AA0000018ED0\n",
      "AA0000018ED2\n",
      "AA0000018EE1\n",
      "AA0000018EE2\n",
      "AA0000018EF0\n",
      "AA0000018EF2\n",
      "AA0000018EF4\n",
      "AA0000018EF9\n",
      "AA0000018F04\n",
      "AA0000018F07\n",
      "AA0000018F0A\n",
      "AA0000018F0F\n",
      "AA0000018F17\n",
      "AA0000018F1F\n",
      "AA0000018F21\n",
      "AA0000018F28\n",
      "AA0000018F2A\n",
      "AA0000018F31\n",
      "AA0000018F41\n",
      "AA0000018F53\n",
      "AA0000018F78\n",
      "AA0000018F83\n",
      "AA0000018F91\n",
      "AA0000018F95\n",
      "AA0000018F9B\n",
      "AA0000018F9C\n",
      "AA0000018FAC\n",
      "AA0000018FAD\n",
      "AA0000018FAE\n",
      "AA0000018FBB\n",
      "AA0000018FBC\n",
      "AA0000018FBD\n",
      "AA0000018FC1\n",
      "AA0000018FF0\n",
      "AA0000019033\n",
      "AA00000191C1\n",
      "AA00000191D0\n",
      "AA00000191F4\n",
      "AA0000019204\n",
      "AA0000019252\n",
      "AA0000019261\n",
      "AA000001926A\n",
      "AA000001926D\n",
      "AA0000019277\n",
      "AA000001927D\n",
      "AA00000192DE\n",
      "AA0000019325\n",
      "AA0000019329\n",
      "AA0000019350\n",
      "AA0000019352\n",
      "AA0000019356\n",
      "AA000001935D\n",
      "AA0000019365\n",
      "AA000001936A\n",
      "AA000001937B\n",
      "AA0000019383\n",
      "AA0000019387\n",
      "AA000001938E\n",
      "AA00000193A8\n",
      "AA0000019689\n",
      "AA000001971E\n",
      "AA000001CEEC\n",
      "Execution time:93.77067184448242\n",
      "AA000000D44C\n",
      "AA0000011276\n",
      "AA0000011277\n",
      "AA000001128F\n",
      "AA0000011290\n",
      "AA00000112BE\n",
      "AA00000112CB\n",
      "AA00000112CD\n",
      "AA000001167D\n",
      "AA0000011681\n",
      "AA000001169B\n",
      "Execution time:10.256747722625732\n"
     ]
    }
   ],
   "source": [
    "lights_alarms = big_preprocess_lights(lights_alarms)\n",
    "eboxes_alarms = big_preprocess_eboxes(eboxes_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights_alarms = pd.merge(lights_alarms, nodes, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights_alarms = join_light_alarms_readings_meteo(\n",
    "    light_errors = lights_alarms,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                0\n",
       "week-4                            0\n",
       "hours_week-4                      0\n",
       "week-3                            0\n",
       "hours_week-3                      0\n",
       "                                 ..\n",
       "Pres_avg_avg_current_week         0\n",
       "Pres_avg_std_current_week         0\n",
       "Pres_min_min_current_week         0\n",
       "Precipitation_avg_current_week    0\n",
       "Precipitation_sum_current_week    0\n",
       "Length: 183, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lights_alarms.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25993"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lights_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp_max_max</th>\n",
       "      <th>Temp_avg_mean</th>\n",
       "      <th>Temp_avg_std</th>\n",
       "      <th>Temp_min_min</th>\n",
       "      <th>Dew_max_max</th>\n",
       "      <th>Dew_avg_mean</th>\n",
       "      <th>Dew_avg_std</th>\n",
       "      <th>Dew_min_min</th>\n",
       "      <th>Hum_max_max</th>\n",
       "      <th>Hum_avg_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Wind_avg_mean</th>\n",
       "      <th>Wind_avg_std</th>\n",
       "      <th>Wind_min_min</th>\n",
       "      <th>Pres_max_max</th>\n",
       "      <th>Pres_avg_mean</th>\n",
       "      <th>Pres_avg_std</th>\n",
       "      <th>Pres_min_min</th>\n",
       "      <th>Precipitation_mean</th>\n",
       "      <th>Precipitation_sum</th>\n",
       "      <th>dated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>8.785714</td>\n",
       "      <td>1.262969</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.634921</td>\n",
       "      <td>2.459729</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>93</td>\n",
       "      <td>67.485714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.871429</td>\n",
       "      <td>0.930438</td>\n",
       "      <td>1</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.442857</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>9.825397</td>\n",
       "      <td>1.715167</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>5.603175</td>\n",
       "      <td>2.086461</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>94</td>\n",
       "      <td>75.728571</td>\n",
       "      <td>...</td>\n",
       "      <td>11.285714</td>\n",
       "      <td>3.068892</td>\n",
       "      <td>0</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.428571</td>\n",
       "      <td>0.179947</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.063492</td>\n",
       "      <td>1.462645</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>2.499324</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>100</td>\n",
       "      <td>77.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.785714</td>\n",
       "      <td>1.366783</td>\n",
       "      <td>0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>29.885714</td>\n",
       "      <td>0.146385</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>1.418934</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.595238</td>\n",
       "      <td>3.009196</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>87</td>\n",
       "      <td>56.114286</td>\n",
       "      <td>...</td>\n",
       "      <td>11.271429</td>\n",
       "      <td>3.034092</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>30.057143</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.777778</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>3.076193</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.202692</td>\n",
       "      <td>-3.888889</td>\n",
       "      <td>93</td>\n",
       "      <td>65.085714</td>\n",
       "      <td>...</td>\n",
       "      <td>13.114286</td>\n",
       "      <td>3.745855</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.457143</td>\n",
       "      <td>0.214920</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>18.888889</td>\n",
       "      <td>12.023810</td>\n",
       "      <td>1.523093</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>7.873016</td>\n",
       "      <td>1.477442</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>100</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>1.145176</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>30.242857</td>\n",
       "      <td>0.127242</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>17.222222</td>\n",
       "      <td>10.706349</td>\n",
       "      <td>1.953870</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.968254</td>\n",
       "      <td>1.631057</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>88</td>\n",
       "      <td>69.385714</td>\n",
       "      <td>...</td>\n",
       "      <td>9.514286</td>\n",
       "      <td>1.475837</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.157143</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>16.111111</td>\n",
       "      <td>10.785714</td>\n",
       "      <td>1.563613</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>4.007937</td>\n",
       "      <td>3.081110</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>94</td>\n",
       "      <td>64.814286</td>\n",
       "      <td>...</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>5.014645</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>0.404145</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>13.888889</td>\n",
       "      <td>6.619048</td>\n",
       "      <td>1.889006</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>-2.388889</td>\n",
       "      <td>3.175912</td>\n",
       "      <td>-12.222222</td>\n",
       "      <td>81</td>\n",
       "      <td>54.871429</td>\n",
       "      <td>...</td>\n",
       "      <td>11.271429</td>\n",
       "      <td>3.436914</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>30.042857</td>\n",
       "      <td>0.151186</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>12.777778</td>\n",
       "      <td>6.203704</td>\n",
       "      <td>0.680111</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.481481</td>\n",
       "      <td>0.940624</td>\n",
       "      <td>-3.888889</td>\n",
       "      <td>81</td>\n",
       "      <td>63.433333</td>\n",
       "      <td>...</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>1.349074</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.116667</td>\n",
       "      <td>0.172240</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Temp_max_max  Temp_avg_mean  Temp_avg_std  Temp_min_min  Dew_max_max  \\\n",
       "0       17.222222       8.785714      1.262969      2.777778    10.000000   \n",
       "1       17.222222       9.825397      1.715167      3.888889    12.222222   \n",
       "2       15.000000       8.063492      1.462645      2.222222    12.222222   \n",
       "3       15.000000       8.500000      1.418934      2.222222     5.000000   \n",
       "4       17.777778       8.777778      3.076193      2.222222     8.888889   \n",
       "..            ...            ...           ...           ...          ...   \n",
       "417     18.888889      12.023810      1.523093      7.222222    13.888889   \n",
       "418     17.222222      10.706349      1.953870      3.888889    12.222222   \n",
       "419     16.111111      10.785714      1.563613      5.000000    12.222222   \n",
       "420     13.888889       6.619048      1.889006     -1.111111     6.111111   \n",
       "421     12.777778       6.203704      0.680111      2.222222     5.000000   \n",
       "\n",
       "     Dew_avg_mean  Dew_avg_std  Dew_min_min  Hum_max_max  Hum_avg_mean  ...  \\\n",
       "0        2.634921     2.459729    -5.000000           93     67.485714  ...   \n",
       "1        5.603175     2.086461     1.111111           94     75.728571  ...   \n",
       "2        4.095238     2.499324    -1.111111          100     77.200000  ...   \n",
       "3       -0.595238     3.009196   -13.888889           87     56.114286  ...   \n",
       "4        2.111111     2.202692    -3.888889           93     65.085714  ...   \n",
       "..            ...          ...          ...          ...           ...  ...   \n",
       "417      7.873016     1.477442     2.777778          100     76.500000  ...   \n",
       "418      4.968254     1.631057     1.111111           88     69.385714  ...   \n",
       "419      4.007937     3.081110    -7.222222           94     64.814286  ...   \n",
       "420     -2.388889     3.175912   -12.222222           81     54.871429  ...   \n",
       "421     -0.481481     0.940624    -3.888889           81     63.433333  ...   \n",
       "\n",
       "     Wind_avg_mean  Wind_avg_std  Wind_min_min  Pres_max_max  Pres_avg_mean  \\\n",
       "0        10.871429      0.930438             1          30.6      30.442857   \n",
       "1        11.285714      3.068892             0          30.7      30.428571   \n",
       "2         9.785714      1.366783             0          30.2      29.885714   \n",
       "3        11.271429      3.034092             0          30.3      30.057143   \n",
       "4        13.114286      3.745855             1          30.0      29.457143   \n",
       "..             ...           ...           ...           ...            ...   \n",
       "417       7.714286      1.145176             0          30.5      30.242857   \n",
       "418       9.514286      1.475837             0          30.4      30.157143   \n",
       "419      12.400000      5.014645             0          30.4      29.900000   \n",
       "420      11.271429      3.436914             0          30.3      30.042857   \n",
       "421       9.700000      1.349074             0          30.4      30.116667   \n",
       "\n",
       "     Pres_avg_std  Pres_min_min  Precipitation_mean  Precipitation_sum  \\\n",
       "0        0.097590          30.2                 0.0                0.0   \n",
       "1        0.179947          30.2                 0.0                0.0   \n",
       "2        0.146385          29.7                 0.0                0.0   \n",
       "3        0.171825          29.7                 0.0                0.0   \n",
       "4        0.214920          29.1                 0.0                0.0   \n",
       "..            ...           ...                 ...                ...   \n",
       "417      0.127242          30.1                 0.0                0.0   \n",
       "418      0.171825          29.8                 0.0                0.0   \n",
       "419      0.404145          29.2                 0.0                0.0   \n",
       "420      0.151186          29.6                 0.0                0.0   \n",
       "421      0.172240          29.9                 0.0                0.0   \n",
       "\n",
       "         dated  \n",
       "0   2015-01-01  \n",
       "1   2015-01-08  \n",
       "2   2015-01-15  \n",
       "3   2015-01-22  \n",
       "4   2015-01-29  \n",
       "..         ...  \n",
       "417 2022-12-29  \n",
       "418 2023-01-05  \n",
       "419 2023-01-12  \n",
       "420 2023-01-19  \n",
       "421 2023-01-26  \n",
       "\n",
       "[422 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/preprocessing_results/out_lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                             0\n",
       "id                                     0\n",
       "week-4                                 0\n",
       "hours_week-4                           0\n",
       "week-3                                 0\n",
       "hours_week-3                           0\n",
       "week-2                                 0\n",
       "hours_week-2                           0\n",
       "week-1                                 0\n",
       "hours_week-1                           0\n",
       "current_week                           0\n",
       "hours_current_week                     0\n",
       "week+1                                 0\n",
       "hours_week+1                           0\n",
       "week+2                                 0\n",
       "hours_week+2                           0\n",
       "week+3                                 0\n",
       "hours_week+3                           0\n",
       "week+4                                 0\n",
       "hours_week+4                           0\n",
       "type                               48642\n",
       "ebox_id                            57189\n",
       "lat                                48642\n",
       "lon                                48642\n",
       "ReactivePeak_week-4               192681\n",
       "ReactivePeak_week-3               193006\n",
       "ReactivePeak_week-2               193331\n",
       "ReactivePeak_week-1               193656\n",
       "ReactivePeak_current_week         193981\n",
       "powerReactive_sum_week-4          192707\n",
       "powerReactive_p1_week-4           192707\n",
       "powerReactive_p2_week-4           192707\n",
       "powerReactive_p3_week-4           192707\n",
       "powerReactive_sum_week-3          193032\n",
       "powerReactive_p1_week-3           193032\n",
       "powerReactive_p2_week-3           193032\n",
       "powerReactive_p3_week-3           193032\n",
       "powerReactive_sum_week-2          193357\n",
       "powerReactive_p1_week-2           193357\n",
       "powerReactive_p2_week-2           193357\n",
       "powerReactive_p3_week-2           193357\n",
       "powerReactive_sum_week-1          193682\n",
       "powerReactive_p1_week-1           193682\n",
       "powerReactive_p2_week-1           193682\n",
       "powerReactive_p3_week-1           193682\n",
       "powerReactive_sum_current_week    194007\n",
       "powerReactive_p1_current_week     194007\n",
       "powerReactive_p2_current_week     194007\n",
       "powerReactive_p3_current_week     194007\n",
       "ActivePeak_week-4                 189811\n",
       "ActivePeak_week-3                 190136\n",
       "ActivePeak_week-2                 190461\n",
       "ActivePeak_week-1                 190786\n",
       "ActivePeak_current_week           191111\n",
       "powerActive_sum_week-4            192681\n",
       "powerActive_p1_week-4             192681\n",
       "powerActive_p2_week-4             192681\n",
       "powerActive_p3_week-4             192681\n",
       "powerActive_sum_week-3            193006\n",
       "powerActive_p1_week-3             193006\n",
       "powerActive_p2_week-3             193006\n",
       "powerActive_p3_week-3             193006\n",
       "powerActive_sum_week-2            193331\n",
       "powerActive_p1_week-2             193331\n",
       "powerActive_p2_week-2             193331\n",
       "powerActive_p3_week-2             193331\n",
       "powerActive_sum_week-1            193656\n",
       "powerActive_p1_week-1             193656\n",
       "powerActive_p2_week-1             193656\n",
       "powerActive_p3_week-1             193656\n",
       "powerActive_sum_current_week      193981\n",
       "powerActive_p1_current_week       193981\n",
       "powerActive_p2_current_week       193981\n",
       "powerActive_p3_current_week       193981\n",
       "Temp_max_max_week-4                    0\n",
       "Temp_avg_avg_week-4                    0\n",
       "Temp_avg_std_week-4                    0\n",
       "Temp_min_min_week-4                    0\n",
       "Dew_max_max_week-4                     0\n",
       "Dew_avg_avg_week-4                     0\n",
       "Dew_avg_std_week-4                     0\n",
       "Dew_min_min_week-4                     0\n",
       "Hum_max_max_week-4                     0\n",
       "Hum_avg_avg_week-4                     0\n",
       "Hum_avg_std_week-4                     0\n",
       "Hum_min_min_week-4                     0\n",
       "Wind_max_max_week-4                    0\n",
       "Wind_avg_avg_week-4                    0\n",
       "Wind_avg_std_week-4                    0\n",
       "Wind_min_min_week-4                    0\n",
       "Pres_max_max_week-4                    0\n",
       "Pres_avg_avg_week-4                    0\n",
       "Pres_avg_std_week-4                    0\n",
       "Pres_min_min_week-4                    0\n",
       "Precipitation_avg_week-4               0\n",
       "Precipitation_sum_week-4               0\n",
       "Temp_max_max_week-3                    0\n",
       "Temp_avg_avg_week-3                    0\n",
       "Temp_avg_std_week-3                    0\n",
       "Temp_min_min_week-3                    0\n",
       "Dew_max_max_week-3                     0\n",
       "Dew_avg_avg_week-3                     0\n",
       "Dew_avg_std_week-3                     0\n",
       "Dew_min_min_week-3                     0\n",
       "Hum_max_max_week-3                     0\n",
       "Hum_avg_avg_week-3                     0\n",
       "Hum_avg_std_week-3                     0\n",
       "Hum_min_min_week-3                     0\n",
       "Wind_max_max_week-3                    0\n",
       "Wind_avg_avg_week-3                    0\n",
       "Wind_avg_std_week-3                    0\n",
       "Wind_min_min_week-3                    0\n",
       "Pres_max_max_week-3                    0\n",
       "Pres_avg_avg_week-3                    0\n",
       "Pres_avg_std_week-3                    0\n",
       "Pres_min_min_week-3                    0\n",
       "Precipitation_avg_week-3               0\n",
       "Precipitation_sum_week-3               0\n",
       "Temp_max_max_week-2                    0\n",
       "Temp_avg_avg_week-2                    0\n",
       "Temp_avg_std_week-2                    0\n",
       "Temp_min_min_week-2                    0\n",
       "Dew_max_max_week-2                     0\n",
       "Dew_avg_avg_week-2                     0\n",
       "Dew_avg_std_week-2                     0\n",
       "Dew_min_min_week-2                     0\n",
       "Hum_max_max_week-2                     0\n",
       "Hum_avg_avg_week-2                     0\n",
       "Hum_avg_std_week-2                     0\n",
       "Hum_min_min_week-2                     0\n",
       "Wind_max_max_week-2                    0\n",
       "Wind_avg_avg_week-2                    0\n",
       "Wind_avg_std_week-2                    0\n",
       "Wind_min_min_week-2                    0\n",
       "Pres_max_max_week-2                    0\n",
       "Pres_avg_avg_week-2                    0\n",
       "Pres_avg_std_week-2                    0\n",
       "Pres_min_min_week-2                    0\n",
       "Precipitation_avg_week-2               0\n",
       "Precipitation_sum_week-2               0\n",
       "Temp_max_max_week-1                    0\n",
       "Temp_avg_avg_week-1                    0\n",
       "Temp_avg_std_week-1                    0\n",
       "Temp_min_min_week-1                    0\n",
       "Dew_max_max_week-1                     0\n",
       "Dew_avg_avg_week-1                     0\n",
       "Dew_avg_std_week-1                     0\n",
       "Dew_min_min_week-1                     0\n",
       "Hum_max_max_week-1                     0\n",
       "Hum_avg_avg_week-1                     0\n",
       "Hum_avg_std_week-1                     0\n",
       "Hum_min_min_week-1                     0\n",
       "Wind_max_max_week-1                    0\n",
       "Wind_avg_avg_week-1                    0\n",
       "Wind_avg_std_week-1                    0\n",
       "Wind_min_min_week-1                    0\n",
       "Pres_max_max_week-1                    0\n",
       "Pres_avg_avg_week-1                    0\n",
       "Pres_avg_std_week-1                    0\n",
       "Pres_min_min_week-1                    0\n",
       "Precipitation_avg_week-1               0\n",
       "Precipitation_sum_week-1               0\n",
       "Temp_max_max_current_week              0\n",
       "Temp_avg_avg_current_week              0\n",
       "Temp_avg_std_current_week              0\n",
       "Temp_min_min_current_week              0\n",
       "Dew_max_max_current_week               0\n",
       "Dew_avg_avg_current_week               0\n",
       "Dew_avg_std_current_week               0\n",
       "Dew_min_min_current_week               0\n",
       "Hum_max_max_current_week               0\n",
       "Hum_avg_avg_current_week               0\n",
       "Hum_avg_std_current_week               0\n",
       "Hum_min_min_current_week               0\n",
       "Wind_max_max_current_week              0\n",
       "Wind_avg_avg_current_week              0\n",
       "Wind_avg_std_current_week              0\n",
       "Wind_min_min_current_week              0\n",
       "Pres_max_max_current_week              0\n",
       "Pres_avg_avg_current_week              0\n",
       "Pres_avg_std_current_week              0\n",
       "Pres_min_min_current_week              0\n",
       "Precipitation_avg_current_week         0\n",
       "Precipitation_sum_current_week         0\n",
       "location                               0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(light_alarms.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_light = light_alarms.loc[light_alarms[\"Pres_min_min_current_week\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-03 00:00:00')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eboxes_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-06'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nans_light[\"current_week\"].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I need the last 4 weeks of readings, errors and all the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:00:11\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/leibniz/Desktop/IHMAN\"\n",
    "municipality = \"mejorada\"\n",
    "# Get the last 4 weeks of alarms, readings and the nodes\n",
    "\n",
    "eboxes_alarms = pd.read_csv(f\"{data_dir}/raw_data/{municipality}_eboxes_alarms.csv\")\n",
    "light_alarms = pd.read_csv(f\"{data_dir}/raw_data/{municipality}_lights_alarms.csv\", usecols=lambda column: column != \"info_value\")\n",
    "eboxes_alarms = first_eboxes_preprocess(eboxes_alarms)\n",
    "light_alarms = first_lights_preprocess(light_alarms)\n",
    "\n",
    "eboxes_alarms[\"dated\"] = pd.to_datetime(eboxes_alarms[\"dated\"])\n",
    "light_alarms[\"dated\"] = pd.to_datetime(light_alarms[\"dated\"])\n",
    "\n",
    "print(light_alarms[\"dated\"].max())\n",
    "# eboxes: 30-12-2019 to 02-02-2020\n",
    "# lights: 27-02-2023 to 02-04-2023\n",
    "eboxes_low = pd.to_datetime(\"2019-12-30\")\n",
    "eboxes_top = pd.to_datetime(\"2020-02-02\")\n",
    "light_low = pd.to_datetime(\"2023-02-27\")\n",
    "light_top = pd.to_datetime(\"2023-04-02\")\n",
    "\n",
    "eboxes_alarms = eboxes_alarms.loc[(eboxes_low <= eboxes_alarms[\"dated\"]) & (eboxes_alarms[\"dated\"] < eboxes_top)]\n",
    "light_alarms = light_alarms.loc[(light_low <= light_alarms[\"dated\"]) & (light_alarms[\"dated\"] < light_top)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just load the whole mejorada meteo data and get the dates from the ranges we need:\n",
    "meteo = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/meteo_raw_data/new_meteo_mejorada.csv\")\n",
    "meteo[\"Date\"] = pd.to_datetime(meteo[\"Date\"])\n",
    "\n",
    "eboxes_meteo = meteo.loc[(eboxes_low <= meteo[\"Date\"]) & (meteo[\"Date\"] < eboxes_top)]\n",
    "light_meteo = meteo.loc[(light_low <= meteo[\"Date\"]) & (meteo[\"Date\"] < light_top)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store this dataframes to preprocess them in the predict_main.py module:\n",
    "eboxes_alarms.to_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/eboxes_alarms_to_predict.csv\")\n",
    "light_alarms.to_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/light_alarms_to_predict.csv\")\n",
    "\n",
    "eboxes_meteo.to_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/eboxes_meteo_to_predict.csv\")\n",
    "light_meteo.to_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/light_meteo_to_predict.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from predictive_models.predictive_models_functions import adboc_predict\n",
    "from preprocessing_functions.functions_preprocessing_alarms import first_eboxes_preprocess, first_lights_preprocess, big_preprocess_eboxes, big_preprocess_lights\n",
    "from preprocessing_functions.functions_preprocessing_readings import split_readings, pre_power, pre_power_peak, get_indexes_split, pre_readings_batches\n",
    "from preprocessing_functions.functions_preprocessing_meteo import first_meteo_preprocess, meteo_groupby\n",
    "from preprocessing_functions.functions_joins import join_light_alarms_readings_meteo, join_eboxes_alarms_readings_meteo\n",
    "from preprocessing_functions.utils_preprocessing import return_monday\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_eboxes_preprocess(eboxes_alarms: pd.DataFrame, for_predicting = False, min_date = \"1982-01-04\", max_date = \"2023-04-04\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs basic preprocessing on the raw data of alarms such as filtering the errors that\n",
    "    we want to predict (\"lightcomm\", \"lighterr\"), remove useless columns, parse dates and \n",
    "    eliminate dates out of a defined range.\n",
    "    \"\"\"\n",
    "\n",
    "    eboxes_alarms = eboxes_alarms.loc[eboxes_alarms[\"subtype\"] == \"mcbpower\"]\n",
    "\n",
    "    eboxes_alarms = eboxes_alarms[[\"id\", \"dated\", \"subtype\", \"flag\"]]\n",
    "\n",
    "    eboxes_alarms[\"dated\"] = pd.to_datetime(eboxes_alarms[\"dated\"])\n",
    "\n",
    "    if not for_predicting:\n",
    "        # We have to do some filtering for the raw data because in the datasets for training we have some nonsense data\n",
    "        # like data from the future.\n",
    "        eboxes_alarms = eboxes_alarms.loc[(min_date <= eboxes_alarms[\"dated\"]) & (eboxes_alarms[\"dated\"] <= max_date)]\n",
    "\n",
    "    return eboxes_alarms\n",
    "\n",
    "def big_preprocess_eboxes(eboxes_alarms: pd.DataFrame, predicting_min_date: str = None, predicting_max_date: str = None, for_predicting: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The function converts the data of the alarms into a dataframe that is usable for training the models.\n",
    "    Review the coments on the code to understand step by step what the code does.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # We create a dataframe of each one of the ids and we will clean the alarms. The final objective is to calculate\n",
    "    # the percentage of time the ebox is not working wich is the time that passes from an alarm \"off\" utill it is \n",
    "    # turned \"on\". In the dataframe we have cases in wich there are two consecutive \"on\" or \"off\" alarms so we have\n",
    "    # filter and just keep the first \"on\" and the first \"off\" in this case so we get the real time the ebox has been\n",
    "    # not functioning. In the case that the alarm is turned \"on\" during a week and it is not turned \"off\" untill the next\n",
    "    # week we will insert a fake \"off\" alarm at the last moment of the week and we will turn it \"on\" again in the first moment\n",
    "    # of the following week.\n",
    "\n",
    "    # The first step will be to dowload the data:\n",
    "    # Read recipe inputs\n",
    "    df = eboxes_alarms.copy()\n",
    "\n",
    "    # We will reuse the code of the light alarms for the eboxes: The main thing that we have to modify for the eboxes\n",
    "    # is that for the alarm subtype brdpower the flag \"off\" means that the ebox is suffering a breakdown and untill the\n",
    "    # alarm turns \"on\" we will consider that the ebox has been down. This is exactly opposite to what happened with the\n",
    "    # lamposts.\n",
    "\n",
    "    # The first step is to change the name of the column subtype. Even tough this columns means a subtype of alarm, in this\n",
    "    # code and from now on we will rename it as alarm so we are able to replicate a similar code as the one used for the lights\n",
    "    df.rename(columns={\"subtype\": \"alarm\"}, inplace=True)\n",
    "\n",
    "    # There are four different flags in this case: {off, on, offM, onM} depending if the error has been detected digitaly or manualy.\n",
    "    # we do not care so we fill change this categories to {on off}:\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"onM\"], \"on\")\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"offM\"], \"off\")\n",
    "\n",
    "    # As we already said the flags in the eboxes are opposite as the one in the lamposts, so to be able to use the same\n",
    "    # code we will just change the values \"off\" to \"on\" and the \"on\" to \"off\" and then apply the same exact code that\n",
    "    # we used for the lamposts.\n",
    "    df[\"flag\"] = df[\"flag\"].replace({'on': 'offf', 'off': 'on'}).replace({\"offf\": \"off\"})\n",
    "\n",
    "    # Sort by id and date of the alarm:\n",
    "    df[\"dated\"] = pd.to_datetime(df[\"dated\"])\n",
    "    df = df.sort_values([\"id\", \"dated\"])\n",
    "\n",
    "    # Generate all the weeks from the first day we have data untill the last day. We will need this weeks later to\n",
    "    # do a left join with the weeks that there are alarms\n",
    "\n",
    "    # If we are preprocessing the data for doing predictions we will want to add the date_min and date max manualy,\n",
    "    # otherwise the function will only consider the dates of the errors and you may end up with nan rows\n",
    "    if for_predicting: \n",
    "        start_date = pd.to_datetime(predicting_min_date)\n",
    "        end_date = pd.to_datetime(predicting_max_date)\n",
    "    else:\n",
    "        start_date = df[\"dated\"].min()\n",
    "        end_date = return_monday(df[\"dated\"].max()) + pd.Timedelta(days=7)\n",
    "\n",
    "    weeks = pd.date_range(start=start_date, end=end_date, freq='W').floor(\"D\").strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    # Transform to a dataframe to do operations later:\n",
    "    weeks = pd.DataFrame(weeks, columns=[\"week\"])\n",
    "    weeks[\"week\"] = pd.to_datetime(weeks[\"week\"])\n",
    "    weeks[\"week\"] = weeks[\"week\"].apply(lambda x: return_monday(x))\n",
    "\n",
    "    # For each unique id we will implement all the code:\n",
    "    # First create the dataframe where we will store all the resturned data:\n",
    "    general_lag_dataframe = pd.DataFrame()\n",
    "\n",
    "    # List of all the ids:\n",
    "    ids_list = df[\"id\"].unique()\n",
    "\n",
    "    # Store the name of the columns in oder to later reordenate the final returned dataframe.\n",
    "    if for_predicting:\n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\"]\n",
    "    else:     \n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\", \"week+1\", \"hours_week+1\", \"week+2\", \"hours_week+2\", \"week+3\", \"hours_week+3\", \"week+4\", \"hours_week+4\"]\n",
    "\n",
    "    # Generate a dataframe for each one of the ids and then apply all the transformations. Once done, add the data\n",
    "    # to the dataframe general_lag_dataframe.\n",
    "    for idd in ids_list:\n",
    "        print(idd)\n",
    "        \n",
    "        # Dataframe of all the alarms for the id \"idd\":\n",
    "        tt = df.loc[df[\"id\"] == idd]\n",
    "        \n",
    "        #For each one of the elements in the list we have: elem[0] contains the week represented by sunday \n",
    "        # and elem[1] has the data in a dataframe\n",
    "        grouped_weeks = list(tt.groupby(pd.Grouper(key=\"dated\", freq=\"W\")))\n",
    "        # Here we will transform all the tuples to lists so we can change the elements after\n",
    "        grouped_weeks = [list(elem) for elem in grouped_weeks]\n",
    "\n",
    "        # Now we have to filtrate the empty dataframes that generates groupby for the weeks where there are no alarms\n",
    "        grouped_weeks = [elem for elem in grouped_weeks if not elem[1].empty]\n",
    "\n",
    "        # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\" for the first element\n",
    "        # of the list grouped_weeks. We do it because in the iteration we will not consider it so we have to do it now\n",
    "        first_week_data = grouped_weeks[0][1]\n",
    "        first_week_data[\"prev_flag\"] = first_week_data[\"flag\"].shift()\n",
    "\n",
    "        first_week_data = first_week_data.loc[\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"] == \"off\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"] == \"on\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We store it to the data of the first week:\n",
    "        grouped_weeks[0][1] = first_week_data\n",
    "\n",
    "        # Here we begin the iteration for all the other weeks:\n",
    "        for i, (week, data) in enumerate(grouped_weeks[1:], 1): # We do not consider the first week because it has no previous week to check\n",
    "            # Get the data and week from the previous week:\n",
    "            previous_week, previous_week_data = grouped_weeks[i-1][0], grouped_weeks[i-1][1]\n",
    "\n",
    "            # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\"\n",
    "            data[\"prev_flag\"] = data[\"flag\"].shift()\n",
    "            data = data.loc[\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"] == \"off\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"] == \"on\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"].isna()))\n",
    "            ]\n",
    "\n",
    "            # Data that we will need:\n",
    "            last_moment_previous_week = previous_week + pd.Timedelta(hours=23, minutes=59, seconds=59) #We will need the last moment of the week\n",
    "            first_moment_current_week = (week - pd.Timedelta(days=week.dayofweek)).replace(hour=0, minute=0, second=0) #We will need to the first moment of the current week\n",
    "\n",
    "            #We have to get the last flag of the previous week\n",
    "            last_flag_previous_week = previous_week_data.loc[previous_week_data.index[-1], \"flag\"]\n",
    "\n",
    "            if last_flag_previous_week == \"on\":\n",
    "                # If the last flag from the previous week is \"on\" then we have to set a new row on the previous week data\n",
    "                # in the last position to set a flag \"off\". Then, in the current week data we will add a new row before the\n",
    "                # first week to set again the alarm to \"on\"\n",
    "\n",
    "                # Here we create the new row to set the alarm \"off\" in the previous week\n",
    "                new_row_previous_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [last_moment_previous_week],\n",
    "                    \"alarm\": [\"turn_off_end_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"off\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very end of the week\n",
    "                new_previous_week_data = pd.concat(\n",
    "                    [previous_week_data, new_row_previous_week],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i-1][1] = new_previous_week_data\n",
    "\n",
    "                # Now we have to set the flag \"on\" in the first moment of the current week:\n",
    "                # Here we create the new row to set the alarm \"on\" in the current week\n",
    "                new_row_current_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [first_moment_current_week],\n",
    "                    \"alarm\": [\"turn_on_begining_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"on\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very begining of the week\n",
    "                new_current_week_data = pd.concat(\n",
    "                    [new_row_current_week, data],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i][1] = new_current_week_data\n",
    "            else:\n",
    "                # Simply update the dataframe with the same but with removed rows that contain two identical flags in a row\n",
    "                grouped_weeks[i][1] = data\n",
    "\n",
    "        # Once this is done we have to check if the last alarm of the last week is \"on\". In this case we will add a row turning it off\n",
    "        # in the last moment of the week:\n",
    "        last_recorded_week, last_recorded_week_data = grouped_weeks[-1][0], grouped_weeks[-1][1]\n",
    "\n",
    "        # Get the last flag from the last week\n",
    "        last_flag = last_recorded_week_data[\"flag\"].values[-1]\n",
    "        # and get the last moment of the last week\n",
    "        last_moment_last_recorded_week = last_recorded_week + pd.Timedelta(hours=23, minutes=59, seconds=59)\n",
    "\n",
    "        if last_flag == \"on\":\n",
    "            new_row_last_flag = pd.DataFrame(\n",
    "                {\n",
    "                \"id\": [idd],\n",
    "                \"dated\": [last_moment_last_recorded_week],\n",
    "                \"alarm\": [\"turn_off_end_last_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                \"flag\": [\"off\"],\n",
    "                \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_recorded_week_data = pd.concat(\n",
    "                [last_recorded_week_data, new_row_last_flag],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "            grouped_weeks[-1][1] = last_recorded_week_data\n",
    "\n",
    "        # Now we have to concat all the dataframes from all the weeks contained in the list grouped_weeks in one big dataframe\n",
    "        concatenated_weeks = pd.concat(\n",
    "            [week_data[1] for week_data in grouped_weeks],\n",
    "            sort=True # Remove the warning of pd.concat\n",
    "        )\n",
    "\n",
    "        # At this point there are some cases where we will still have two \"on\" alarms or two \"off\" alarms in a row. For example\n",
    "        # in the case that we have two weeks in a row where we only have \"on\" alarms utill now the code is going to return the \n",
    "        # begining of the end of the previous week with \"off\", the beggining of the current week with \"off\" and before the \"off\"\n",
    "        # of the end of the week we will still have an \"on\" alarm. This is caused because the deletion of the same alarms in a row is done \n",
    "        # before the add of the new rows in the beggining and end of the week\n",
    "\n",
    "        # So let's eliminate this cases too:\n",
    "        # first we have to frop the old prev_flag column that now is useless:\n",
    "        concatenated_weeks.drop(\"prev_flag\", axis=1, inplace=True)\n",
    "\n",
    "        # and create the new one:\n",
    "        concatenated_weeks[\"prev_flag_concat\"] = concatenated_weeks[\"flag\"].shift()\n",
    "\n",
    "        concatenated_weeks = concatenated_weeks.loc[\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"] == \"off\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"] == \"on\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We have some weeks where there is no data and thus if this weeks are between an \"on\" and \"off\" flag they will not\n",
    "        # appear on the dataframe. What we will do is a left join with the variable week generated at the begining\n",
    "        # of the notebook to see the rows that do not appear. Then we will activate the alarm at the begining of the week\n",
    "        # and deactivate it at the end of the same week:\n",
    "\n",
    "        # Set the day to monday\n",
    "        concatenated_weeks[\"week\"] = concatenated_weeks[\"dated\"].apply(lambda x: return_monday(x))\n",
    "        # Left join with weeks to detect the missing weeks\n",
    "        concatenated_weeks_merged = pd.merge(weeks, concatenated_weeks, on=\"week\", how=\"left\")\n",
    "\n",
    "        # Store the result of the filling:\n",
    "        filled_dataframe = pd.DataFrame()\n",
    "\n",
    "        # We add the first row\n",
    "        if not pd.isna(concatenated_weeks_merged.iloc[0][\"alarm\"]):\n",
    "            first_new_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"week\": [concatenated_weeks_merged.iloc[0][\"week\"]],\n",
    "                    \"alarm\": [concatenated_weeks_merged.iloc[0][\"alarm\"]],\n",
    "                    \"dated\": [concatenated_weeks_merged.iloc[0][\"dated\"]],\n",
    "                    \"flag\": [concatenated_weeks_merged.iloc[0][\"flag\"]],\n",
    "                    \"id\": [concatenated_weeks_merged.iloc[0][\"id\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            filled_dataframe = pd.concat(\n",
    "                [filled_dataframe, first_new_row],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Begin the iteration where we will add one by one the rows to the dataframe filled_dataframe\n",
    "        for i in range(1, len(concatenated_weeks_merged)-1):\n",
    "\n",
    "            current_row = concatenated_weeks_merged.iloc[i]\n",
    "\n",
    "            # If we find a normal row we add it to the dataframe\n",
    "            if (current_row[\"alarm\"] == \"mcbpower\") | (current_row[\"alarm\"] == \"turn_off_end_week\") | (current_row[\"alarm\"] == \"turn_on_begining_week\"):\n",
    "                current_row[\"dated\"] = pd.to_datetime(current_row[\"dated\"])\n",
    "\n",
    "                new_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"week\": [current_row[\"week\"]],\n",
    "                        \"alarm\": [current_row[\"alarm\"]],\n",
    "                        \"dated\": [current_row[\"dated\"]],\n",
    "                        \"flag\": [current_row[\"flag\"]],\n",
    "                        \"id\": [current_row[\"id\"]]\n",
    "                    }\n",
    "                )\n",
    "                filled_dataframe = pd.concat([filled_dataframe, new_row], sort=True)\n",
    "\n",
    "            if not filled_dataframe.empty:\n",
    "                # If the last row of the filled dataframe is a \"turn_off_week\" ant the current is a Emty we add the \"on\" and\n",
    "                # off for the begining and the end of the week\n",
    "                last_row_filled_dataframe = filled_dataframe.iloc[-1]\n",
    "\n",
    "                if (pd.isna(current_row[\"alarm\"])) & (last_row_filled_dataframe[\"alarm\"] in [\"turn_off_end_week\", \"turn_off_end_week_filled\"]):\n",
    "\n",
    "                    new_row_begining_dated = datetime.datetime.combine(current_row[\"week\"], datetime.time(0,0,0))\n",
    "                    new_row_begining_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_on_begining_week_filled\"],\n",
    "                            \"dated\": [new_row_begining_dated], # Add the time to the date\n",
    "                            \"flag\": [\"on\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "                    # We have to add 6 days because the representative of the week in this case is \n",
    "                    new_row_end_dated = datetime.datetime.combine(current_row[\"week\"]+ pd.Timedelta(days=6), datetime.time(23,59,59))\n",
    "                    new_row_end_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_off_end_week_filled\"],\n",
    "                            \"dated\": [new_row_end_dated], # In this case we add the time to represent the last moment of the week\n",
    "                            \"flag\": [\"off\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    filled_dataframe = pd.concat(\n",
    "                        [filled_dataframe, new_row_begining_week, new_row_end_week],\n",
    "                        sort=True\n",
    "                    )\n",
    "\n",
    "        # Again we have to do a left join with the dataframe weeks to detect the NaN values:\n",
    "        filled_dataframe_merged = pd.merge(weeks, filled_dataframe, on=\"week\", how=\"left\")\n",
    "        \n",
    "        # Now for each one of the weeks we have to calculate the total time passed between an \"on\" alarm and an \"off\" alarm\n",
    "\n",
    "        # In this dataframe we will store the amount of hours of a ebox that has been malfunctioning for each week\n",
    "        week_hours_dataframe = pd.DataFrame()\n",
    "        for week in weeks[\"week\"]:\n",
    "            # In this variable we will store the amount of hours for this week:\n",
    "            total_hours = 0\n",
    "            on_timestamp = None\n",
    "\n",
    "            # Dataframe with the alarms of the week:\n",
    "            week_alarms_dataframe = filled_dataframe_merged.loc[filled_dataframe_merged[\"week\"] == week]\n",
    "            # Iterate trough the df to count the hours:\n",
    "            for _, row in week_alarms_dataframe.iterrows():\n",
    "                if row[\"flag\"] == \"on\":\n",
    "                    on_timestamp = row[\"dated\"]\n",
    "\n",
    "                if (row[\"flag\"] == \"off\") & (on_timestamp is not None):\n",
    "                    total_hours += (row[\"dated\"] - on_timestamp).total_seconds() / 3600\n",
    "\n",
    "                    on_timestamp = None\n",
    "\n",
    "            new_week_hours = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "                    \"week\": [week],\n",
    "                    \"malfunctioning_hours\": [total_hours] \n",
    "                }\n",
    "            )\n",
    "\n",
    "            week_hours_dataframe = pd.concat(\n",
    "                [week_hours_dataframe, new_week_hours],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Now we want to get the data in the format:\n",
    "        # row = {\"week-4\": date_week_prev_4, \"hours_week-4\": hours_week_prev_4, ..., \"week-1\": date_week_prev_1, \"hours_week-1\": hours_week_prev_1, \"current_week\": date_current_week, \"hours_current_week\":  \"week+1\": date_week_next_1, \"hours_week+1\": hours_week_next_1 ..., \"week+4\": date_week_next_4, \"hours_week+4\": hours_week_next_4}\n",
    "\n",
    "        # In this dataframe we will store the data in the format we have mentioned:\n",
    "        lag_dataframe = pd.DataFrame()\n",
    "\n",
    "        # Here we have to follow two different paths. The first one will be when preparing the data for training and the other one will be when \n",
    "        # preparing the data for predicting. We shall begin with predicting:\n",
    "\n",
    "        if for_predicting:\n",
    "            lag_dataframe = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "\n",
    "                    \"week-4\": [week_hours_dataframe.iloc[0][\"week\"]],\n",
    "                    \"hours_week-4\": [week_hours_dataframe.iloc[0][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-3\": [week_hours_dataframe.iloc[1][\"week\"]],\n",
    "                    \"hours_week-3\": [week_hours_dataframe.iloc[1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-2\": [week_hours_dataframe.iloc[2][\"week\"]],\n",
    "                    \"hours_week-2\": [week_hours_dataframe.iloc[2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-1\": [week_hours_dataframe.iloc[3][\"week\"]],\n",
    "                    \"hours_week-1\": [week_hours_dataframe.iloc[3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"current_week\": [week_hours_dataframe.iloc[4][\"week\"]],\n",
    "                    \"hours_current_week\": [week_hours_dataframe.iloc[4][\"malfunctioning_hours\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Begin the loop at 4 and end at -4 so we don't get the error: \"Out of range\"\n",
    "            for i in range(4, len(weeks)-4):\n",
    "                # Create the new row to add:\n",
    "                to_add_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": [idd],\n",
    "\n",
    "                        \"week-4\": [week_hours_dataframe.iloc[i-4][\"week\"]],\n",
    "                        \"hours_week-4\": [week_hours_dataframe.iloc[i-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-3\": [week_hours_dataframe.iloc[i-3][\"week\"]],\n",
    "                        \"hours_week-3\": [week_hours_dataframe.iloc[i-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-2\": [week_hours_dataframe.iloc[i-2][\"week\"]],\n",
    "                        \"hours_week-2\": [week_hours_dataframe.iloc[i-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-1\": [week_hours_dataframe.iloc[i-1][\"week\"]],\n",
    "                        \"hours_week-1\": [week_hours_dataframe.iloc[i-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"current_week\": [week_hours_dataframe.iloc[i][\"week\"]],\n",
    "                        \"hours_current_week\": [week_hours_dataframe.iloc[i][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+1\": [week_hours_dataframe.iloc[i+1][\"week\"]],\n",
    "                        \"hours_week+1\": [week_hours_dataframe.iloc[i+1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+2\": [week_hours_dataframe.iloc[i+2][\"week\"]],\n",
    "                        \"hours_week+2\": [week_hours_dataframe.iloc[i+2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+3\": [week_hours_dataframe.iloc[i+3][\"week\"]],\n",
    "                        \"hours_week+3\": [week_hours_dataframe.iloc[i+3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+4\": [week_hours_dataframe.iloc[i+4][\"week\"]],\n",
    "                        \"hours_week+4\": [week_hours_dataframe.iloc[i+4][\"malfunctioning_hours\"]]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                lag_dataframe = pd.concat(\n",
    "                    [lag_dataframe, to_add_row],\n",
    "                    sort=True,\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "    # Reordenate with the list columns_order:\n",
    "    general_lag_dataframe = general_lag_dataframe[columns_order]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Execution time:\" + str(end_time - start_time))\n",
    "\n",
    "    return general_lag_dataframe\n",
    "\n",
    "def change_format(date: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the date from a datetime object and convert it into a string\n",
    "    \"\"\"\n",
    "    return str(date)[:10]\n",
    "\n",
    "def join_eboxes_alarms_readings_meteo(\n",
    "        eboxes_alarms: pd.DataFrame, \n",
    "        eboxes_powerReactivePeak: pd.DataFrame,\n",
    "        eboxes_powerReactive: pd.DataFrame,\n",
    "        eboxes_powerActive: pd.DataFrame,\n",
    "        eboxes_powerActivePeak: pd.DataFrame,\n",
    "        meteo: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins the preprocessed data from light_alarms, readings, and meteo data by weeks\n",
    "    \"\"\"\n",
    "\n",
    "    #Change meteo date format\n",
    "    meteo['dated'] = meteo['dated'].apply(change_format)\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "        eboxes_alarms[week] = eboxes_alarms[week].apply(lambda x: str(x))\n",
    "\n",
    "    # joins for the dataset eboxes_powerReactivePeak:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerReactivePeak.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(columns={\"ReactivePeak\": \"ReactivePeak_\" + week}, inplace=True)\n",
    "\n",
    "    # joins for the dataset eboxes_powerReactive:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerReactive.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"powerReactive_sum\": \"powerReactive_sum_\" + week,\n",
    "                \"powerReactive_p1\": \"powerReactive_p1_\" + week,\n",
    "                \"powerReactive_p2\": \"powerReactive_p2_\" + week,\n",
    "                \"powerReactive_p3\": \"powerReactive_p3_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "\n",
    "    # joins for the dataset eboxes_powerActivePeak:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerActivePeak.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(columns={\"ActivePeak\": \"ActivePeak_\" + week}, inplace=True)\n",
    "\n",
    "    # joins for the dataset eboxes_powerActive:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerActive.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"powerActive_sum\": \"powerActive_sum_\" + week,\n",
    "                \"powerActive_p1\": \"powerActive_p1_\" + week,\n",
    "                \"powerActive_p2\": \"powerActive_p2_\" + week,\n",
    "                \"powerActive_p3\": \"powerActive_p3_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "        \n",
    "\n",
    "    # joins for the dataset meteo_canyelles:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            meteo.rename(columns={\"dated\": week}), \n",
    "            on=[week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"Dew_max_max\": \"Dew_max_max_\" + week,\n",
    "                \"Hum_min_min\": \"Hum_min_min_\" + week,\n",
    "                \"Wind_max_max\": \"Wind_max_max_\" + week,\n",
    "                \"Temp_max_max\": \"Temp_max_max_\" + week,\n",
    "                \"Temp_min_min\": \"Temp_min_min_\" + week,\n",
    "                \"Pres_min_min\": \"Pres_min_min_\" + week,\n",
    "                \"Wind_avg_mean\": \"Wind_avg_avg_\" + week,\n",
    "                \"Wind_avg_std\": \"Wind_avg_std_\" + week,\n",
    "                \"Hum_avg_mean\": \"Hum_avg_avg_\" + week,\n",
    "                \"Dew_avg_mean\": \"Dew_avg_avg_\" + week,\n",
    "                \"Dew_avg_std\": \"Dew_avg_std_\" + week,\n",
    "                \"Hum_max_max\": \"Hum_max_max_\" + week,\n",
    "                \"Temp_avg_mean\": \"Temp_avg_avg_\" + week,\n",
    "                \"Temp_avg_std\": \"Temp_avg_std_\" + week,\n",
    "                \"Pres_max_max\": \"Pres_max_max_\" + week,\n",
    "                \"Pres_avg_mean\": \"Pres_avg_avg_\" + week,\n",
    "                \"Pres_avg_std\": \"Pres_avg_std_\" + week,\n",
    "                \"Precipitation_mean\": \"Precipitation_avg_\" + week,\n",
    "                \"Precipitation_sum\": \"Precipitation_sum_\" + week,\n",
    "                \"Dew_min_min\": \"Dew_min_min_\" + week,\n",
    "                \"Wind_min_min\": \"Wind_min_min_\" + week,\n",
    "                \"Hum_avg_std\": \"Hum_avg_std_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "    \n",
    "    return eboxes_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eboxes_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/eboxes_alarms.csv\")\n",
    "light_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/lights_alarms.csv\")\n",
    "nodes = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/nodes.csv\")\n",
    "powerActive = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/powerActive.csv\")\n",
    "powerReactive = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/powerReactive.csv\")\n",
    "powerActivePeak = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/powerActivePeak.csv\")\n",
    "powerReactivePeak = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/powerActivePeak.csv\")\n",
    "\n",
    "# meteo here:\n",
    "meteo = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/new_meteo_mejorada.csv\")\n",
    "\n",
    "predicting_min = \"2023-04-17\"\n",
    "predicting_max = \"2023-05-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA0000011277\n",
      "Execution time:0.03579998016357422\n",
      "AA000000AC39\n",
      "AA000000AC94\n",
      "AA000000AF94\n",
      "AA000000B302\n",
      "AA000000B328\n",
      "AA000000B366\n",
      "AA000000B367\n",
      "AA000000B36E\n",
      "AA000000B719\n",
      "AA000000B8A6\n",
      "AA000000BD3C\n",
      "AA000000BE7F\n",
      "AA00000148E5\n",
      "AA0000017347\n",
      "AA000001734B\n",
      "AA0000018F1C\n",
      "AA00000192D8\n",
      "AA00000195F9\n",
      "Execution time:1.709451675415039\n"
     ]
    }
   ],
   "source": [
    "# For the lights and eboxes that have alarms in the last 5 weeks we have to do the normal preprocessing:\n",
    "# First the eboxes:\n",
    "powerReactive, powerActive = pre_power(powerReactive, \"Reactive\"), pre_power(powerActive, \"Active\")\n",
    "powerReactivePeak, powerActivePeak = pre_power_peak(powerReactivePeak, \"Reactive\"), pre_power_peak(powerActivePeak, \"Active\")\n",
    "\n",
    "meteo = first_meteo_preprocess(\n",
    "    meteo = meteo,\n",
    "    filter_dates = False\n",
    ")\n",
    "\n",
    "meteo = meteo_groupby(meteo)\n",
    "\n",
    "eboxes_alarms = first_eboxes_preprocess(eboxes_alarms, for_predicting=True)\n",
    "\n",
    "eboxes_alarms = big_preprocess_eboxes(\n",
    "    eboxes_alarms = eboxes_alarms,\n",
    "    for_predicting = True,\n",
    "    predicting_min_date = predicting_min,\n",
    "    predicting_max_date = predicting_max\n",
    ")\n",
    "\n",
    "eboxes_alarms = join_eboxes_alarms_readings_meteo(\n",
    "    eboxes_alarms = eboxes_alarms,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")\n",
    "\n",
    "# Untill this point we have just considered the eboxes that have had an error in the last 4 weeks. \n",
    "# We should consider too all the other alarms that have no errors in the last 4 weeks and put them\n",
    "# in a dataframe format identical to the eboxes-alarms dataframe. This way we will be able to do predictionns\n",
    "# for all the eboxes in the city.\n",
    "\n",
    "# Let's get the nodes that have not suffered any alarms in the last weeks:\n",
    "eboxes_nodes = nodes.loc[nodes[\"type\"] == \"box\"]\n",
    "non_error_eboxes_nodes = eboxes_nodes.loc[~eboxes_nodes[\"id\"].isin(eboxes_alarms[\"id\"])]\n",
    "\n",
    "# Now we have to build a dataframe with the same structure as light_eboxes for this non_error_eboxes:\n",
    "non_error_eboxes = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": non_error_eboxes_nodes[\"id\"],\n",
    "        \"week-4\": eboxes_alarms[\"week-4\"][0],\n",
    "        \"hours_week-4\": 0,\n",
    "        \"week-3\": eboxes_alarms[\"week-3\"][0],\n",
    "        \"hours_week-3\": 0,\n",
    "        \"week-2\": eboxes_alarms[\"week-2\"][0],\n",
    "        \"hours_week-2\": 0,\n",
    "        \"week-1\": eboxes_alarms[\"week-1\"][0],\n",
    "        \"hours_week-1\": 0,\n",
    "        \"current_week\": eboxes_alarms[\"current_week\"][0],\n",
    "        \"hours_current_week\": 0,\n",
    "        \"lat\": non_error_eboxes_nodes[\"lat\"],\n",
    "        \"lon\": non_error_eboxes_nodes[\"lon\"]\n",
    "    }\n",
    ")\n",
    "# Create the dataframe ready for the model:\n",
    "\n",
    "non_error_eboxes = join_eboxes_alarms_readings_meteo(\n",
    "    eboxes_alarms = non_error_eboxes,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")\n",
    "\n",
    "# Now for the the lights:\n",
    "\n",
    "light_alarms = first_lights_preprocess(light_alarms, for_predicting=True)\n",
    "\n",
    "light_alarms = big_preprocess_lights(\n",
    "    light_alarms = light_alarms,\n",
    "    for_predicting = True,\n",
    "    predicting_min_date = predicting_min,\n",
    "    predicting_max_date = predicting_max\n",
    ")\n",
    "\n",
    "light_alarms = pd.merge(light_alarms, nodes, on=\"id\", how=\"left\")\n",
    "\n",
    "light_alarms = join_light_alarms_readings_meteo(\n",
    "    light_errors = light_alarms,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")\n",
    "\n",
    "# Untill this point we have just considered the lights that have had an error in the last 4 weeks. \n",
    "# We should consider too all the other alarms that have no errors in the last 4 weeks and put them\n",
    "# in a dataframe format identical to the light-alarms dataframe. This way we will be able to do predictionns\n",
    "# for all the lights in the city.\n",
    "\n",
    "# Let's get the nodes that have not suffered any alarms in the last weeks:\n",
    "light_nodes = nodes.loc[nodes[\"type\"] == \"light\"]\n",
    "non_error_lights_nodes = light_nodes.loc[~light_nodes[\"id\"].isin(light_alarms[\"id\"])]\n",
    "\n",
    "# Now we have to build a dataframe with the same structure as light_alarms for this non_error_lights:\n",
    "non_error_lights = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": non_error_lights_nodes[\"id\"],\n",
    "        \"week-4\": light_alarms[\"week-4\"][0],\n",
    "        \"hours_week-4\": 0,\n",
    "        \"week-3\": light_alarms[\"week-3\"][0],\n",
    "        \"hours_week-3\": 0,\n",
    "        \"week-2\": light_alarms[\"week-2\"][0],\n",
    "        \"hours_week-2\": 0,\n",
    "        \"week-1\": light_alarms[\"week-1\"][0],\n",
    "        \"hours_week-1\": 0,\n",
    "        \"current_week\": light_alarms[\"current_week\"][0],\n",
    "        \"hours_current_week\": 0,\n",
    "        \"ebox_id\": non_error_lights_nodes[\"ebox_id\"],\n",
    "        \"lat\": non_error_lights_nodes[\"lat\"],\n",
    "        \"lon\": non_error_lights_nodes[\"lon\"]\n",
    "    }\n",
    ")\n",
    "# Create the dataframe ready for the model:\n",
    "\n",
    "non_error_lights = join_light_alarms_readings_meteo(\n",
    "    light_errors = non_error_lights,\n",
    "    eboxes_powerReactivePeak = powerReactivePeak,\n",
    "    eboxes_powerReactive = powerReactive,\n",
    "    eboxes_powerActive = powerActive,\n",
    "    eboxes_powerActivePeak = powerActivePeak,\n",
    "    meteo = meteo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17\n",
      "2023-05-21\n"
     ]
    }
   ],
   "source": [
    "print(predicting_min)\n",
    "print(predicting_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-04-17', '2023-04-24', '2023-05-01', '2023-05-08', '2023-05-15']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_range = pd.date_range(start=predicting_min, end=predicting_max, freq=\"W\")\n",
    "dates_range = [str(return_monday(elem)) for elem in dates_range]\n",
    "dates_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week-4</th>\n",
       "      <th>hours_week-4</th>\n",
       "      <th>week-3</th>\n",
       "      <th>hours_week-3</th>\n",
       "      <th>week-2</th>\n",
       "      <th>hours_week-2</th>\n",
       "      <th>week-1</th>\n",
       "      <th>hours_week-1</th>\n",
       "      <th>current_week</th>\n",
       "      <th>...</th>\n",
       "      <th>Wind_max_max_current_week</th>\n",
       "      <th>Wind_avg_avg_current_week</th>\n",
       "      <th>Wind_avg_std_current_week</th>\n",
       "      <th>Wind_min_min_current_week</th>\n",
       "      <th>Pres_max_max_current_week</th>\n",
       "      <th>Pres_avg_avg_current_week</th>\n",
       "      <th>Pres_avg_std_current_week</th>\n",
       "      <th>Pres_min_min_current_week</th>\n",
       "      <th>Precipitation_avg_current_week</th>\n",
       "      <th>Precipitation_sum_current_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA000000AC39</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.035278</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA000000AC94</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA000000AF94</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA000000B302</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>13.598889</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.020278</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.024722</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA000000B328</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AA000000B366</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>8.363333</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AA000000B367</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>27.837500</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>14.141111</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>14.355833</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AA000000B36E</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>54.708611</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>69.746111</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>71.045556</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>72.341667</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AA000000B719</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.104722</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.419167</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.213889</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AA000000B8A6</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>4.895278</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AA000000BD3C</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>13.796111</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AA000000BE7F</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.218611</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.039722</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AA00000148E5</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AA0000017347</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.049167</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>14.495278</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AA000001734B</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>82.173611</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>87.856944</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>99.434444</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>86.776111</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AA0000018F1C</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AA00000192D8</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>148.001389</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AA00000195F9</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>40.646667</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>28.915278</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.742857</td>\n",
       "      <td>1.632847</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.828571</td>\n",
       "      <td>0.149603</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id      week-4  hours_week-4      week-3  hours_week-3  \\\n",
       "0   AA000000AC39  2023-04-17      0.000000  2023-04-24      0.035278   \n",
       "1   AA000000AC94  2023-04-17      0.000000  2023-04-24      0.000000   \n",
       "2   AA000000AF94  2023-04-17      0.019722  2023-04-24      0.000000   \n",
       "3   AA000000B302  2023-04-17     13.598889  2023-04-24      0.000000   \n",
       "4   AA000000B328  2023-04-17      0.000000  2023-04-24      0.820000   \n",
       "5   AA000000B366  2023-04-17      0.000000  2023-04-24      8.363333   \n",
       "6   AA000000B367  2023-04-17      0.017222  2023-04-24     27.837500   \n",
       "7   AA000000B36E  2023-04-17     54.708611  2023-04-24     69.746111   \n",
       "8   AA000000B719  2023-04-17      0.104722  2023-04-24      0.419167   \n",
       "9   AA000000B8A6  2023-04-17      0.000000  2023-04-24      4.895278   \n",
       "10  AA000000BD3C  2023-04-17      0.000000  2023-04-24     13.796111   \n",
       "11  AA000000BE7F  2023-04-17      0.016944  2023-04-24      0.218611   \n",
       "12  AA00000148E5  2023-04-17      0.000000  2023-04-24      0.007222   \n",
       "13  AA0000017347  2023-04-17      0.046944  2023-04-24      0.049167   \n",
       "14  AA000001734B  2023-04-17     82.173611  2023-04-24     87.856944   \n",
       "15  AA0000018F1C  2023-04-17      0.000000  2023-04-24      0.076667   \n",
       "16  AA00000192D8  2023-04-17    148.001389  2023-04-24      0.000000   \n",
       "17  AA00000195F9  2023-04-17     40.646667  2023-04-24      0.010000   \n",
       "\n",
       "        week-2  hours_week-2      week-1  hours_week-1 current_week  ...  \\\n",
       "0   2023-05-01      0.000000  2023-05-08      0.000000   2023-05-15  ...   \n",
       "1   2023-05-01      0.003611  2023-05-08      0.013611   2023-05-15  ...   \n",
       "2   2023-05-01      0.000000  2023-05-08      0.013889   2023-05-15  ...   \n",
       "3   2023-05-01      0.020278  2023-05-08      0.024722   2023-05-15  ...   \n",
       "4   2023-05-01      0.000000  2023-05-08      0.000000   2023-05-15  ...   \n",
       "5   2023-05-01      0.000000  2023-05-08      0.000000   2023-05-15  ...   \n",
       "6   2023-05-01     14.141111  2023-05-08     14.355833   2023-05-15  ...   \n",
       "7   2023-05-01     71.045556  2023-05-08     72.341667   2023-05-15  ...   \n",
       "8   2023-05-01      0.025556  2023-05-08      0.213889   2023-05-15  ...   \n",
       "9   2023-05-01      0.000000  2023-05-08      0.000000   2023-05-15  ...   \n",
       "10  2023-05-01      0.000000  2023-05-08      0.016667   2023-05-15  ...   \n",
       "11  2023-05-01      0.010833  2023-05-08      0.039722   2023-05-15  ...   \n",
       "12  2023-05-01      0.000000  2023-05-08      0.011111   2023-05-15  ...   \n",
       "13  2023-05-01      0.013333  2023-05-08     14.495278   2023-05-15  ...   \n",
       "14  2023-05-01     99.434444  2023-05-08     86.776111   2023-05-15  ...   \n",
       "15  2023-05-01      0.153333  2023-05-08      0.133333   2023-05-15  ...   \n",
       "16  2023-05-01      0.000000  2023-05-08      0.000000   2023-05-15  ...   \n",
       "17  2023-05-01      0.000000  2023-05-08     28.915278   2023-05-15  ...   \n",
       "\n",
       "    Wind_max_max_current_week Wind_avg_avg_current_week  \\\n",
       "0                          20                  5.742857   \n",
       "1                          20                  5.742857   \n",
       "2                          20                  5.742857   \n",
       "3                          20                  5.742857   \n",
       "4                          20                  5.742857   \n",
       "5                          20                  5.742857   \n",
       "6                          20                  5.742857   \n",
       "7                          20                  5.742857   \n",
       "8                          20                  5.742857   \n",
       "9                          20                  5.742857   \n",
       "10                         20                  5.742857   \n",
       "11                         20                  5.742857   \n",
       "12                         20                  5.742857   \n",
       "13                         20                  5.742857   \n",
       "14                         20                  5.742857   \n",
       "15                         20                  5.742857   \n",
       "16                         20                  5.742857   \n",
       "17                         20                  5.742857   \n",
       "\n",
       "   Wind_avg_std_current_week  Wind_min_min_current_week  \\\n",
       "0                   1.632847                          0   \n",
       "1                   1.632847                          0   \n",
       "2                   1.632847                          0   \n",
       "3                   1.632847                          0   \n",
       "4                   1.632847                          0   \n",
       "5                   1.632847                          0   \n",
       "6                   1.632847                          0   \n",
       "7                   1.632847                          0   \n",
       "8                   1.632847                          0   \n",
       "9                   1.632847                          0   \n",
       "10                  1.632847                          0   \n",
       "11                  1.632847                          0   \n",
       "12                  1.632847                          0   \n",
       "13                  1.632847                          0   \n",
       "14                  1.632847                          0   \n",
       "15                  1.632847                          0   \n",
       "16                  1.632847                          0   \n",
       "17                  1.632847                          0   \n",
       "\n",
       "    Pres_max_max_current_week  Pres_avg_avg_current_week  \\\n",
       "0                        28.1                  27.828571   \n",
       "1                        28.1                  27.828571   \n",
       "2                        28.1                  27.828571   \n",
       "3                        28.1                  27.828571   \n",
       "4                        28.1                  27.828571   \n",
       "5                        28.1                  27.828571   \n",
       "6                        28.1                  27.828571   \n",
       "7                        28.1                  27.828571   \n",
       "8                        28.1                  27.828571   \n",
       "9                        28.1                  27.828571   \n",
       "10                       28.1                  27.828571   \n",
       "11                       28.1                  27.828571   \n",
       "12                       28.1                  27.828571   \n",
       "13                       28.1                  27.828571   \n",
       "14                       28.1                  27.828571   \n",
       "15                       28.1                  27.828571   \n",
       "16                       28.1                  27.828571   \n",
       "17                       28.1                  27.828571   \n",
       "\n",
       "    Pres_avg_std_current_week  Pres_min_min_current_week  \\\n",
       "0                    0.149603                       27.6   \n",
       "1                    0.149603                       27.6   \n",
       "2                    0.149603                       27.6   \n",
       "3                    0.149603                       27.6   \n",
       "4                    0.149603                       27.6   \n",
       "5                    0.149603                       27.6   \n",
       "6                    0.149603                       27.6   \n",
       "7                    0.149603                       27.6   \n",
       "8                    0.149603                       27.6   \n",
       "9                    0.149603                       27.6   \n",
       "10                   0.149603                       27.6   \n",
       "11                   0.149603                       27.6   \n",
       "12                   0.149603                       27.6   \n",
       "13                   0.149603                       27.6   \n",
       "14                   0.149603                       27.6   \n",
       "15                   0.149603                       27.6   \n",
       "16                   0.149603                       27.6   \n",
       "17                   0.149603                       27.6   \n",
       "\n",
       "    Precipitation_avg_current_week  Precipitation_sum_current_week  \n",
       "0                              0.0                             0.0  \n",
       "1                              0.0                             0.0  \n",
       "2                              0.0                             0.0  \n",
       "3                              0.0                             0.0  \n",
       "4                              0.0                             0.0  \n",
       "5                              0.0                             0.0  \n",
       "6                              0.0                             0.0  \n",
       "7                              0.0                             0.0  \n",
       "8                              0.0                             0.0  \n",
       "9                              0.0                             0.0  \n",
       "10                             0.0                             0.0  \n",
       "11                             0.0                             0.0  \n",
       "12                             0.0                             0.0  \n",
       "13                             0.0                             0.0  \n",
       "14                             0.0                             0.0  \n",
       "15                             0.0                             0.0  \n",
       "16                             0.0                             0.0  \n",
       "17                             0.0                             0.0  \n",
       "\n",
       "[18 rows x 175 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_lights(light_alarms: pd.DataFrame, model_type: str = \"default\") -> None:\n",
    "\n",
    "    # In this case we don't use the ada boost combined predictor and the model will take into accout the readings. This\n",
    "    # model has an overall better accuracy than the adboc but fails to detect the sudden errors\n",
    "    if model_type == \"default\":\n",
    "        with open(\"predictive_models/ada_model_readings.pk1\", \"rb\") as file:\n",
    "            ada_model = pickle.load(file)\n",
    "        with open(\"predictive_models/ada_prob_readings.pk1\", \"rb\") as file:\n",
    "            prob_ada_model = pickle.load(file)[\"prob_ada_model\"]\n",
    "        \n",
    "        df = light_alarms.copy()\n",
    "\n",
    "        # Drop some usless columns for the model:\n",
    "        drop_cols = [\n",
    "            col for col in df.columns if\n",
    "                (col in [\"lat\", \"lon\"]) | \n",
    "                (col == \"Unnamed: 0\") |\n",
    "                (col.startswith(\"week\")) |\n",
    "                (col == \"current_week\") |\n",
    "                (col == \"type\") |\n",
    "                (col == \"ebox_id\") |\n",
    "                (col == \"location\")\n",
    "        ]\n",
    "        df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "        # Interpolate some left missing values:\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "        predictions = ada_model.predict_proba(df.drop(\"id\", axis=1))[:, 1]\n",
    "\n",
    "        predictions_out = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": df[\"id\"],\n",
    "                \"pred\": predictions\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"Predictions:\")\n",
    "        print(\"Probability threshold recommended for this model for lights: \" + str(prob_ada_model))\n",
    "        print(predictions_out)\n",
    "\n",
    "    # In this case we will use the adboc model. This model has a worse overall accuracy than the default but has a better chance \n",
    "    # at dettecting sudden errors. The model does not use the readings in this case.\n",
    "    if model_type == \"adboc\":\n",
    "        with open(\"predictive_models/ada_model.pk1\", \"rb\") as file:\n",
    "            ada_model = pickle.load(file)\n",
    "        with open(\"predictive_models/ada_sudden_model.pk1\", \"rb\") as file:\n",
    "            ada_sudden_model = pickle.load(file)\n",
    "        with open(\"predictive_models/ada_prob.pk1\", \"rb\") as file:\n",
    "            probs_dict = pickle.load(file)\n",
    "            prob_ada_model = probs_dict[\"prob_ada_model\"]\n",
    "            prob_sudden_model = probs_dict[\"prob_sudden_model\"]\n",
    "        \n",
    "        df = light_alarms.copy()\n",
    "        drop_cols = [\n",
    "                    col for col in df.columns if \n",
    "                    (col.startswith(\"power\")) | (col.startswith(\"Active\")) | (col.startswith(\"Reactive\") | \n",
    "                    (col in [\"lat\", \"lon\"])) | \n",
    "                    (col == \"Unnamed: 0\") |\n",
    "                    (col.startswith(\"week\")) |\n",
    "                    (col == \"current_week\") |\n",
    "                    (col == \"type\") |\n",
    "                    (col == \"ebox_id\") |\n",
    "                    (col == \"location\")\n",
    "                ]\n",
    "        df = df.drop(drop_cols, axis=1)\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "        predictions = adboc_predict(\n",
    "            df = df,\n",
    "            ada_model = ada_model,\n",
    "            ada_sudden_model = ada_sudden_model,\n",
    "            prob_threshold_ada_model = prob_ada_model,\n",
    "            prob_threshold_ada_sudden_model = prob_sudden_model,\n",
    "        )\n",
    "\n",
    "        predictions_out = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": predictions.keys(),\n",
    "                \"pred\": predictions.values()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"Predictions:\")\n",
    "        print(\"Probability threshold recommended for the model ada_model for lights: \" + str(prob_ada_model))\n",
    "        print(\"Probability threshold recommended for the model ada_sudden_model for lights: \" + str(prob_sudden_model))\n",
    "        print(predictions_out)\n",
    "\n",
    "def make_predictions_eboxes(eboxes_alarms: pd.DataFrame) -> None:\n",
    "\n",
    "    with open(\"predictive_models/ada_model_eboxes.pk1\", \"rb\") as file:\n",
    "        ada_model = pickle.load(file)\n",
    "\n",
    "    with open(\"predictive_models/ada_prob_eboxes.pk1\", \"rb\") as file:\n",
    "        probs_dict = pickle.load(file)\n",
    "        prob_ada_model = probs_dict[\"prob_ada_model\"]\n",
    "    \n",
    "    df = eboxes_alarms.copy()\n",
    "    drop_cols = [\n",
    "                col for col in df.columns if \n",
    "                (col.startswith(\"power\")) | (col.startswith(\"Active\")) | (col.startswith(\"Reactive\") | \n",
    "                (col in [\"lat\", \"lon\"])) | \n",
    "                (col == \"Unnamed: 0\") |\n",
    "                (col.startswith(\"week\")) |\n",
    "                (col == \"current_week\") |\n",
    "                (col == \"type\") |\n",
    "                (col == \"location\")\n",
    "            ]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "    predictions = ada_model.predict_proba(df.drop(\"id\", axis=1))[:, 1]\n",
    "\n",
    "    predictions_out = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": df[\"id\"],\n",
    "            \"pred\": predictions\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Predictions:\")\n",
    "    print(\"Probability threshold recommended for this model for eboxes: \" + str(prob_ada_model))\n",
    "    print(predictions_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'light_alarms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m light_alarms\n",
      "\u001b[0;31mNameError\u001b[0m: name 'light_alarms' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions test for eboxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_eboxes_preprocess(eboxes_alarms: pd.DataFrame, for_predicting = False, min_date = \"1982-01-04\", max_date = \"2023-04-04\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs basic preprocessing on the raw data of alarms such as filtering the errors that\n",
    "    we want to predict (\"lightcomm\", \"lighterr\"), remove useless columns, parse dates and \n",
    "    eliminate dates out of a defined range.\n",
    "    \"\"\"\n",
    "\n",
    "    eboxes_alarms = eboxes_alarms.loc[eboxes_alarms[\"subtype\"] == \"mcbpower\"]\n",
    "\n",
    "    eboxes_alarms = eboxes_alarms[[\"id\", \"dated\", \"subtype\", \"flag\"]]\n",
    "\n",
    "    eboxes_alarms[\"dated\"] = pd.to_datetime(eboxes_alarms[\"dated\"])\n",
    "\n",
    "    if not for_predicting:\n",
    "        # We have to do some filtering for the raw data because in the datasets for training we have some nonsense data\n",
    "        # like data from the future.\n",
    "        eboxes_alarms = eboxes_alarms.loc[(min_date <= eboxes_alarms[\"dated\"]) & (eboxes_alarms[\"dated\"] <= max_date)]\n",
    "\n",
    "    return eboxes_alarms\n",
    "\n",
    "def big_preprocess_eboxes(eboxes_alarms: pd.DataFrame, predicting_min_date: str = None, predicting_max_date: str = None, for_predicting: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The function converts the data of the alarms into a dataframe that is usable for training the models.\n",
    "    Review the coments on the code to understand step by step what the code does.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # We create a dataframe of each one of the ids and we will clean the alarms. The final objective is to calculate\n",
    "    # the percentage of time the ebox is not working wich is the time that passes from an alarm \"off\" utill it is \n",
    "    # turned \"on\". In the dataframe we have cases in wich there are two consecutive \"on\" or \"off\" alarms so we have\n",
    "    # filter and just keep the first \"on\" and the first \"off\" in this case so we get the real time the ebox has been\n",
    "    # not functioning. In the case that the alarm is turned \"on\" during a week and it is not turned \"off\" untill the next\n",
    "    # week we will insert a fake \"off\" alarm at the last moment of the week and we will turn it \"on\" again in the first moment\n",
    "    # of the following week.\n",
    "\n",
    "    # The first step will be to dowload the data:\n",
    "    # Read recipe inputs\n",
    "    df = eboxes_alarms.copy()\n",
    "\n",
    "    # We will reuse the code of the light alarms for the eboxes: The main thing that we have to modify for the eboxes\n",
    "    # is that for the alarm subtype brdpower the flag \"off\" means that the ebox is suffering a breakdown and untill the\n",
    "    # alarm turns \"on\" we will consider that the ebox has been down. This is exactly opposite to what happened with the\n",
    "    # lamposts.\n",
    "\n",
    "    # The first step is to change the name of the column subtype. Even tough this columns means a subtype of alarm, in this\n",
    "    # code and from now on we will rename it as alarm so we are able to replicate a similar code as the one used for the lights\n",
    "    df.rename(columns={\"subtype\": \"alarm\"}, inplace=True)\n",
    "\n",
    "    # There are four different flags in this case: {off, on, offM, onM} depending if the error has been detected digitaly or manualy.\n",
    "    # we do not care so we fill change this categories to {on off}:\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"onM\"], \"on\")\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"offM\"], \"off\")\n",
    "\n",
    "    # As we already said the flags in the eboxes are opposite as the one in the lamposts, so to be able to use the same\n",
    "    # code we will just change the values \"off\" to \"on\" and the \"on\" to \"off\" and then apply the same exact code that\n",
    "    # we used for the lamposts.\n",
    "    df[\"flag\"] = df[\"flag\"].replace({'on': 'offf', 'off': 'on'}).replace({\"offf\": \"off\"})\n",
    "\n",
    "    # Sort by id and date of the alarm:\n",
    "    df[\"dated\"] = pd.to_datetime(df[\"dated\"])\n",
    "    df = df.sort_values([\"id\", \"dated\"])\n",
    "\n",
    "    # Generate all the weeks from the first day we have data untill the last day. We will need this weeks later to\n",
    "    # do a left join with the weeks that there are alarms\n",
    "\n",
    "    # If we are preprocessing the data for doing predictions we will want to add the date_min and date max manualy,\n",
    "    # otherwise the function will only consider the dates of the errors and you may end up with nan rows\n",
    "    if for_predicting: \n",
    "        start_date = pd.to_datetime(predicting_min_date)\n",
    "        end_date = pd.to_datetime(predicting_max_date)\n",
    "    else:\n",
    "        start_date = df[\"dated\"].min()\n",
    "        end_date = return_monday(df[\"dated\"].max()) + pd.Timedelta(days=7)\n",
    "\n",
    "    weeks = pd.date_range(start=start_date, end=end_date, freq='W').floor(\"D\").strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    # Transform to a dataframe to do operations later:\n",
    "    weeks = pd.DataFrame(weeks, columns=[\"week\"])\n",
    "    weeks[\"week\"] = pd.to_datetime(weeks[\"week\"])\n",
    "    weeks[\"week\"] = weeks[\"week\"].apply(lambda x: return_monday(x))\n",
    "\n",
    "    # For each unique id we will implement all the code:\n",
    "    # First create the dataframe where we will store all the resturned data:\n",
    "    general_lag_dataframe = pd.DataFrame()\n",
    "\n",
    "    # List of all the ids:\n",
    "    ids_list = df[\"id\"].unique()\n",
    "\n",
    "    # Store the name of the columns in oder to later reordenate the final returned dataframe.\n",
    "    if for_predicting:\n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\"]\n",
    "    else:     \n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\", \"week+1\", \"hours_week+1\", \"week+2\", \"hours_week+2\", \"week+3\", \"hours_week+3\", \"week+4\", \"hours_week+4\"]\n",
    "\n",
    "    # Generate a dataframe for each one of the ids and then apply all the transformations. Once done, add the data\n",
    "    # to the dataframe general_lag_dataframe.\n",
    "    for idd in ids_list:\n",
    "        print(idd)\n",
    "        \n",
    "        # Dataframe of all the alarms for the id \"idd\":\n",
    "        tt = df.loc[df[\"id\"] == idd]\n",
    "        \n",
    "        #For each one of the elements in the list we have: elem[0] contains the week represented by sunday \n",
    "        # and elem[1] has the data in a dataframe\n",
    "        grouped_weeks = list(tt.groupby(pd.Grouper(key=\"dated\", freq=\"W\")))\n",
    "        # Here we will transform all the tuples to lists so we can change the elements after\n",
    "        grouped_weeks = [list(elem) for elem in grouped_weeks]\n",
    "\n",
    "        # Now we have to filtrate the empty dataframes that generates groupby for the weeks where there are no alarms\n",
    "        grouped_weeks = [elem for elem in grouped_weeks if not elem[1].empty]\n",
    "\n",
    "        # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\" for the first element\n",
    "        # of the list grouped_weeks. We do it because in the iteration we will not consider it so we have to do it now\n",
    "        first_week_data = grouped_weeks[0][1]\n",
    "        first_week_data[\"prev_flag\"] = first_week_data[\"flag\"].shift()\n",
    "\n",
    "        first_week_data = first_week_data.loc[\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"] == \"off\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"] == \"on\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We store it to the data of the first week:\n",
    "        grouped_weeks[0][1] = first_week_data\n",
    "\n",
    "        # Here we begin the iteration for all the other weeks:\n",
    "        for i, (week, data) in enumerate(grouped_weeks[1:], 1): # We do not consider the first week because it has no previous week to check\n",
    "            # Get the data and week from the previous week:\n",
    "            previous_week, previous_week_data = grouped_weeks[i-1][0], grouped_weeks[i-1][1]\n",
    "\n",
    "            # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\"\n",
    "            data[\"prev_flag\"] = data[\"flag\"].shift()\n",
    "            data = data.loc[\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"] == \"off\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"] == \"on\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"].isna()))\n",
    "            ]\n",
    "\n",
    "            # Data that we will need:\n",
    "            last_moment_previous_week = previous_week + pd.Timedelta(hours=23, minutes=59, seconds=59) #We will need the last moment of the week\n",
    "            first_moment_current_week = (week - pd.Timedelta(days=week.dayofweek)).replace(hour=0, minute=0, second=0) #We will need to the first moment of the current week\n",
    "\n",
    "            #We have to get the last flag of the previous week\n",
    "            last_flag_previous_week = previous_week_data.loc[previous_week_data.index[-1], \"flag\"]\n",
    "\n",
    "            if last_flag_previous_week == \"on\":\n",
    "                # If the last flag from the previous week is \"on\" then we have to set a new row on the previous week data\n",
    "                # in the last position to set a flag \"off\". Then, in the current week data we will add a new row before the\n",
    "                # first week to set again the alarm to \"on\"\n",
    "\n",
    "                # Here we create the new row to set the alarm \"off\" in the previous week\n",
    "                new_row_previous_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [last_moment_previous_week],\n",
    "                    \"alarm\": [\"turn_off_end_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"off\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very end of the week\n",
    "                new_previous_week_data = pd.concat(\n",
    "                    [previous_week_data, new_row_previous_week],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i-1][1] = new_previous_week_data\n",
    "\n",
    "                # Now we have to set the flag \"on\" in the first moment of the current week:\n",
    "                # Here we create the new row to set the alarm \"on\" in the current week\n",
    "                new_row_current_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [first_moment_current_week],\n",
    "                    \"alarm\": [\"turn_on_begining_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"on\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very begining of the week\n",
    "                new_current_week_data = pd.concat(\n",
    "                    [new_row_current_week, data],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i][1] = new_current_week_data\n",
    "            else:\n",
    "                # Simply update the dataframe with the same but with removed rows that contain two identical flags in a row\n",
    "                grouped_weeks[i][1] = data\n",
    "\n",
    "        # Once this is done we have to check if the last alarm of the last week is \"on\". In this case we will add a row turning it off\n",
    "        # in the last moment of the week:\n",
    "        last_recorded_week, last_recorded_week_data = grouped_weeks[-1][0], grouped_weeks[-1][1]\n",
    "\n",
    "        # Get the last flag from the last week\n",
    "        last_flag = last_recorded_week_data[\"flag\"].values[-1]\n",
    "        # and get the last moment of the last week\n",
    "        last_moment_last_recorded_week = last_recorded_week + pd.Timedelta(hours=23, minutes=59, seconds=59)\n",
    "\n",
    "        if last_flag == \"on\":\n",
    "            new_row_last_flag = pd.DataFrame(\n",
    "                {\n",
    "                \"id\": [idd],\n",
    "                \"dated\": [last_moment_last_recorded_week],\n",
    "                \"alarm\": [\"turn_off_end_last_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                \"flag\": [\"off\"],\n",
    "                \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_recorded_week_data = pd.concat(\n",
    "                [last_recorded_week_data, new_row_last_flag],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "            grouped_weeks[-1][1] = last_recorded_week_data\n",
    "\n",
    "        # Now we have to concat all the dataframes from all the weeks contained in the list grouped_weeks in one big dataframe\n",
    "        concatenated_weeks = pd.concat(\n",
    "            [week_data[1] for week_data in grouped_weeks],\n",
    "            sort=True # Remove the warning of pd.concat\n",
    "        )\n",
    "\n",
    "        # At this point there are some cases where we will still have two \"on\" alarms or two \"off\" alarms in a row. For example\n",
    "        # in the case that we have two weeks in a row where we only have \"on\" alarms utill now the code is going to return the \n",
    "        # begining of the end of the previous week with \"off\", the beggining of the current week with \"off\" and before the \"off\"\n",
    "        # of the end of the week we will still have an \"on\" alarm. This is caused because the deletion of the same alarms in a row is done \n",
    "        # before the add of the new rows in the beggining and end of the week\n",
    "\n",
    "        # So let's eliminate this cases too:\n",
    "        # first we have to frop the old prev_flag column that now is useless:\n",
    "        concatenated_weeks.drop(\"prev_flag\", axis=1, inplace=True)\n",
    "\n",
    "        # and create the new one:\n",
    "        concatenated_weeks[\"prev_flag_concat\"] = concatenated_weeks[\"flag\"].shift()\n",
    "\n",
    "        concatenated_weeks = concatenated_weeks.loc[\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"] == \"off\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"] == \"on\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We have some weeks where there is no data and thus if this weeks are between an \"on\" and \"off\" flag they will not\n",
    "        # appear on the dataframe. What we will do is a left join with the variable week generated at the begining\n",
    "        # of the notebook to see the rows that do not appear. Then we will activate the alarm at the begining of the week\n",
    "        # and deactivate it at the end of the same week:\n",
    "\n",
    "        # Set the day to monday\n",
    "        concatenated_weeks[\"week\"] = concatenated_weeks[\"dated\"].apply(lambda x: return_monday(x))\n",
    "        # Left join with weeks to detect the missing weeks\n",
    "        concatenated_weeks_merged = pd.merge(weeks, concatenated_weeks, on=\"week\", how=\"left\")\n",
    "\n",
    "        # Store the result of the filling:\n",
    "        filled_dataframe = pd.DataFrame()\n",
    "\n",
    "        # We add the first row\n",
    "        if not pd.isna(concatenated_weeks_merged.iloc[0][\"alarm\"]):\n",
    "            first_new_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"week\": [concatenated_weeks_merged.iloc[0][\"week\"]],\n",
    "                    \"alarm\": [concatenated_weeks_merged.iloc[0][\"alarm\"]],\n",
    "                    \"dated\": [concatenated_weeks_merged.iloc[0][\"dated\"]],\n",
    "                    \"flag\": [concatenated_weeks_merged.iloc[0][\"flag\"]],\n",
    "                    \"id\": [concatenated_weeks_merged.iloc[0][\"id\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            filled_dataframe = pd.concat(\n",
    "                [filled_dataframe, first_new_row],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Begin the iteration where we will add one by one the rows to the dataframe filled_dataframe\n",
    "        for i in range(1, len(concatenated_weeks_merged)-1):\n",
    "\n",
    "            current_row = concatenated_weeks_merged.iloc[i]\n",
    "\n",
    "            # If we find a normal row we add it to the dataframe\n",
    "            if (current_row[\"alarm\"] == \"mcbpower\") | (current_row[\"alarm\"] == \"turn_off_end_week\") | (current_row[\"alarm\"] == \"turn_on_begining_week\"):\n",
    "                current_row[\"dated\"] = pd.to_datetime(current_row[\"dated\"])\n",
    "\n",
    "                new_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"week\": [current_row[\"week\"]],\n",
    "                        \"alarm\": [current_row[\"alarm\"]],\n",
    "                        \"dated\": [current_row[\"dated\"]],\n",
    "                        \"flag\": [current_row[\"flag\"]],\n",
    "                        \"id\": [current_row[\"id\"]]\n",
    "                    }\n",
    "                )\n",
    "                filled_dataframe = pd.concat([filled_dataframe, new_row], sort=True)\n",
    "\n",
    "            if not filled_dataframe.empty:\n",
    "                # If the last row of the filled dataframe is a \"turn_off_week\" ant the current is a Emty we add the \"on\" and\n",
    "                # off for the begining and the end of the week\n",
    "                last_row_filled_dataframe = filled_dataframe.iloc[-1]\n",
    "\n",
    "                if (pd.isna(current_row[\"alarm\"])) & (last_row_filled_dataframe[\"alarm\"] in [\"turn_off_end_week\", \"turn_off_end_week_filled\"]):\n",
    "\n",
    "                    new_row_begining_dated = datetime.datetime.combine(current_row[\"week\"], datetime.time(0,0,0))\n",
    "                    new_row_begining_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_on_begining_week_filled\"],\n",
    "                            \"dated\": [new_row_begining_dated], # Add the time to the date\n",
    "                            \"flag\": [\"on\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "                    # We have to add 6 days because the representative of the week in this case is \n",
    "                    new_row_end_dated = datetime.datetime.combine(current_row[\"week\"]+ pd.Timedelta(days=6), datetime.time(23,59,59))\n",
    "                    new_row_end_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_off_end_week_filled\"],\n",
    "                            \"dated\": [new_row_end_dated], # In this case we add the time to represent the last moment of the week\n",
    "                            \"flag\": [\"off\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    filled_dataframe = pd.concat(\n",
    "                        [filled_dataframe, new_row_begining_week, new_row_end_week],\n",
    "                        sort=True\n",
    "                    )\n",
    "\n",
    "        # Again we have to do a left join with the dataframe weeks to detect the NaN values:\n",
    "        filled_dataframe_merged = pd.merge(weeks, filled_dataframe, on=\"week\", how=\"left\")\n",
    "        \n",
    "        # Now for each one of the weeks we have to calculate the total time passed between an \"on\" alarm and an \"off\" alarm\n",
    "\n",
    "        # In this dataframe we will store the amount of hours of a ebox that has been malfunctioning for each week\n",
    "        week_hours_dataframe = pd.DataFrame()\n",
    "        for week in weeks[\"week\"]:\n",
    "            # In this variable we will store the amount of hours for this week:\n",
    "            total_hours = 0\n",
    "            on_timestamp = None\n",
    "\n",
    "            # Dataframe with the alarms of the week:\n",
    "            week_alarms_dataframe = filled_dataframe_merged.loc[filled_dataframe_merged[\"week\"] == week]\n",
    "            # Iterate trough the df to count the hours:\n",
    "            for _, row in week_alarms_dataframe.iterrows():\n",
    "                if row[\"flag\"] == \"on\":\n",
    "                    on_timestamp = row[\"dated\"]\n",
    "\n",
    "                if (row[\"flag\"] == \"off\") & (on_timestamp is not None):\n",
    "                    total_hours += (row[\"dated\"] - on_timestamp).total_seconds() / 3600\n",
    "\n",
    "                    on_timestamp = None\n",
    "\n",
    "            new_week_hours = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "                    \"week\": [week],\n",
    "                    \"malfunctioning_hours\": [total_hours] \n",
    "                }\n",
    "            )\n",
    "\n",
    "            week_hours_dataframe = pd.concat(\n",
    "                [week_hours_dataframe, new_week_hours],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Now we want to get the data in the format:\n",
    "        # row = {\"week-4\": date_week_prev_4, \"hours_week-4\": hours_week_prev_4, ..., \"week-1\": date_week_prev_1, \"hours_week-1\": hours_week_prev_1, \"current_week\": date_current_week, \"hours_current_week\":  \"week+1\": date_week_next_1, \"hours_week+1\": hours_week_next_1 ..., \"week+4\": date_week_next_4, \"hours_week+4\": hours_week_next_4}\n",
    "\n",
    "        # In this dataframe we will store the data in the format we have mentioned:\n",
    "        lag_dataframe = pd.DataFrame()\n",
    "\n",
    "        # Here we have to follow two different paths. The first one will be when preparing the data for training and the other one will be when \n",
    "        # preparing the data for predicting. We shall begin with predicting:\n",
    "\n",
    "        if for_predicting:\n",
    "            lag_dataframe = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "\n",
    "                    \"week-4\": [week_hours_dataframe.iloc[0][\"week\"]],\n",
    "                    \"hours_week-4\": [week_hours_dataframe.iloc[0][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-3\": [week_hours_dataframe.iloc[1][\"week\"]],\n",
    "                    \"hours_week-3\": [week_hours_dataframe.iloc[1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-2\": [week_hours_dataframe.iloc[2][\"week\"]],\n",
    "                    \"hours_week-2\": [week_hours_dataframe.iloc[2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-1\": [week_hours_dataframe.iloc[3][\"week\"]],\n",
    "                    \"hours_week-1\": [week_hours_dataframe.iloc[3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"current_week\": [week_hours_dataframe.iloc[4][\"week\"]],\n",
    "                    \"hours_current_week\": [week_hours_dataframe.iloc[4][\"malfunctioning_hours\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Begin the loop at 4 and end at -4 so we don't get the error: \"Out of range\"\n",
    "            for i in range(4, len(weeks)-4):\n",
    "                # Create the new row to add:\n",
    "                to_add_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": [idd],\n",
    "\n",
    "                        \"week-4\": [week_hours_dataframe.iloc[i-4][\"week\"]],\n",
    "                        \"hours_week-4\": [week_hours_dataframe.iloc[i-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-3\": [week_hours_dataframe.iloc[i-3][\"week\"]],\n",
    "                        \"hours_week-3\": [week_hours_dataframe.iloc[i-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-2\": [week_hours_dataframe.iloc[i-2][\"week\"]],\n",
    "                        \"hours_week-2\": [week_hours_dataframe.iloc[i-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-1\": [week_hours_dataframe.iloc[i-1][\"week\"]],\n",
    "                        \"hours_week-1\": [week_hours_dataframe.iloc[i-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"current_week\": [week_hours_dataframe.iloc[i][\"week\"]],\n",
    "                        \"hours_current_week\": [week_hours_dataframe.iloc[i][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+1\": [week_hours_dataframe.iloc[i+1][\"week\"]],\n",
    "                        \"hours_week+1\": [week_hours_dataframe.iloc[i+1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+2\": [week_hours_dataframe.iloc[i+2][\"week\"]],\n",
    "                        \"hours_week+2\": [week_hours_dataframe.iloc[i+2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+3\": [week_hours_dataframe.iloc[i+3][\"week\"]],\n",
    "                        \"hours_week+3\": [week_hours_dataframe.iloc[i+3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+4\": [week_hours_dataframe.iloc[i+4][\"week\"]],\n",
    "                        \"hours_week+4\": [week_hours_dataframe.iloc[i+4][\"malfunctioning_hours\"]]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                lag_dataframe = pd.concat(\n",
    "                    [lag_dataframe, to_add_row],\n",
    "                    sort=True,\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "    # Reordenate with the list columns_order:\n",
    "    general_lag_dataframe = general_lag_dataframe[columns_order]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Execution time:\" + str(end_time - start_time))\n",
    "\n",
    "    return general_lag_dataframe\n",
    "\n",
    "def change_format(date: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the date from a datetime object and convert it into a string\n",
    "    \"\"\"\n",
    "    return str(date)[:10]\n",
    "\n",
    "def join_eboxes_alarms_readings_meteo(\n",
    "        eboxes_alarms: pd.DataFrame, \n",
    "        eboxes_powerReactivePeak: pd.DataFrame,\n",
    "        eboxes_powerReactive: pd.DataFrame,\n",
    "        eboxes_powerActive: pd.DataFrame,\n",
    "        eboxes_powerActivePeak: pd.DataFrame,\n",
    "        meteo: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins the preprocessed data from light_alarms, readings, and meteo data by weeks\n",
    "    \"\"\"\n",
    "\n",
    "    #Change meteo date format\n",
    "    meteo['dated'] = meteo['dated'].apply(change_format)\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "        eboxes_alarms[week] = eboxes_alarms[week].apply(lambda x: str(x))\n",
    "\n",
    "    display(eboxes_alarms)\n",
    "    display(eboxes_powerReactivePeak)\n",
    "    # joins for the dataset eboxes_powerReactivePeak:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerReactivePeak.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(columns={\"ReactivePeak\": \"ReactivePeak_\" + week}, inplace=True)\n",
    "\n",
    "    print(\"Here!\")\n",
    "    # joins for the dataset eboxes_powerReactive:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerReactive.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"powerReactive_sum\": \"powerReactive_sum_\" + week,\n",
    "                \"powerReactive_p1\": \"powerReactive_p1_\" + week,\n",
    "                \"powerReactive_p2\": \"powerReactive_p2_\" + week,\n",
    "                \"powerReactive_p3\": \"powerReactive_p3_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "    \n",
    "    print(\"here!\")\n",
    "\n",
    "    # joins for the dataset eboxes_powerActivePeak:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerActivePeak.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(columns={\"ActivePeak\": \"ActivePeak_\" + week}, inplace=True)\n",
    "\n",
    "    # joins for the dataset eboxes_powerActive:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            eboxes_powerActive.rename(columns={\"dated\": week}), \n",
    "            on=[\"id\", week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"powerActive_sum\": \"powerActive_sum_\" + week,\n",
    "                \"powerActive_p1\": \"powerActive_p1_\" + week,\n",
    "                \"powerActive_p2\": \"powerActive_p2_\" + week,\n",
    "                \"powerActive_p3\": \"powerActive_p3_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "        \n",
    "\n",
    "    # joins for the dataset meteo_canyelles:\n",
    "    for week in [\"week-4\", \"week-3\", \"week-2\", \"week-1\", \"current_week\"]:\n",
    "\n",
    "        eboxes_alarms = pd.merge(\n",
    "            eboxes_alarms, \n",
    "            meteo.rename(columns={\"dated\": week}), \n",
    "            on=[week],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        eboxes_alarms.rename(\n",
    "            columns={\n",
    "                \"Dew_max_max\": \"Dew_max_max_\" + week,\n",
    "                \"Hum_min_min\": \"Hum_min_min_\" + week,\n",
    "                \"Wind_max_max\": \"Wind_max_max_\" + week,\n",
    "                \"Temp_max_max\": \"Temp_max_max_\" + week,\n",
    "                \"Temp_min_min\": \"Temp_min_min_\" + week,\n",
    "                \"Pres_min_min\": \"Pres_min_min_\" + week,\n",
    "                \"Wind_avg_mean\": \"Wind_avg_avg_\" + week,\n",
    "                \"Wind_avg_std\": \"Wind_avg_std_\" + week,\n",
    "                \"Hum_avg_mean\": \"Hum_avg_avg_\" + week,\n",
    "                \"Dew_avg_mean\": \"Dew_avg_avg_\" + week,\n",
    "                \"Dew_avg_std\": \"Dew_avg_std_\" + week,\n",
    "                \"Hum_max_max\": \"Hum_max_max_\" + week,\n",
    "                \"Temp_avg_mean\": \"Temp_avg_avg_\" + week,\n",
    "                \"Temp_avg_std\": \"Temp_avg_std_\" + week,\n",
    "                \"Pres_max_max\": \"Pres_max_max_\" + week,\n",
    "                \"Pres_avg_mean\": \"Pres_avg_avg_\" + week,\n",
    "                \"Pres_avg_std\": \"Pres_avg_std_\" + week,\n",
    "                \"Precipitation_mean\": \"Precipitation_avg_\" + week,\n",
    "                \"Precipitation_sum\": \"Precipitation_sum_\" + week,\n",
    "                \"Dew_min_min\": \"Dew_min_min_\" + week,\n",
    "                \"Wind_min_min\": \"Wind_min_min_\" + week,\n",
    "                \"Hum_avg_std\": \"Hum_avg_std_\" + week\n",
    "                    },\n",
    "            inplace=True)\n",
    "    \n",
    "    return eboxes_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA0000011277\n",
      "Execution time:0.04645967483520508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week-4</th>\n",
       "      <th>hours_week-4</th>\n",
       "      <th>week-3</th>\n",
       "      <th>hours_week-3</th>\n",
       "      <th>week-2</th>\n",
       "      <th>hours_week-2</th>\n",
       "      <th>week-1</th>\n",
       "      <th>hours_week-1</th>\n",
       "      <th>current_week</th>\n",
       "      <th>hours_current_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA0000011277</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.061111</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.815278</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      week-4  hours_week-4      week-3  hours_week-3  \\\n",
       "0  AA0000011277  2023-04-17           0.0  2023-04-24           0.0   \n",
       "\n",
       "       week-2  hours_week-2      week-1  hours_week-1 current_week  \\\n",
       "0  2023-05-01      1.061111  2023-05-08      0.815278   2023-05-15   \n",
       "\n",
       "   hours_current_week  \n",
       "0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ebox_id</th>\n",
       "      <th>dated</th>\n",
       "      <th>ReactivePeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA000000AA88</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>1.577872e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA000000AA88</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1.498229e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA000000AA88</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.438945e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA000000AA88</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1.413214e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA000000AA88</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>1.378143e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AA000000D458</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>2.168676e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AA000000D458</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>2.099196e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AA000000D458</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2.045789e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AA000000D458</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>2.040446e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AA000000D458</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>2.023542e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AA000000D492</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>3.380848e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AA000000D492</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>3.319509e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AA000000D492</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>3.279196e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AA000000D492</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>3.245312e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AA000000D492</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>3.212411e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AA000000D494</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>1.778155e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AA000000D494</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1.719524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AA000000D494</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.677515e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AA000000D494</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1.641845e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AA000000D494</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>1.608586e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AA0000011279</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>1.584122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AA0000011279</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1.533274e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AA0000011279</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.480957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AA0000011279</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1.462307e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AA0000011279</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>1.432262e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AA00000112E3</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>1.561265e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AA00000112E3</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1.514747e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AA00000112E3</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.474568e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AA00000112E3</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1.438051e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AA00000112E3</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>1.407530e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AA0000011668</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>8.161161e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AA0000011668</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>7.903720e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AA0000011668</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>7.721726e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AA0000011668</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>7.539435e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AA0000011668</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>7.381101e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AA000001166B</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>5.237485e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>AA000001166B</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>5.179122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AA000001166B</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>5.117113e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AA000001166B</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>5.058110e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AA000001166B</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>4.196243e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>AA0000011689</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>2.085104e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AA0000011689</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>2.023452e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AA0000011689</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>1.975997e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AA0000011689</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1.931801e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AA0000011689</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>1.892336e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ebox_id       dated  ReactivePeak\n",
       "0   AA000000AA88  2023-04-17  1.577872e+06\n",
       "1   AA000000AA88  2023-04-24  1.498229e+06\n",
       "2   AA000000AA88  2023-05-01  1.438945e+06\n",
       "3   AA000000AA88  2023-05-08  1.413214e+06\n",
       "4   AA000000AA88  2023-05-15  1.378143e+06\n",
       "5   AA000000D458  2023-04-17  2.168676e+06\n",
       "6   AA000000D458  2023-04-24  2.099196e+06\n",
       "7   AA000000D458  2023-05-01  2.045789e+06\n",
       "8   AA000000D458  2023-05-08  2.040446e+06\n",
       "9   AA000000D458  2023-05-15  2.023542e+06\n",
       "10  AA000000D492  2023-04-17  3.380848e+06\n",
       "11  AA000000D492  2023-04-24  3.319509e+06\n",
       "12  AA000000D492  2023-05-01  3.279196e+06\n",
       "13  AA000000D492  2023-05-08  3.245312e+06\n",
       "14  AA000000D492  2023-05-15  3.212411e+06\n",
       "15  AA000000D494  2023-04-17  1.778155e+06\n",
       "16  AA000000D494  2023-04-24  1.719524e+06\n",
       "17  AA000000D494  2023-05-01  1.677515e+06\n",
       "18  AA000000D494  2023-05-08  1.641845e+06\n",
       "19  AA000000D494  2023-05-15  1.608586e+06\n",
       "20  AA0000011279  2023-04-17  1.584122e+06\n",
       "21  AA0000011279  2023-04-24  1.533274e+06\n",
       "22  AA0000011279  2023-05-01  1.480957e+06\n",
       "23  AA0000011279  2023-05-08  1.462307e+06\n",
       "24  AA0000011279  2023-05-15  1.432262e+06\n",
       "25  AA00000112E3  2023-04-17  1.561265e+06\n",
       "26  AA00000112E3  2023-04-24  1.514747e+06\n",
       "27  AA00000112E3  2023-05-01  1.474568e+06\n",
       "28  AA00000112E3  2023-05-08  1.438051e+06\n",
       "29  AA00000112E3  2023-05-15  1.407530e+06\n",
       "30  AA0000011668  2023-04-17  8.161161e+05\n",
       "31  AA0000011668  2023-04-24  7.903720e+05\n",
       "32  AA0000011668  2023-05-01  7.721726e+05\n",
       "33  AA0000011668  2023-05-08  7.539435e+05\n",
       "34  AA0000011668  2023-05-15  7.381101e+05\n",
       "35  AA000001166B  2023-04-17  5.237485e+06\n",
       "36  AA000001166B  2023-04-24  5.179122e+06\n",
       "37  AA000001166B  2023-05-01  5.117113e+06\n",
       "38  AA000001166B  2023-05-08  5.058110e+06\n",
       "39  AA000001166B  2023-05-15  4.196243e+06\n",
       "40  AA0000011689  2023-04-17  2.085104e+06\n",
       "41  AA0000011689  2023-04-24  2.023452e+06\n",
       "42  AA0000011689  2023-05-01  1.975997e+06\n",
       "43  AA0000011689  2023-05-08  1.931801e+06\n",
       "44  AA0000011689  2023-05-15  1.892336e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19320/4098002106.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredicting_min_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicting_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredicting_max_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicting_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m eboxes_alarms = join_eboxes_alarms_readings_meteo(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0meboxes_alarms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meboxes_alarms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0meboxes_powerReactivePeak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpowerReactivePeak\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0meboxes_powerReactive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpowerReactive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19320/570793281.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(eboxes_alarms, eboxes_powerReactivePeak, eboxes_powerReactive, eboxes_powerActive, eboxes_powerActivePeak, meteo)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meboxes_powerReactivePeak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;31m# joins for the dataset eboxes_powerReactivePeak:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mweek\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"week-4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"week-3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"week-2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"week-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_week\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         eboxes_alarms = pd.merge(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0meboxes_alarms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0meboxes_powerReactivePeak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dated\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweek\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 110\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m         (\n\u001b[1;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/predic_maint/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "eboxes_alarms = pd.read_csv(\"/home/leibniz/Desktop/IHMAN/data_to_predict/eboxes_alarms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = len(pd.DataFrame()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not True:\n",
    "    print(\"oel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_preprocess_lights(light_alarms: pd.DataFrame, predicting_min_date: str = None, predicting_max_date: str = None, for_predicting: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The function converts the data of the alarms into a dataframe that is usable for training the models.\n",
    "    Review the coments on the code to understand step by step what the code does.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # We create a dataframe of each one of the ids and we will clean the alarms. The final objective is to calculate\n",
    "    # the percentage of time the lampost is not working wich is the time that passes from an alarm \"on\" utill it is \n",
    "    # turned \"off\". In the dataframe we have cases in wich there are two consecutive \"on\" or \"off\" alarms so we have\n",
    "    # filter and just keep the first \"on\" and the first \"off\" in this case so we get the real time the lampost has been\n",
    "    # not functioning. In the case that the alarm is turned \"on\" during a week and it is not turned \"off\" untill the next\n",
    "    # week we will insert a fake \"off\" alarm at the last moment of the week and we will turn it \"on\" again in the first moment\n",
    "    # of the following week.\n",
    "\n",
    "    # The first step will be to dowload the data:\n",
    "    # Read recipe inputs\n",
    "\n",
    "    df = light_alarms.copy()\n",
    "\n",
    "    # Sort by id and date of the alarm:\n",
    "    df[\"dated\"] = pd.to_datetime(df[\"dated\"])\n",
    "    df = df.sort_values([\"id\", \"dated\"])\n",
    "    # As far as we know if an alarm is \"set\" it means it is \"off\" so to keep things simple we will sustitute the\n",
    "    # \"set\" values with \"off\"\n",
    "    df.replace(to_replace=\"set\", value=\"off\", inplace=True)\n",
    "\n",
    "    # Generate all the weeks from the first day we have data untill the last day. We will need this weeks later to\n",
    "    # do a left join with the weeks that there are alarms\n",
    "\n",
    "    # If we are preprocessing the data for doing predictions we will want to add the date_min and date max manualy,\n",
    "    # otherwise the function will only consider the dates of the errors and you may end up with nan rows\n",
    "    if for_predicting: \n",
    "        start_date = pd.to_datetime(predicting_min_date)\n",
    "        end_date = pd.to_datetime(predicting_max_date)\n",
    "    else:\n",
    "        start_date = df[\"dated\"].min()\n",
    "        end_date = return_monday(df[\"dated\"].max()) + pd.Timedelta(days=7)\n",
    "    \n",
    "    weeks = pd.date_range(start=start_date, end=end_date, freq='W').floor(\"D\").strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    # Transform to a dataframe to do operations later:\n",
    "    weeks = pd.DataFrame(weeks, columns=[\"week\"])\n",
    "    weeks[\"week\"] = pd.to_datetime(weeks[\"week\"])\n",
    "    weeks[\"week\"] = weeks[\"week\"].apply(lambda x: return_monday(x))\n",
    "    # For each unique id we will implement all the code:\n",
    "    # First create the dataframe where we will store all the resturned data:\n",
    "    general_lag_dataframe = pd.DataFrame()\n",
    "\n",
    "    # List of all the ids:\n",
    "    ids_list = df[\"id\"].unique()\n",
    "\n",
    "    # Store the name of the columns in oder to later reordenate the final returned dataframe.\n",
    "    if for_predicting:\n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\"]\n",
    "    else:     \n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\", \"week+1\", \"hours_week+1\", \"week+2\", \"hours_week+2\", \"week+3\", \"hours_week+3\", \"week+4\", \"hours_week+4\"]\n",
    "\n",
    "    # Generate a dataframe for each one of the ids and then apply all the transformations. Once done, add the data\n",
    "    # to the dataframe general_lag_dataframe.\n",
    "    for idd in ids_list:\n",
    "        print(idd)\n",
    "        \n",
    "        # Dataframe of all the alarms for the id \"idd\":\n",
    "        tt = df.loc[df[\"id\"] == idd]\n",
    "        \n",
    "        #For each one of the elements in the list we have: elem[0] contains the week represented by sunday \n",
    "        # and elem[1] has the data in a dataframe\n",
    "        grouped_weeks = list(tt.groupby(pd.Grouper(key=\"dated\", freq=\"W\")))\n",
    "        # Here we will transform all the tuples to lists so we can change the elements after\n",
    "        grouped_weeks = [list(elem) for elem in grouped_weeks]\n",
    "\n",
    "        # Now we have to filtrate the empty dataframes that generates groupby for the weeks where there are no alarms\n",
    "        grouped_weeks = [elem for elem in grouped_weeks if not elem[1].empty]\n",
    "\n",
    "        # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\" for the first element\n",
    "        # of the list grouped_weeks. We do it because in the iteration we will not consider it so we have to do it now\n",
    "        first_week_data = grouped_weeks[0][1]\n",
    "        first_week_data[\"prev_flag\"] = first_week_data[\"flag\"].shift()\n",
    "\n",
    "        first_week_data = first_week_data.loc[\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"] == \"off\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"] == \"on\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We store it to the data of the first week:\n",
    "        grouped_weeks[0][1] = first_week_data\n",
    "\n",
    "        # Here we begin the iteration for all the other weeks:\n",
    "        for i, (week, data) in enumerate(grouped_weeks[1:], 1): # We do not consider the first week because it has no previous week to check\n",
    "            # Get the data and week from the previous week:\n",
    "            previous_week, previous_week_data = grouped_weeks[i-1][0], grouped_weeks[i-1][1]\n",
    "\n",
    "            # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\"\n",
    "            data[\"prev_flag\"] = data[\"flag\"].shift()\n",
    "            data = data.loc[\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"] == \"off\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"] == \"on\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"].isna()))\n",
    "            ]\n",
    "\n",
    "            # Data that we will need:\n",
    "            last_moment_previous_week = previous_week + pd.Timedelta(hours=23, minutes=59, seconds=59) #We will need the last moment of the week\n",
    "            first_moment_current_week = (week - pd.Timedelta(days=week.dayofweek)).replace(hour=0, minute=0, second=0) #We will need to the first moment of the current week\n",
    "\n",
    "            #We have to get the last flag of the previous week\n",
    "            last_flag_previous_week = previous_week_data.loc[previous_week_data.index[-1], \"flag\"]\n",
    "\n",
    "            if last_flag_previous_week == \"on\":\n",
    "                # If the last flag from the previous week is \"on\" then we have to set a new row on the previous week data\n",
    "                # in the last position to set a flag \"off\". Then, in the current week data we will add a new row before the\n",
    "                # first week to set again the alarm to \"on\"\n",
    "\n",
    "                # Here we create the new row to set the alarm \"off\" in the previous week\n",
    "                new_row_previous_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [last_moment_previous_week],\n",
    "                    \"alarm\": [\"turn_off_end_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"off\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very end of the week\n",
    "                new_previous_week_data = pd.concat(\n",
    "                    [previous_week_data, new_row_previous_week],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i-1][1] = new_previous_week_data\n",
    "\n",
    "                # Now we have to set the flag \"on\" in the first moment of the current week:\n",
    "                # Here we create the new row to set the alarm \"on\" in the current week\n",
    "                new_row_current_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [first_moment_current_week],\n",
    "                    \"alarm\": [\"turn_on_begining_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"on\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very begining of the week\n",
    "                new_current_week_data = pd.concat(\n",
    "                    [new_row_current_week, data],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i][1] = new_current_week_data\n",
    "            else:\n",
    "                # Simply update the dataframe with the same but with removed rows that contain two identical flags in a row\n",
    "                grouped_weeks[i][1] = data\n",
    "\n",
    "        # Once this is done we have to check if the last alarm of the last week is \"on\". In this case we will add a row turning it off\n",
    "        # in the last moment of the week:\n",
    "        last_recorded_week, last_recorded_week_data = grouped_weeks[-1][0], grouped_weeks[-1][1]\n",
    "\n",
    "        # Get the last flag from the last week\n",
    "        last_flag = last_recorded_week_data[\"flag\"].values[-1]\n",
    "        # and get the last moment of the last week\n",
    "        last_moment_last_recorded_week = last_recorded_week + pd.Timedelta(hours=23, minutes=59, seconds=59)\n",
    "\n",
    "        if last_flag == \"on\":\n",
    "            new_row_last_flag = pd.DataFrame(\n",
    "                {\n",
    "                \"id\": [idd],\n",
    "                \"dated\": [last_moment_last_recorded_week],\n",
    "                \"alarm\": [\"turn_off_end_last_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                \"flag\": [\"off\"],\n",
    "                \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_recorded_week_data = pd.concat(\n",
    "                [last_recorded_week_data, new_row_last_flag],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "            grouped_weeks[-1][1] = last_recorded_week_data\n",
    "\n",
    "        # Now we have to concat all the dataframes from all the weeks contained in the list grouped_weeks in one big dataframe\n",
    "        concatenated_weeks = pd.concat(\n",
    "            [week_data[1] for week_data in grouped_weeks],\n",
    "            sort=True # Remove the warning of pd.concat\n",
    "        )\n",
    "\n",
    "        # At this point there are some cases where we will still have two \"on\" alarms or two \"off\" alarms in a row. For example\n",
    "        # in the case that we have two weeks in a row where we only have \"on\" alarms utill now the code is going to return the \n",
    "        # begining of the end of the previous week with \"off\", the beggining of the current week with \"off\" and before the \"off\"\n",
    "        # of the end of the week we will still have an \"on\" alarm. This is caused because the deletion of the same alarms in a row is done \n",
    "        # before the add of the new rows in the beggining and end of the week\n",
    "\n",
    "        # So let's eliminate this cases too:\n",
    "        # first we have to frop the old prev_flag column that now is useless:\n",
    "        concatenated_weeks.drop(\"prev_flag\", axis=1, inplace=True)\n",
    "\n",
    "        # and create the new one:\n",
    "        concatenated_weeks[\"prev_flag_concat\"] = concatenated_weeks[\"flag\"].shift()\n",
    "\n",
    "        concatenated_weeks = concatenated_weeks.loc[\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"] == \"off\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"] == \"on\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We have some weeks where there is no data and thus if this weeks are between an \"on\" and \"off\" flag they will not\n",
    "        # appear on the dataframe. What we will do is a left join with the variable week generated at the begining\n",
    "        # of the notebook to see the rows that do not appear. Then we will activate the alarm at the begining of the week\n",
    "        # and deactivate it at the end of the same week:\n",
    "\n",
    "        # Set the day to monday\n",
    "        concatenated_weeks[\"week\"] = concatenated_weeks[\"dated\"].apply(lambda x: return_monday(x))\n",
    "        # Left join with weeks to detect the missing weeks\n",
    "        concatenated_weeks_merged = pd.merge(weeks, concatenated_weeks, on=\"week\", how=\"left\")\n",
    "\n",
    "        # Store the result of the filling:\n",
    "        filled_dataframe = pd.DataFrame()\n",
    "\n",
    "        # We add the first row\n",
    "        if not pd.isna(concatenated_weeks_merged.iloc[0][\"alarm\"]):\n",
    "            first_new_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"week\": [concatenated_weeks_merged.iloc[0][\"week\"]],\n",
    "                    \"alarm\": [concatenated_weeks_merged.iloc[0][\"alarm\"]],\n",
    "                    \"dated\": [concatenated_weeks_merged.iloc[0][\"dated\"]],\n",
    "                    \"flag\": [concatenated_weeks_merged.iloc[0][\"flag\"]],\n",
    "                    \"id\": [concatenated_weeks_merged.iloc[0][\"id\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            filled_dataframe = pd.concat(\n",
    "                [filled_dataframe, first_new_row],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Begin the iteration where we will add one by one the rows to the dataframe filled_dataframe\n",
    "        for i in range(1, len(concatenated_weeks_merged)-1):\n",
    "\n",
    "            current_row = concatenated_weeks_merged.iloc[i]\n",
    "\n",
    "            # If we find a normal row we add it to the dataframe\n",
    "            if (current_row[\"alarm\"] == \"lightcomm\") | (current_row[\"alarm\"] == \"lighterr\") | (current_row[\"alarm\"] == \"turn_off_end_week\") | (current_row[\"alarm\"] == \"turn_on_begining_week\"):\n",
    "                current_row[\"dated\"] = pd.to_datetime(current_row[\"dated\"])\n",
    "\n",
    "                new_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"week\": [current_row[\"week\"]],\n",
    "                        \"alarm\": [current_row[\"alarm\"]],\n",
    "                        \"dated\": [current_row[\"dated\"]],\n",
    "                        \"flag\": [current_row[\"flag\"]],\n",
    "                        \"id\": [current_row[\"id\"]]\n",
    "                    }\n",
    "                )\n",
    "                filled_dataframe = pd.concat([filled_dataframe, new_row], sort=True)\n",
    "\n",
    "            if not filled_dataframe.empty:\n",
    "                # If the last row of the filled dataframe is a \"turn_off_week\" ant the current is a Emty we add the \"on\" and\n",
    "                # off for the begining and the end of the week\n",
    "                last_row_filled_dataframe = filled_dataframe.iloc[-1]\n",
    "\n",
    "                if (pd.isna(current_row[\"alarm\"])) & (last_row_filled_dataframe[\"alarm\"] in [\"turn_off_end_week\", \"turn_off_end_week_filled\"]):\n",
    "\n",
    "                    new_row_begining_dated = datetime.datetime.combine(current_row[\"week\"], datetime.time(0,0,0))\n",
    "                    new_row_begining_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_on_begining_week_filled\"],\n",
    "                            \"dated\": [new_row_begining_dated], # Add the time to the date\n",
    "                            \"flag\": [\"on\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "                    # We have to add 6 days because the representative of the week in this case is \n",
    "                    new_row_end_dated = datetime.datetime.combine(current_row[\"week\"]+ pd.Timedelta(days=6), datetime.time(23,59,59))\n",
    "                    new_row_end_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_off_end_week_filled\"],\n",
    "                            \"dated\": [new_row_end_dated], # In this case we add the time to represent the last moment of the week\n",
    "                            \"flag\": [\"off\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    filled_dataframe = pd.concat(\n",
    "                        [filled_dataframe, new_row_begining_week, new_row_end_week],\n",
    "                        sort=True\n",
    "                    )\n",
    "\n",
    "\n",
    "        # Again we have to do a left join with the dataframe weeks to detect the NaN values:\n",
    "        filled_dataframe_merged = pd.merge(weeks, filled_dataframe, on=\"week\", how=\"left\")\n",
    "\n",
    "        # At this point there are some cases where the last alarm of the dataframe is an \"on\" so we have to turn the alarm off at the end of the week:\n",
    "        if filled_dataframe_merged[\"flag\"].iloc[-1] == \"on\": # Check if the last alarm is still on\n",
    "            \n",
    "            # get the last moment of the week\n",
    "            end_of_week = pd.to_datetime(filled_dataframe_merged[\"week\"].iloc[-1]) + pd.Timedelta(days=6, hours=23, minutes=59, seconds=59)\n",
    "\n",
    "            row_to_add = pd.DataFrame(\n",
    "                {\n",
    "                    \"week\": [filled_dataframe_merged[\"week\"].iloc[-1]],\n",
    "                    \"alarm\": [\"turn_off_end_week\"],\n",
    "                    \"dated\": end_of_week,\n",
    "                    \"flag\": [\"off\"],\n",
    "                    \"id\": idd\n",
    "                }\n",
    "            )\n",
    "\n",
    "            filled_dataframe_merged = pd.concat(\n",
    "                [filled_dataframe_merged, row_to_add],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "        # Now for each one of the weeks we have to calculate the total time passed between an \"on\" alarm and an \"off\" alarm\n",
    "\n",
    "        # In this dataframe we will store the amount of hours of a light that has been malfunctioning for each week\n",
    "        week_hours_dataframe = pd.DataFrame()\n",
    "        for week in weeks[\"week\"]:\n",
    "            # In this variable we will store the amount of hours for this week:\n",
    "            total_hours = 0\n",
    "            on_timestamp = None\n",
    "\n",
    "            # Dataframe with the alarms of the week:\n",
    "            week_alarms_dataframe = filled_dataframe_merged.loc[filled_dataframe_merged[\"week\"] == week]\n",
    "            # Iterate trough the df to count the hours:\n",
    "            for _, row in week_alarms_dataframe.iterrows():\n",
    "                if row[\"flag\"] == \"on\":\n",
    "                    on_timestamp = row[\"dated\"]\n",
    "\n",
    "                if (row[\"flag\"] == \"off\") & (on_timestamp is not None):\n",
    "                    total_hours += (row[\"dated\"] - on_timestamp).total_seconds() / 3600\n",
    "\n",
    "                    on_timestamp = None\n",
    "\n",
    "            new_week_hours = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "                    \"week\": [week],\n",
    "                    \"malfunctioning_hours\": [total_hours] \n",
    "                }\n",
    "            )\n",
    "\n",
    "            week_hours_dataframe = pd.concat(\n",
    "                [week_hours_dataframe, new_week_hours],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Now we want to get the data in the format:\n",
    "        # row = {\"week-4\": date_week_prev_4, \"hours_week-4\": hours_week_prev_4, ..., \"week-1\": date_week_prev_1, \"hours_week-1\": hours_week_prev_1, \"current_week\": date_current_week, \"hours_current_week\":  \"week+1\": date_week_next_1, \"hours_week+1\": hours_week_next_1 ..., \"week+4\": date_week_next_4, \"hours_week+4\": hours_week_next_4}\n",
    "\n",
    "        # In this dataframe we will store the data in the format we have mentioned:\n",
    "        lag_dataframe = pd.DataFrame()\n",
    "        \n",
    "        # Here we have to follow two different paths. The first one will be when preparing the data for training and the other one will be when \n",
    "        # preparing the data for predicting. We shall begin with predicting:\n",
    "\n",
    "        if for_predicting:\n",
    "            lag_dataframe = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "\n",
    "                    \"week-4\": [week_hours_dataframe.iloc[-4][\"week\"]],\n",
    "                    \"hours_week-4\": [week_hours_dataframe.iloc[-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-3\": [week_hours_dataframe.iloc[-3][\"week\"]],\n",
    "                    \"hours_week-3\": [week_hours_dataframe.iloc[-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-2\": [week_hours_dataframe.iloc[-2][\"week\"]],\n",
    "                    \"hours_week-2\": [week_hours_dataframe.iloc[-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-1\": [week_hours_dataframe.iloc[-1][\"week\"]],\n",
    "                    \"hours_week-1\": [week_hours_dataframe.iloc[-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"current_week\": [week_hours_dataframe.iloc[0][\"week\"]],\n",
    "                    \"hours_current_week\": [week_hours_dataframe.iloc[0][\"malfunctioning_hours\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Begin the loop at 4 and end at -4 so we don't get the error: \"Out of range\"\n",
    "            for i in range(4, len(weeks)-4):\n",
    "                \n",
    "                # Create the new row to add:\n",
    "                to_add_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": [idd],\n",
    "\n",
    "                        \"week-4\": [week_hours_dataframe.iloc[i-4][\"week\"]],\n",
    "                        \"hours_week-4\": [week_hours_dataframe.iloc[i-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-3\": [week_hours_dataframe.iloc[i-3][\"week\"]],\n",
    "                        \"hours_week-3\": [week_hours_dataframe.iloc[i-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-2\": [week_hours_dataframe.iloc[i-2][\"week\"]],\n",
    "                        \"hours_week-2\": [week_hours_dataframe.iloc[i-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-1\": [week_hours_dataframe.iloc[i-1][\"week\"]],\n",
    "                        \"hours_week-1\": [week_hours_dataframe.iloc[i-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"current_week\": [week_hours_dataframe.iloc[i][\"week\"]],\n",
    "                        \"hours_current_week\": [week_hours_dataframe.iloc[i][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+1\": [week_hours_dataframe.iloc[i+1][\"week\"]],\n",
    "                        \"hours_week+1\": [week_hours_dataframe.iloc[i+1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+2\": [week_hours_dataframe.iloc[i+2][\"week\"]],\n",
    "                        \"hours_week+2\": [week_hours_dataframe.iloc[i+2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+3\": [week_hours_dataframe.iloc[i+3][\"week\"]],\n",
    "                        \"hours_week+3\": [week_hours_dataframe.iloc[i+3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+4\": [week_hours_dataframe.iloc[i+4][\"week\"]],\n",
    "                        \"hours_week+4\": [week_hours_dataframe.iloc[i+4][\"malfunctioning_hours\"]]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                lag_dataframe = pd.concat(\n",
    "                    [lag_dataframe, to_add_row],\n",
    "                    sort=True,\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "    # Reordenate with the list columns_order:\n",
    "    general_lag_dataframe = general_lag_dataframe[columns_order]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Execution time:\" + str(end_time - start_time))\n",
    "\n",
    "    return general_lag_dataframe\n",
    "\n",
    "def big_preprocess_eboxes(eboxes_alarms: pd.DataFrame, predicting_min_date: str = None, predicting_max_date: str = None, for_predicting: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The function converts the data of the alarms into a dataframe that is usable for training the models.\n",
    "    Review the coments on the code to understand step by step what the code does.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # We create a dataframe of each one of the ids and we will clean the alarms. The final objective is to calculate\n",
    "    # the percentage of time the ebox is not working wich is the time that passes from an alarm \"off\" utill it is \n",
    "    # turned \"on\". In the dataframe we have cases in wich there are two consecutive \"on\" or \"off\" alarms so we have\n",
    "    # filter and just keep the first \"on\" and the first \"off\" in this case so we get the real time the ebox has been\n",
    "    # not functioning. In the case that the alarm is turned \"on\" during a week and it is not turned \"off\" untill the next\n",
    "    # week we will insert a fake \"off\" alarm at the last moment of the week and we will turn it \"on\" again in the first moment\n",
    "    # of the following week.\n",
    "\n",
    "    # The first step will be to dowload the data:\n",
    "    # Read recipe inputs\n",
    "    df = eboxes_alarms.copy()\n",
    "\n",
    "    # We will reuse the code of the light alarms for the eboxes: The main thing that we have to modify for the eboxes\n",
    "    # is that for the alarm subtype brdpower the flag \"off\" means that the ebox is suffering a breakdown and untill the\n",
    "    # alarm turns \"on\" we will consider that the ebox has been down. This is exactly opposite to what happened with the\n",
    "    # lamposts.\n",
    "\n",
    "    # The first step is to change the name of the column subtype. Even tough this columns means a subtype of alarm, in this\n",
    "    # code and from now on we will rename it as alarm so we are able to replicate a similar code as the one used for the lights\n",
    "    df.rename(columns={\"subtype\": \"alarm\"}, inplace=True)\n",
    "\n",
    "    # There are four different flags in this case: {off, on, offM, onM} depending if the error has been detected digitaly or manualy.\n",
    "    # we do not care so we fill change this categories to {on off}:\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"onM\"], \"on\")\n",
    "    df[\"flag\"] = df[\"flag\"].replace([\"offM\"], \"off\")\n",
    "\n",
    "    # As we already said the flags in the eboxes are opposite as the one in the lamposts, so to be able to use the same\n",
    "    # code we will just change the values \"off\" to \"on\" and the \"on\" to \"off\" and then apply the same exact code that\n",
    "    # we used for the lamposts.\n",
    "    df[\"flag\"] = df[\"flag\"].replace({'on': 'offf', 'off': 'on'}).replace({\"offf\": \"off\"})\n",
    "\n",
    "    # Sort by id and date of the alarm:\n",
    "    df[\"dated\"] = pd.to_datetime(df[\"dated\"])\n",
    "    df = df.sort_values([\"id\", \"dated\"])\n",
    "\n",
    "    # Generate all the weeks from the first day we have data untill the last day. We will need this weeks later to\n",
    "    # do a left join with the weeks that there are alarms\n",
    "\n",
    "    # If we are preprocessing the data for doing predictions we will want to add the date_min and date max manualy,\n",
    "    # otherwise the function will only consider the dates of the errors and you may end up with nan rows\n",
    "    if for_predicting: \n",
    "        start_date = pd.to_datetime(predicting_min_date)\n",
    "        end_date = pd.to_datetime(predicting_max_date)\n",
    "    else:\n",
    "        start_date = df[\"dated\"].min()\n",
    "        end_date = return_monday(df[\"dated\"].max()) + pd.Timedelta(days=7)\n",
    "\n",
    "    weeks = pd.date_range(start=start_date, end=end_date, freq='W').floor(\"D\").strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    # Transform to a dataframe to do operations later:\n",
    "    weeks = pd.DataFrame(weeks, columns=[\"week\"])\n",
    "    weeks[\"week\"] = pd.to_datetime(weeks[\"week\"])\n",
    "    weeks[\"week\"] = weeks[\"week\"].apply(lambda x: return_monday(x))\n",
    "\n",
    "    # For each unique id we will implement all the code:\n",
    "    # First create the dataframe where we will store all the resturned data:\n",
    "    general_lag_dataframe = pd.DataFrame()\n",
    "\n",
    "    # List of all the ids:\n",
    "    ids_list = df[\"id\"].unique()\n",
    "\n",
    "    # Store the name of the columns in oder to later reordenate the final returned dataframe.\n",
    "    if for_predicting:\n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\"]\n",
    "    else:     \n",
    "        columns_order = [\"id\", \"week-4\", \"hours_week-4\", \"week-3\", \"hours_week-3\", \"week-2\", \"hours_week-2\", \"week-1\", \"hours_week-1\",\n",
    "                        \"current_week\", \"hours_current_week\", \"week+1\", \"hours_week+1\", \"week+2\", \"hours_week+2\", \"week+3\", \"hours_week+3\", \"week+4\", \"hours_week+4\"]\n",
    "\n",
    "    # Generate a dataframe for each one of the ids and then apply all the transformations. Once done, add the data\n",
    "    # to the dataframe general_lag_dataframe.\n",
    "    for idd in ids_list:\n",
    "        print(idd)\n",
    "        \n",
    "        # Dataframe of all the alarms for the id \"idd\":\n",
    "        tt = df.loc[df[\"id\"] == idd]\n",
    "        \n",
    "        #For each one of the elements in the list we have: elem[0] contains the week represented by sunday \n",
    "        # and elem[1] has the data in a dataframe\n",
    "        grouped_weeks = list(tt.groupby(pd.Grouper(key=\"dated\", freq=\"W\")))\n",
    "        # Here we will transform all the tuples to lists so we can change the elements after\n",
    "        grouped_weeks = [list(elem) for elem in grouped_weeks]\n",
    "\n",
    "        # Now we have to filtrate the empty dataframes that generates groupby for the weeks where there are no alarms\n",
    "        grouped_weeks = [elem for elem in grouped_weeks if not elem[1].empty]\n",
    "\n",
    "        # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\" for the first element\n",
    "        # of the list grouped_weeks. We do it because in the iteration we will not consider it so we have to do it now\n",
    "        first_week_data = grouped_weeks[0][1]\n",
    "        first_week_data[\"prev_flag\"] = first_week_data[\"flag\"].shift()\n",
    "\n",
    "        first_week_data = first_week_data.loc[\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"] == \"off\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"] == \"on\")) |\n",
    "            ((first_week_data[\"flag\"] == \"off\") & (first_week_data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((first_week_data[\"flag\"] == \"on\") & (first_week_data[\"prev_flag\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We store it to the data of the first week:\n",
    "        grouped_weeks[0][1] = first_week_data\n",
    "\n",
    "        # Here we begin the iteration for all the other weeks:\n",
    "        for i, (week, data) in enumerate(grouped_weeks[1:], 1): # We do not consider the first week because it has no previous week to check\n",
    "            # Get the data and week from the previous week:\n",
    "            previous_week, previous_week_data = grouped_weeks[i-1][0], grouped_weeks[i-1][1]\n",
    "\n",
    "            # Eliminate the repeated flags such as \"off\" followed by \"off\" or \"on\" followed by \"on\"\n",
    "            data[\"prev_flag\"] = data[\"flag\"].shift()\n",
    "            data = data.loc[\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"] == \"off\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"] == \"on\")) |\n",
    "                ((data[\"flag\"] == \"off\") & (data[\"prev_flag\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "                ((data[\"flag\"] == \"on\") & (data[\"prev_flag\"].isna()))\n",
    "            ]\n",
    "\n",
    "            # Data that we will need:\n",
    "            last_moment_previous_week = previous_week + pd.Timedelta(hours=23, minutes=59, seconds=59) #We will need the last moment of the week\n",
    "            first_moment_current_week = (week - pd.Timedelta(days=week.dayofweek)).replace(hour=0, minute=0, second=0) #We will need to the first moment of the current week\n",
    "\n",
    "            #We have to get the last flag of the previous week\n",
    "            last_flag_previous_week = previous_week_data.loc[previous_week_data.index[-1], \"flag\"]\n",
    "\n",
    "            if last_flag_previous_week == \"on\":\n",
    "                # If the last flag from the previous week is \"on\" then we have to set a new row on the previous week data\n",
    "                # in the last position to set a flag \"off\". Then, in the current week data we will add a new row before the\n",
    "                # first week to set again the alarm to \"on\"\n",
    "\n",
    "                # Here we create the new row to set the alarm \"off\" in the previous week\n",
    "                new_row_previous_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [last_moment_previous_week],\n",
    "                    \"alarm\": [\"turn_off_end_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"off\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very end of the week\n",
    "                new_previous_week_data = pd.concat(\n",
    "                    [previous_week_data, new_row_previous_week],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i-1][1] = new_previous_week_data\n",
    "\n",
    "                # Now we have to set the flag \"on\" in the first moment of the current week:\n",
    "                # Here we create the new row to set the alarm \"on\" in the current week\n",
    "                new_row_current_week = pd.DataFrame(\n",
    "                    {\n",
    "                    \"id\": [idd],\n",
    "                    \"dated\": [first_moment_current_week],\n",
    "                    \"alarm\": [\"turn_on_begining_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                    \"flag\": [\"on\"],\n",
    "                    \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # We create the new df with the new row at the very begining of the week\n",
    "                new_current_week_data = pd.concat(\n",
    "                    [new_row_current_week, data],\n",
    "                    sort=True # Remove the warning of pd.concat\n",
    "                )\n",
    "                # We update the dataframe in the list grouped_weeks\n",
    "                grouped_weeks[i][1] = new_current_week_data\n",
    "            else:\n",
    "                # Simply update the dataframe with the same but with removed rows that contain two identical flags in a row\n",
    "                grouped_weeks[i][1] = data\n",
    "\n",
    "        # Once this is done we have to check if the last alarm of the last week is \"on\". In this case we will add a row turning it off\n",
    "        # in the last moment of the week:\n",
    "        last_recorded_week, last_recorded_week_data = grouped_weeks[-1][0], grouped_weeks[-1][1]\n",
    "\n",
    "        # Get the last flag from the last week\n",
    "        last_flag = last_recorded_week_data[\"flag\"].values[-1]\n",
    "        # and get the last moment of the last week\n",
    "        last_moment_last_recorded_week = last_recorded_week + pd.Timedelta(hours=23, minutes=59, seconds=59)\n",
    "\n",
    "        if last_flag == \"on\":\n",
    "            new_row_last_flag = pd.DataFrame(\n",
    "                {\n",
    "                \"id\": [idd],\n",
    "                \"dated\": [last_moment_last_recorded_week],\n",
    "                \"alarm\": [\"turn_off_end_last_week\"], # We will put this in alarm to know witch alarms where inserted by us\n",
    "                \"flag\": [\"off\"],\n",
    "                \"prev_flag\": [\"Empty\"] # Empty because we do not need it anymore and it will help us identify this rows\n",
    "                }\n",
    "            )\n",
    "\n",
    "            last_recorded_week_data = pd.concat(\n",
    "                [last_recorded_week_data, new_row_last_flag],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "            grouped_weeks[-1][1] = last_recorded_week_data\n",
    "\n",
    "        # Now we have to concat all the dataframes from all the weeks contained in the list grouped_weeks in one big dataframe\n",
    "        concatenated_weeks = pd.concat(\n",
    "            [week_data[1] for week_data in grouped_weeks],\n",
    "            sort=True # Remove the warning of pd.concat\n",
    "        )\n",
    "\n",
    "        # At this point there are some cases where we will still have two \"on\" alarms or two \"off\" alarms in a row. For example\n",
    "        # in the case that we have two weeks in a row where we only have \"on\" alarms utill now the code is going to return the \n",
    "        # begining of the end of the previous week with \"off\", the beggining of the current week with \"off\" and before the \"off\"\n",
    "        # of the end of the week we will still have an \"on\" alarm. This is caused because the deletion of the same alarms in a row is done \n",
    "        # before the add of the new rows in the beggining and end of the week\n",
    "\n",
    "        # So let's eliminate this cases too:\n",
    "        # first we have to frop the old prev_flag column that now is useless:\n",
    "        concatenated_weeks.drop(\"prev_flag\", axis=1, inplace=True)\n",
    "\n",
    "        # and create the new one:\n",
    "        concatenated_weeks[\"prev_flag_concat\"] = concatenated_weeks[\"flag\"].shift()\n",
    "\n",
    "        concatenated_weeks = concatenated_weeks.loc[\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"] == \"off\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"] == \"on\")) |\n",
    "            ((concatenated_weeks[\"flag\"] == \"off\") & (concatenated_weeks[\"prev_flag_concat\"].isna())) | # This two last comparisons are for the first rows because they don't have a previous flag\n",
    "            ((concatenated_weeks[\"flag\"] == \"on\") & (concatenated_weeks[\"prev_flag_concat\"].isna()))\n",
    "        ]\n",
    "\n",
    "        # We have some weeks where there is no data and thus if this weeks are between an \"on\" and \"off\" flag they will not\n",
    "        # appear on the dataframe. What we will do is a left join with the variable week generated at the begining\n",
    "        # of the notebook to see the rows that do not appear. Then we will activate the alarm at the begining of the week\n",
    "        # and deactivate it at the end of the same week:\n",
    "\n",
    "        # Set the day to monday\n",
    "        concatenated_weeks[\"week\"] = concatenated_weeks[\"dated\"].apply(lambda x: return_monday(x))\n",
    "        # Left join with weeks to detect the missing weeks\n",
    "        concatenated_weeks_merged = pd.merge(weeks, concatenated_weeks, on=\"week\", how=\"left\")\n",
    "\n",
    "        # Store the result of the filling:\n",
    "        filled_dataframe = pd.DataFrame()\n",
    "\n",
    "        # We add the first row\n",
    "        if not pd.isna(concatenated_weeks_merged.iloc[0][\"alarm\"]):\n",
    "            first_new_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"week\": [concatenated_weeks_merged.iloc[0][\"week\"]],\n",
    "                    \"alarm\": [concatenated_weeks_merged.iloc[0][\"alarm\"]],\n",
    "                    \"dated\": [concatenated_weeks_merged.iloc[0][\"dated\"]],\n",
    "                    \"flag\": [concatenated_weeks_merged.iloc[0][\"flag\"]],\n",
    "                    \"id\": [concatenated_weeks_merged.iloc[0][\"id\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            filled_dataframe = pd.concat(\n",
    "                [filled_dataframe, first_new_row],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Begin the iteration where we will add one by one the rows to the dataframe filled_dataframe\n",
    "        for i in range(1, len(concatenated_weeks_merged)-1):\n",
    "\n",
    "            current_row = concatenated_weeks_merged.iloc[i]\n",
    "\n",
    "            # If we find a normal row we add it to the dataframe\n",
    "            if (current_row[\"alarm\"] == \"brdpower\") | (current_row[\"alarm\"] == \"turn_off_end_week\") | (current_row[\"alarm\"] == \"turn_on_begining_week\"):\n",
    "                current_row[\"dated\"] = pd.to_datetime(current_row[\"dated\"])\n",
    "\n",
    "                new_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"week\": [current_row[\"week\"]],\n",
    "                        \"alarm\": [current_row[\"alarm\"]],\n",
    "                        \"dated\": [current_row[\"dated\"]],\n",
    "                        \"flag\": [current_row[\"flag\"]],\n",
    "                        \"id\": [current_row[\"id\"]]\n",
    "                    }\n",
    "                )\n",
    "                filled_dataframe = pd.concat([filled_dataframe, new_row], sort=True)\n",
    "\n",
    "            if not filled_dataframe.empty:\n",
    "                # If the last row of the filled dataframe is a \"turn_off_week\" ant the current is a Emty we add the \"on\" and\n",
    "                # off for the begining and the end of the week\n",
    "                last_row_filled_dataframe = filled_dataframe.iloc[-1]\n",
    "\n",
    "                if (pd.isna(current_row[\"alarm\"])) & (last_row_filled_dataframe[\"alarm\"] in [\"turn_off_end_week\", \"turn_off_end_week_filled\"]):\n",
    "\n",
    "                    new_row_begining_dated = datetime.datetime.combine(current_row[\"week\"], datetime.time(0,0,0))\n",
    "                    new_row_begining_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_on_begining_week_filled\"],\n",
    "                            \"dated\": [new_row_begining_dated], # Add the time to the date\n",
    "                            \"flag\": [\"on\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "                    # We have to add 6 days because the representative of the week in this case is \n",
    "                    new_row_end_dated = datetime.datetime.combine(current_row[\"week\"]+ pd.Timedelta(days=6), datetime.time(23,59,59))\n",
    "                    new_row_end_week = pd.DataFrame(\n",
    "                        {\n",
    "                            \"week\": [current_row[\"week\"]],\n",
    "                            \"alarm\": [\"turn_off_end_week_filled\"],\n",
    "                            \"dated\": [new_row_end_dated], # In this case we add the time to represent the last moment of the week\n",
    "                            \"flag\": [\"off\"],\n",
    "                            \"id\": [idd]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    filled_dataframe = pd.concat(\n",
    "                        [filled_dataframe, new_row_begining_week, new_row_end_week],\n",
    "                        sort=True\n",
    "                    )\n",
    "\n",
    "        # Again we have to do a left join with the dataframe weeks to detect the NaN values:\n",
    "        filled_dataframe_merged = pd.merge(weeks, filled_dataframe, on=\"week\", how=\"left\")\n",
    "        \n",
    "        # Now for each one of the weeks we have to calculate the total time passed between an \"on\" alarm and an \"off\" alarm\n",
    "\n",
    "        # In this dataframe we will store the amount of hours of a ebox that has been malfunctioning for each week\n",
    "        week_hours_dataframe = pd.DataFrame()\n",
    "        for week in weeks[\"week\"]:\n",
    "            # In this variable we will store the amount of hours for this week:\n",
    "            total_hours = 0\n",
    "            on_timestamp = None\n",
    "\n",
    "            # Dataframe with the alarms of the week:\n",
    "            week_alarms_dataframe = filled_dataframe_merged.loc[filled_dataframe_merged[\"week\"] == week]\n",
    "            # Iterate trough the df to count the hours:\n",
    "            for _, row in week_alarms_dataframe.iterrows():\n",
    "                if row[\"flag\"] == \"on\":\n",
    "                    on_timestamp = row[\"dated\"]\n",
    "\n",
    "                if (row[\"flag\"] == \"off\") & (on_timestamp is not None):\n",
    "                    total_hours += (row[\"dated\"] - on_timestamp).total_seconds() / 3600\n",
    "\n",
    "                    on_timestamp = None\n",
    "\n",
    "            new_week_hours = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "                    \"week\": [week],\n",
    "                    \"malfunctioning_hours\": [total_hours] \n",
    "                }\n",
    "            )\n",
    "\n",
    "            week_hours_dataframe = pd.concat(\n",
    "                [week_hours_dataframe, new_week_hours],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        # Now we want to get the data in the format:\n",
    "        # row = {\"week-4\": date_week_prev_4, \"hours_week-4\": hours_week_prev_4, ..., \"week-1\": date_week_prev_1, \"hours_week-1\": hours_week_prev_1, \"current_week\": date_current_week, \"hours_current_week\":  \"week+1\": date_week_next_1, \"hours_week+1\": hours_week_next_1 ..., \"week+4\": date_week_next_4, \"hours_week+4\": hours_week_next_4}\n",
    "\n",
    "        # In this dataframe we will store the data in the format we have mentioned:\n",
    "        lag_dataframe = pd.DataFrame()\n",
    "\n",
    "        # Here we have to follow two different paths. The first one will be when preparing the data for training and the other one will be when \n",
    "        # preparing the data for predicting. We shall begin with predicting:\n",
    "\n",
    "        if for_predicting:\n",
    "            lag_dataframe = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": [idd],\n",
    "\n",
    "                    \"week-4\": [week_hours_dataframe.iloc[-4][\"week\"]],\n",
    "                    \"hours_week-4\": [week_hours_dataframe.iloc[-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-3\": [week_hours_dataframe.iloc[-3][\"week\"]],\n",
    "                    \"hours_week-3\": [week_hours_dataframe.iloc[-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-2\": [week_hours_dataframe.iloc[-2][\"week\"]],\n",
    "                    \"hours_week-2\": [week_hours_dataframe.iloc[-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"week-1\": [week_hours_dataframe.iloc[-1][\"week\"]],\n",
    "                    \"hours_week-1\": [week_hours_dataframe.iloc[-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                    \"current_week\": [week_hours_dataframe.iloc[0][\"week\"]],\n",
    "                    \"hours_current_week\": [week_hours_dataframe.iloc[0][\"malfunctioning_hours\"]]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Begin the loop at 4 and end at -4 so we don't get the error: \"Out of range\"\n",
    "            for i in range(4, len(weeks)-4):\n",
    "                # Create the new row to add:\n",
    "                to_add_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": [idd],\n",
    "\n",
    "                        \"week-4\": [week_hours_dataframe.iloc[i-4][\"week\"]],\n",
    "                        \"hours_week-4\": [week_hours_dataframe.iloc[i-4][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-3\": [week_hours_dataframe.iloc[i-3][\"week\"]],\n",
    "                        \"hours_week-3\": [week_hours_dataframe.iloc[i-3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-2\": [week_hours_dataframe.iloc[i-2][\"week\"]],\n",
    "                        \"hours_week-2\": [week_hours_dataframe.iloc[i-2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week-1\": [week_hours_dataframe.iloc[i-1][\"week\"]],\n",
    "                        \"hours_week-1\": [week_hours_dataframe.iloc[i-1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"current_week\": [week_hours_dataframe.iloc[i][\"week\"]],\n",
    "                        \"hours_current_week\": [week_hours_dataframe.iloc[i][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+1\": [week_hours_dataframe.iloc[i+1][\"week\"]],\n",
    "                        \"hours_week+1\": [week_hours_dataframe.iloc[i+1][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+2\": [week_hours_dataframe.iloc[i+2][\"week\"]],\n",
    "                        \"hours_week+2\": [week_hours_dataframe.iloc[i+2][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+3\": [week_hours_dataframe.iloc[i+3][\"week\"]],\n",
    "                        \"hours_week+3\": [week_hours_dataframe.iloc[i+3][\"malfunctioning_hours\"]],\n",
    "\n",
    "                        \"week+4\": [week_hours_dataframe.iloc[i+4][\"week\"]],\n",
    "                        \"hours_week+4\": [week_hours_dataframe.iloc[i+4][\"malfunctioning_hours\"]]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                lag_dataframe = pd.concat(\n",
    "                    [lag_dataframe, to_add_row],\n",
    "                    sort=True,\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "            # Reorder the dataframe so it is in the same order we have defined:\n",
    "            lag_dataframe = lag_dataframe[columns_order]\n",
    "            \n",
    "            # Add the dataframe to the general one:\n",
    "            general_lag_dataframe = pd.concat(\n",
    "                [general_lag_dataframe, lag_dataframe],\n",
    "                sort=True\n",
    "            )\n",
    "        \n",
    "    # Reordenate with the list columns_order:\n",
    "    general_lag_dataframe = general_lag_dataframe[columns_order]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Execution time:\" + str(end_time - start_time))\n",
    "\n",
    "    return general_lag_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation predictive maintenance\n",
    "This is the repository of the implementation of the predictive maintenance project for INNERGY. \n",
    "\n",
    "## Description of the project:\n",
    "The main objective of the project is the creation of a machine learning model able to calculate accurate predictions of future breakdowns for both public luminaire (from now on lights) and electric panels (eboxes). The repo contains the whole pipeline from raw data preprocessing to training the models and doing predictions.\n",
    "\n",
    "The model will take as input information about the breakdown history from the last 5 weeks of a light or an ebox together with context information such as electric readings data and weather conditions. The output is the probability of breakdown in the next 4 weeks.\n",
    "\n",
    "The code has four main functionalities that can be combined depending on the necessities of the user:\n",
    "* **Meteo web scrapping:** Gather the meteorological information from a certain municipality on a specified time period using web scraping\n",
    "* **Data preprocessing:** Pipeline to do the necessary data preprocessing of the raw breakdown data, the raw readings data and the scrapped meteorological data\n",
    "* **Model training:** Training of the machine learning models using the prerpocessed data\n",
    "* **Doing predictions:** Gather data from the last 5 weeks, preprocess it and do predictions using the trained models\n",
    "\n",
    "Let's explain step by step how each one of the modules work:\n",
    "\n",
    "## Meteo web scrapping\n",
    "Code in the file scrapy_meteo.py of the folder scrapy_meteo. As we have already mentioned this module gathers the raw meteorological context data from the website .\n",
    "To exectute the scraper, navigate to the folder scrapy_meteo in the repo and run the following in the terminal:\n",
    "\n",
    "```\n",
    "python scrapy_meteo.py --di \"/local/directory/folder\" --mu municipalities_list --dr date_ranges --li list_of_links\n",
    "```\n",
    "Let's take a look at the arguments:\n",
    "* ```--di``` is the local directory of the folder you want to store the data extracted using the scraper.\n",
    "* ```--mu``` is a list of the names of the municipalities you want to get the data.\n",
    "* ```--dr``` is a dictionary where the municipality names serve as keys, and the corresponding values are tuples containing the desired date ranges for data extraction. See the example below for clarification.\n",
    "* ```--li``` is a list of the links for doing the scraping.\n",
    "\n",
    "For example, in case that you want to gather meteo information from the Spanish municipalities of Illora, Mejorada and Canyelles you should run in the terminal:\n",
    "\n",
    "```python scrapy_meteo.py --di \"/home/leibniz/Desktop/IHMAN/meteo_raw_data\" --mu illora mejorada canyelles --dr \"{'illora': ('2015-01-01', '2023-04-01'), 'mejorada': ('2014-01-01', '2023-04-01'), 'canyelles': ('2015-01-01', '2023-04-01')}\" --li \"https://www.wunderground.com/history/monthly/es/íllora/LEMG/date/\" \"https://www.wunderground.com/history/monthly/es/mejorada-del-campo/IMEJOR1/date/\" \"https://www.wunderground.com/history/monthly/es/canyelles/ICANYE10/date/\"```\n",
    "\n",
    "The models have been trained with data from this municipalities and in this exact date ranges so if your objective is to gather the information for replicating the the training some of the arguments have implemented default values to make the syntaxis more clear, so runing the following will gather the same data as the previous command:\n",
    "\n",
    "```python scrapy_meteo.py --di \"/home/leibniz/Desktop/IHMAN/meteo_raw_data\"```\n",
    "\n",
    "It is frequent to get errors when running this code because of conexion issues. We recommend to try to run the code again if you get an error. If the error persists after 3 or 4 tries it will be better to try to get the information in separte dataframes by running one municiaplity at a time.\n",
    "\n",
    "## Data preprocessing\n",
    "The code in the file preprocessing_main.py executes the preprocessing of all the data and prepares it for the training phase. For runing the preprocessing you must have 3 subfolders in the same folder somewhere on your local machine. The first one is the folder **raw_data** where you must have the following files:\n",
    "* municipality_eboxes_alarms.csv For example canyelles_eboxes_alarms.csv\n",
    "* municipality_lights_alarms.csv For example canyelles_lights_alarms.csv\n",
    "* municipality_nodes.csv This file conatains the information of the eboxes associated at each one of the lights. For example canyelles_nodes.csv\n",
    "* municipality_readings_sorted.csv This file contains the readings powerActive, powerReactive, powerActivePeak, powerReactivePeak **sorted** by date. For example canyelles_readings_sorted.csv\n",
    "\n",
    "The second folder is **meteo_raw_data** that must contain the file:\n",
    "* new_meteo_municipality.csv This file contains the meteo data that is the output of the scraper. For example new_meteo_canyelles.csv\n",
    "\n",
    "Note that you must have each one of this files for each one of the municipalities that you want to preprocess.\n",
    "\n",
    "The third folder is **preprocessing_results**. Once you execute the module, the code will look for this folder to drop the prerprocessed dataframes\n",
    "\n",
    "To execute the data preprocess run the following command in the terminal:\n",
    "```\n",
    "python preprocessing_main.py --di \"/local/directory/folder\" --mu municipalities_list --ws n_weeks\n",
    "```\n",
    "Let's take a look at the arguments:\n",
    "* ```--di``` is the local directory of the folder where you must have the subfolders raw_data and meteo_raw_data.\n",
    "* ```--mu``` is a list of the names of the municipalities you want to include in the data preprocessing.\n",
    "* ```--ws``` is the number of weeks data that we include in each one of the batches when prerpocessing the dataframes of readings. If you get a Memory error consider making this argument smaller. It defaults as 40.\n",
    "\n",
    "For example, in case that you want to preprocess data from the Spanish municipalities of Illora, Mejorada and Canyelles with a datasplit of 40 you should run in the terminal:\n",
    "\n",
    "```\n",
    "python preprocessing_main.py --di \"/home/leibniz/Desktop/IHMAN\" --mu illora mejorada canyelles --ws 40\n",
    "```\n",
    "\n",
    "## Model training\n",
    "For training the model you have two options. The first one is to use the step by step code in the jupyter notebook model_train.ipynb where there is a clear explanation of each step of the process. If you choose this approach you will be able to execute grid searches and tune the hyperparameters of the model to better fit your training data. The other option is to train the model with a set of hyperparameters optimized to fit the training dataset composed of data from Illora, Mejorada and Canyelles. If you choose the last option run the following command in the terminal:\n",
    "\n",
    "```\n",
    "python --di \"/local/directory/folder\" --de device --st store --rew rewrite --mo model\n",
    "```\n",
    "Let's take a look at the arguments:\n",
    "* ```--di``` is the local directory where you must have the folder preprocessing_results with the output of the preprocessing module.\n",
    "* ```--de``` is the device for which you want to train a prediction model.\n",
    "* ```--st``` is a boolean that indicates wether or not you want to store the trained model in the folder predictive_models in the repo\n",
    "* ```--rew``` is a boolean that indicates wether or not you want to rewrite the default models that come with the repository\n",
    "* ```--mo``` is the type of model you want to train in the case that you have set the argument ```--de``` to light. It is either \"default\" for a model that uses the electrical readings but is trained in less data or \"adboc\" for training the Ada Boost Combined Predictor which uses much more data but does not use the readings.\n",
    "\n",
    "For example for training a lights breakdown prediction model storing the models without rewriting the models I would run in my machine:\n",
    "\n",
    "```\n",
    "python --di \"/home/leibniz/Desktop/IHMAN\" --de \"lights\" --st True --rew False --mo \"adboc\"\n",
    "```\n",
    "in case I want to train it for eboxes:\n",
    "```\n",
    "python --di \"/home/leibniz/Desktop/IHMAN\" --de \"eboxes\" --st True --rew False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
